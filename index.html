<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yangmour.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="日常笔记">
<meta property="og:type" content="website">
<meta property="og:title" content="希文的个人博客">
<meta property="og:url" content="https://yangmour.github.io/index.html">
<meta property="og:site_name" content="希文的个人博客">
<meta property="og:description" content="日常笔记">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="希文">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://yangmour.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>希文的个人博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="希文的个人博客" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">希文的个人博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">白日依山尽，黄河入海流。欲穷千里目，更上一层楼。</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签<span class="badge">15</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类<span class="badge">0</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档<span class="badge">49</span></a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="heartbeat fa-fw"></i>公益 404</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yangmour.github.io/2022/11/03/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3-ha/Hadoop%20HA%E9%AB%98%E5%8F%AF%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E5%A4%B4%E5%83%8F.gif">
      <meta itemprop="name" content="希文">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="希文的个人博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/03/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3-ha/Hadoop%20HA%E9%AB%98%E5%8F%AF%E7%94%A8/" class="post-title-link" itemprop="url">hadoop HA高可用</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-11-03 13:06:17 / 修改时间：15:01:32" itemprop="dateCreated datePublished" datetime="2022-11-03T13:06:17+08:00">2022-11-03</time>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>22k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>20 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h1 id="Hadoop-HA-高可用"><a href="#Hadoop-HA-高可用" class="headerlink" title="Hadoop HA 高可用"></a><strong>Hadoop HA</strong> 高可用</h1><h2 id="6-1-HA-概述"><a href="#6-1-HA-概述" class="headerlink" title="6.1 HA 概述"></a>6.1 HA 概述</h2><p>（1）   所谓 HA（High Availablity），即高可用（7*24 小时不中断服务）。 </p>
<p>（2）   实现高可用最关键的策略是消除单点故障。HA 严格来说应该分成各个组件的 HA 机制：HDFS 的 HA 和 YARN 的 HA。 </p>
<p>（3）   NameNode 主要在以下两个方面影响 HDFS 集群 </p>
<ul>
<li>➢ NameNode 机器发生意外，如宕机，集群将无法使用，直到管理员重启 </li>
</ul>
<ul>
<li>➢ NameNode 机器需要升级，包括软件、硬件升级，此时集群也将无法使用 </li>
</ul>
<p>HDFS HA 功能通过配置多个 NameNodes(Active/Standby)实现在集群中对 NameNode 的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，这时可通过此种方式将 NameNode 很快的切换到另外一台机器。 </p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/11/03/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3-ha/Hadoop%20HA%E9%AB%98%E5%8F%AF%E7%94%A8/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yangmour.github.io/2022/11/03/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E8%B7%AF%E7%BA%BF/%E8%B7%AF%E7%BA%BF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E5%A4%B4%E5%83%8F.gif">
      <meta itemprop="name" content="希文">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="希文的个人博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/03/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E8%B7%AF%E7%BA%BF/%E8%B7%AF%E7%BA%BF/" class="post-title-link" itemprop="url">学习路线</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-11-03 10:40:17" itemprop="dateCreated datePublished" datetime="2022-11-03T10:40:17+08:00">2022-11-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-01 21:59:58" itemprop="dateModified" datetime="2022-11-01T21:59:58+08:00">2022-11-01</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>9</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="路线图（持续更新）"><a href="#路线图（持续更新）" class="headerlink" title="路线图（持续更新）"></a>路线图（持续更新）</h2><p><img src="https://image.3001.net/images/20221101/16673111878038.png" alt="图片"></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/11/03/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E8%B7%AF%E7%BA%BF/%E8%B7%AF%E7%BA%BF/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yangmour.github.io/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/zookeeper/zookeeper-3.5.7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E5%A4%B4%E5%83%8F.gif">
      <meta itemprop="name" content="希文">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="希文的个人博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/zookeeper/zookeeper-3.5.7/" class="post-title-link" itemprop="url">zookeeper</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-11-02 23:06:17 / 修改时间：23:24:30" itemprop="dateCreated datePublished" datetime="2022-11-02T23:06:17+08:00">2022-11-02</time>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>24k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>22 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<p>尚硅谷技术之 Zookeeper  </p>
<p>（作者：尚硅谷研究院） </p>
<p>版本：V3.3 </p>
<h1 id="第-1-章-Zookeeper-入门"><a href="#第-1-章-Zookeeper-入门" class="headerlink" title="第 1 章 Zookeeper 入门"></a>第 <strong>1</strong> 章 <strong>Zookeeper</strong> 入门</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><p>Zookeeper 是一个开源的分布式的，为分布式框架提供协调服务的 Apache 项目。 </p>
<p><img src="https://image.3001.net/images/20221102/16673997206641.png" alt="image-20221102223508155"></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/zookeeper/zookeeper-3.5.7/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yangmour.github.io/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/06.%E7%94%9F%E4%BA%A7%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C-%E9%9B%86%E7%BE%A4%E8%BF%81%E7%A7%BB%EF%BC%88cdh%E9%9B%86%E7%BE%A4%E7%89%88%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E5%A4%B4%E5%83%8F.gif">
      <meta itemprop="name" content="希文">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="希文的个人博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/06.%E7%94%9F%E4%BA%A7%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C-%E9%9B%86%E7%BE%A4%E8%BF%81%E7%A7%BB%EF%BC%88cdh%E9%9B%86%E7%BE%A4%E7%89%88%EF%BC%89/" class="post-title-link" itemprop="url">06.生产调优手册-集群迁移（cdh集群版）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-11-02 10:06:17" itemprop="dateCreated datePublished" datetime="2022-11-02T10:06:17+08:00">2022-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-10 14:08:09" itemprop="dateModified" datetime="2022-11-10T14:08:09+08:00">2022-11-10</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6.3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<p>尚硅谷大数据技术之集群迁移（Apache和CDH）</p>
<p>（作者：尚硅谷大数据研发部）</p>
<p>版本：V1.0</p>
<h1 id="第1章迁移数据"><a href="#第1章迁移数据" class="headerlink" title="第1章迁移数据"></a>第1章迁移数据</h1><p>1）准备两套集群，我这使用apache集群和CDH集群。</p>
<p><img src="https://image.3001.net/images/20221110/16680602987890.jpg" alt="img"></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/06.%E7%94%9F%E4%BA%A7%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C-%E9%9B%86%E7%BE%A4%E8%BF%81%E7%A7%BB%EF%BC%88cdh%E9%9B%86%E7%BE%A4%E7%89%88%EF%BC%89/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yangmour.github.io/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/03_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88HDFS%EF%BC%89V3.3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E5%A4%B4%E5%83%8F.gif">
      <meta itemprop="name" content="希文">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="希文的个人博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/03_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88HDFS%EF%BC%89V3.3/" class="post-title-link" itemprop="url">03_尚硅谷大数据技术之Hadoop（HDFS）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-11-02 10:06:17" itemprop="dateCreated datePublished" datetime="2022-11-02T10:06:17+08:00">2022-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-10 17:32:57" itemprop="dateModified" datetime="2022-11-10T17:32:57+08:00">2022-11-10</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>20k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>18 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<p>尚硅谷大数据技术之Hadoop（HDFS）</p>
<p>（作者：尚硅谷大数据研发部）</p>
<p>版本：V3.3</p>
<h1 id="第1章-HDFS概述"><a href="#第1章-HDFS概述" class="headerlink" title="第1章 HDFS概述"></a>第1章 HDFS概述</h1><h2 id="1-1-HDFS产出背景及定义"><a href="#1-1-HDFS产出背景及定义" class="headerlink" title="1.1 HDFS产出背景及定义"></a>1.1 HDFS产出背景及定义</h2><p><strong>1</strong>）HDFS产生背景</p>
<pre><code>    随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种。
</code></pre>
<p><strong>2</strong>）HDFS定义</p>
<pre><code>    HDFS（Hadoop Distributed File System），它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。

    HDFS的使用场景：适合一次写入，多次读出的场景。一个文件经过创建、写入和关闭之后就不需要改变。
</code></pre>
<h2 id="1-2-HDFS优缺点"><a href="#1-2-HDFS优缺点" class="headerlink" title="1.2 HDFS优缺点"></a>1.2 HDFS优缺点</h2><p><img src="https://image.3001.net/images/20221104/1667533867847.png#crop=0&crop=0&crop=1&crop=1&id=EcrA2&originHeight=594&originWidth=1095&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p><img src="https://image.3001.net/images/20221104/16675338853942.png#crop=0&crop=0&crop=1&crop=1&id=Fxk2o&originHeight=605&originWidth=1090&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h2 id="1-3-HDFS组成架构"><a href="#1-3-HDFS组成架构" class="headerlink" title="1.3 HDFS组成架构"></a>1.3 HDFS组成架构</h2><p><img src="https://image.3001.net/images/20221104/16675338952996.png#crop=0&crop=0&crop=1&crop=1&id=q56Zt&originHeight=599&originWidth=1092&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p><img src="https://image.3001.net/images/20221104/1667533911860.png#crop=0&crop=0&crop=1&crop=1&id=eEPR6&originHeight=588&originWidth=1100&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h2 id="1-4-HDFS文件块大小（面试重点）"><a href="#1-4-HDFS文件块大小（面试重点）" class="headerlink" title="1.4 HDFS文件块大小（面试重点）"></a>1.4 HDFS文件块大小（面试重点）</h2><p><img src="https://image.3001.net/images/20221104/16675339217073.png#crop=0&crop=0&crop=1&crop=1&id=sFCnn&originHeight=597&originWidth=1100&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p><img src="https://image.3001.net/images/20221104/16675339534468.png#crop=0&crop=0&crop=1&crop=1&id=sjZqF&originHeight=598&originWidth=1137&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h1 id="第2章-HDFS的Shell操作（开发重点）"><a href="#第2章-HDFS的Shell操作（开发重点）" class="headerlink" title="第2章 HDFS的Shell操作（开发重点）"></a>第2章 HDFS的Shell操作（开发重点）</h1><h2 id="2-1-基本语法"><a href="#2-1-基本语法" class="headerlink" title="2.1 基本语法"></a>2.1 基本语法</h2><p>hadoop fs 具体命令 OR hdfs dfs 具体命令</p>
<p>两个是完全相同的。</p>
<h2 id="2-2-命令大全"><a href="#2-2-命令大全" class="headerlink" title="2.2 命令大全"></a>2.2 命令大全</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ bin/hadoop fs</span><br><span class="line"></span><br><span class="line">[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">        [-chgrp [-R] GROUP PATH...]</span><br><span class="line">        [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">        [-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">        [-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">        [-count [-q] &lt;path&gt; ...]</span><br><span class="line">        [-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-df [-h] [&lt;path&gt; ...]]</span><br><span class="line">        [-du [-s] [-h] &lt;path&gt; ...]</span><br><span class="line">        [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">        [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">        [-help [cmd ...]]</span><br><span class="line">        [-ls [-d] [-h] [-R] [&lt;path&gt; ...]]</span><br><span class="line">        [-mkdir [-p] &lt;path&gt; ...]</span><br><span class="line">        [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">        [-mv &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br><span class="line">        [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</span><br><span class="line">&lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">        [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">        [-stat [format] &lt;path&gt; ...]</span><br><span class="line">        [-tail [-f] &lt;file&gt;]</span><br><span class="line">        [-test -[defsz] &lt;path&gt;]</span><br><span class="line">        [-text [-ignoreCrc] &lt;src&gt; ...]</span><br></pre></td></tr></table></figure>

<h2 id="2-3-常用命令实操"><a href="#2-3-常用命令实操" class="headerlink" title="2.3 常用命令实操"></a>2.3 常用命令实操</h2><h3 id="2-3-1-准备工作"><a href="#2-3-1-准备工作" class="headerlink" title="2.3.1 准备工作"></a>2.3.1 准备工作</h3><p>1）启动Hadoop集群（方便后续的测试）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh</span><br><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<p>2）-help：输出这个命令参数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -help rm</span><br></pre></td></tr></table></figure>

<p>3）创建/sanguo文件夹</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -mkdir /sanguo</span><br></pre></td></tr></table></figure>

<h3 id="2-3-2-上传"><a href="#2-3-2-上传" class="headerlink" title="2.3.2 上传"></a>2.3.2 上传</h3><p>1）-moveFromLocal：从本地剪切粘贴到HDFS</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ vim shuguo.txt</span><br><span class="line"></span><br><span class="line">#输入：</span><br><span class="line">shuguo</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -moveFromLocal ./shuguo.txt /sanguo</span><br></pre></td></tr></table></figure>

<p>2）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ vim weiguo.txt</span><br><span class="line">#输入：</span><br><span class="line">weiguo</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -copyFromLocal weiguo.txt /sanguo</span><br></pre></td></tr></table></figure>

<p>3）-put：等同于copyFromLocal，生产环境更习惯用put</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ vim wuguo.txt</span><br><span class="line">输入：</span><br><span class="line">wuguo</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -put ./wuguo.txt /sanguo</span><br></pre></td></tr></table></figure>

<p>4）-appendToFile：追加一个文件到已经存在的文件末尾</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ vim liubei.txt</span><br><span class="line">输入：</span><br><span class="line">liubei</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo.txt</span><br></pre></td></tr></table></figure>

<h3 id="2-3-3-下载"><a href="#2-3-3-下载" class="headerlink" title="2.3.3 下载"></a>2.3.3 下载</h3><p>1）-copyToLocal：从HDFS拷贝到本地</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -copyToLocal /sanguo/shuguo.txt ./</span><br></pre></td></tr></table></figure>

<p>2）-get：等同于copyToLocal，生产环境更习惯用get</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -get /sanguo/shuguo.txt ./shuguo2.txt</span><br></pre></td></tr></table></figure>

<h3 id="2-3-4-HDFS直接操作"><a href="#2-3-4-HDFS直接操作" class="headerlink" title="2.3.4 HDFS直接操作"></a>2.3.4 HDFS直接操作</h3><p>1）-ls: 显示目录信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -ls /sanguo</span><br></pre></td></tr></table></figure>

<p>2）-cat：显示文件内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -cat /sanguo/shuguo.txt</span><br></pre></td></tr></table></figure>

<p>3）-chgrp、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -chmod 666 /sanguo/shuguo.txt</span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -chown atguigu:atguigu /sanguo/shuguo.txt</span><br></pre></td></tr></table></figure>

<p>4）-mkdir：创建路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -mkdir /jinguo</span><br></pre></td></tr></table></figure>

<p>5）-cp：从HDFS的一个路径拷贝到HDFS的另一个路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -cp /sanguo/shuguo.txt /jinguo</span><br></pre></td></tr></table></figure>

<p>6）-mv：在HDFS目录中移动文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -mv /sanguo/wuguo.txt /jinguo</span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -mv /sanguo/weiguo.txt /jinguo</span><br></pre></td></tr></table></figure>

<p>7）-tail：显示一个文件的末尾1kb的数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -tail /jinguo/shuguo.txt</span><br></pre></td></tr></table></figure>

<p>8）-rm：删除文件或文件夹</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -rm /sanguo/shuguo.txt</span><br></pre></td></tr></table></figure>

<p>9）-rm -r：递归删除目录及目录里面内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -rm -r /sanguo</span><br></pre></td></tr></table></figure>

<p>10）-du统计文件夹的大小信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -du -s -h /jinguo</span><br><span class="line">27  81  /jinguo</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -du  -h /jinguo</span><br><span class="line">14  42  /jinguo/shuguo.txt</span><br><span class="line">7   21   /jinguo/weiguo.txt</span><br><span class="line">6   18   /jinguo/wuguo.txt</span><br></pre></td></tr></table></figure>

<p>    说明：27表示文件大小；81表示27*3个副本；/jinguo表示查看的目录</p>
<p>11）-setrep：设置HDFS中文件的副本数量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -setrep 10 /jinguo/shuguo.txt</span><br></pre></td></tr></table></figure>

<pre><code>这里设置的副本数只是记录在NameNode的元数据中，是否真的会有这么多副本，还得看DataNode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。
</code></pre>
<h1 id="第3章-HDFS的API操作"><a href="#第3章-HDFS的API操作" class="headerlink" title="第3章 HDFS的API操作"></a>第3章 HDFS的API操作</h1><h2 id="3-1-客户端环境准备"><a href="#3-1-客户端环境准备" class="headerlink" title="3.1 客户端环境准备"></a>3.1 客户端环境准备</h2><p>1）找到资料包路径下的Windows依赖文件夹，拷贝hadoop-3.1.0到非中文路径（比如d:\）。</p>
<p><strong>2</strong>）配置HADOOP_HOME环境变量</p>
<p>     <img src="https://image.3001.net/images/20221104/16675342245925.png#crop=0&crop=0&crop=1&crop=1&id=wTEoA&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p><strong>3</strong>）配置Path环境变量。</p>
<p><strong>注意：如果环境变量不起作用，可以重启电脑试试。</strong></p>
<p>     <img src="https://image.3001.net/images/20221104/16675342428838.png#crop=0&crop=0&crop=1&crop=1&id=CgBMD&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>验证Hadoop环境变量是否正常。双击winutils.exe，如果报如下错误。说明缺少微软运行库（正版系统往往有这个问题）。再资料包里面有对应的微软运行库安装包双击安装即可。</p>
<p><strong>4</strong>）在IDEA中创建一个Maven工程HdfsClientDemo，并导入相应的依赖坐标+日志添加**</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.1.3&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;4.12&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.slf4j&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.7.30&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure>

<p>在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger=INFO, stdout  </span><br><span class="line">log4j.appender.stdout=org.apache.log4j.ConsoleAppender  </span><br><span class="line">log4j.appender.stdout.layout=org.apache.log4j.PatternLayout  </span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  </span><br><span class="line">log4j.appender.logfile=org.apache.log4j.FileAppender  </span><br><span class="line">log4j.appender.logfile.File=target/spring.log  </span><br><span class="line">log4j.appender.logfile.layout=org.apache.log4j.PatternLayout  </span><br><span class="line">log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n</span><br></pre></td></tr></table></figure>

<p><strong>5</strong>）创建包名：com.atguigu.hdfs</p>
<p><strong>6</strong>）创建HdfsClient类</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public class HdfsClient &#123;</span><br><span class="line"></span><br><span class="line">    @Test</span><br><span class="line">    public void testMkdirs() throws IOException, URISyntaxException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        // 1 获取文件系统</span><br><span class="line">        Configuration configuration = new Configuration();</span><br><span class="line"></span><br><span class="line">        // FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:8020&quot;), configuration);</span><br><span class="line">        FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:8020&quot;), configuration,&quot;atguigu&quot;);</span><br><span class="line"></span><br><span class="line">        // 2 创建目录</span><br><span class="line">        fs.mkdirs(new Path(&quot;/xiyou/huaguoshan/&quot;));</span><br><span class="line"></span><br><span class="line">        // 3 关闭资源</span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>7</strong>）执行程序</p>
<pre><code>客户端去操作HDFS时，是有一个用户身份的。默认情况下，HDFS客户端API会从采用Windows默认用户访问HDFS，会报权限异常错误。所以在访问HDFS时，一定要配置用户。
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.security.AccessControlException: Permission denied: user=56576, access=WRITE, inode=&quot;/xiyou/huaguoshan&quot;:atguigu:supergroup:drwxr-xr-x</span><br></pre></td></tr></table></figure>

<h2 id="3-2-HDFS的API案例实操"><a href="#3-2-HDFS的API案例实操" class="headerlink" title="3.2 HDFS的API案例实操"></a>3.2 HDFS的API案例实操</h2><h3 id="3-2-1-HDFS文件上传（测试参数优先级）"><a href="#3-2-1-HDFS文件上传（测试参数优先级）" class="headerlink" title="3.2.1 HDFS文件上传（测试参数优先级）"></a>3.2.1 HDFS文件上传（测试参数优先级）</h3><p><strong>1</strong>）编写源代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testCopyFromLocalFile() throws IOException, InterruptedException, URISyntaxException &#123;</span><br><span class="line"></span><br><span class="line">    // 1 获取文件系统</span><br><span class="line">    Configuration configuration = new Configuration();</span><br><span class="line">    configuration.set(&quot;dfs.replication&quot;, &quot;2&quot;);</span><br><span class="line">    FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:8020&quot;), configuration, &quot;atguigu&quot;);</span><br><span class="line"></span><br><span class="line">    // 2 上传文件</span><br><span class="line">    fs.copyFromLocalFile(new Path(&quot;d:/sunwukong.txt&quot;), new Path(&quot;/xiyou/huaguoshan&quot;));</span><br><span class="line"></span><br><span class="line">    // 3 关闭资源</span><br><span class="line">    fs.close();</span><br><span class="line">｝</span><br></pre></td></tr></table></figure>

<p><strong>2</strong>）将hdfs-site.xml拷贝到项目的resources资源目录下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">         &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p><strong>3</strong>）参数优先级</p>
<p>参数优先级排序：（1）客户端代码中设置的值 &gt;（2）ClassPath下的用户自定义配置文件 &gt;（3）然后是服务器的自定义配置（xxx-site.xml） &gt;（4）服务器的默认配置（xxx-default.xml）</p>
<h3 id="3-2-2-HDFS文件下载"><a href="#3-2-2-HDFS文件下载" class="headerlink" title="3.2.2 HDFS文件下载"></a>3.2.2 HDFS文件下载</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testCopyToLocalFile() throws IOException, InterruptedException, URISyntaxException&#123;</span><br><span class="line"></span><br><span class="line">    // 1 获取文件系统</span><br><span class="line">    Configuration configuration = new Configuration();</span><br><span class="line">    FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:8020&quot;), configuration, &quot;atguigu&quot;);</span><br><span class="line">    </span><br><span class="line">    // 2 执行下载操作</span><br><span class="line">    // boolean delSrc 指是否将原文件删除</span><br><span class="line">    // Path src 指要下载的文件路径</span><br><span class="line">    // Path dst 指将文件下载到的路径</span><br><span class="line">    // boolean useRawLocalFileSystem 是否开启文件校验</span><br><span class="line">    fs.copyToLocalFile(false, new Path(&quot;/xiyou/huaguoshan/sunwukong.txt&quot;), new Path(&quot;d:/sunwukong2.txt&quot;), true);</span><br><span class="line">    </span><br><span class="line">    // 3 关闭资源</span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意：如果执行上面代码，下载不了文件，有可能是你电脑的微软支持的运行库少，需要安装一下微软运行库。</p>
<h3 id="3-2-3-HDFS文件更名和移动"><a href="#3-2-3-HDFS文件更名和移动" class="headerlink" title="3.2.3 HDFS文件更名和移动"></a>3.2.3 HDFS文件更名和移动</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testRename() throws IOException, InterruptedException, URISyntaxException&#123;</span><br><span class="line"></span><br><span class="line">	// 1 获取文件系统</span><br><span class="line">	Configuration configuration = new Configuration();</span><br><span class="line">	FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:8020&quot;), configuration, &quot;atguigu&quot;); </span><br><span class="line">		</span><br><span class="line">	// 2 修改文件名称</span><br><span class="line">	fs.rename(new Path(&quot;/xiyou/huaguoshan/sunwukong.txt&quot;), new Path(&quot;/xiyou/huaguoshan/meihouwang.txt&quot;));</span><br><span class="line">		</span><br><span class="line">	// 3 关闭资源</span><br><span class="line">	fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-2-4-HDFS删除文件和目录"><a href="#3-2-4-HDFS删除文件和目录" class="headerlink" title="3.2.4 HDFS删除文件和目录"></a>3.2.4 HDFS删除文件和目录</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testDelete() throws IOException, InterruptedException, URISyntaxException&#123;</span><br><span class="line"></span><br><span class="line">	// 1 获取文件系统</span><br><span class="line">	Configuration configuration = new Configuration();</span><br><span class="line">	FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:8020&quot;), configuration, &quot;atguigu&quot;);</span><br><span class="line">		</span><br><span class="line">	// 2 执行删除</span><br><span class="line">	fs.delete(new Path(&quot;/xiyou&quot;), true);</span><br><span class="line">		</span><br><span class="line">	// 3 关闭资源</span><br><span class="line">	fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-2-5-HDFS文件详情查看"><a href="#3-2-5-HDFS文件详情查看" class="headerlink" title="3.2.5 HDFS文件详情查看"></a>3.2.5 HDFS文件详情查看</h3><p>查看文件名称、权限、长度、块信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testListFiles() throws IOException, InterruptedException, URISyntaxException &#123;</span><br><span class="line"></span><br><span class="line">	// 1获取文件系统</span><br><span class="line">	Configuration configuration = new Configuration();</span><br><span class="line">	FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:8020&quot;), configuration, &quot;atguigu&quot;);</span><br><span class="line"></span><br><span class="line">	// 2 获取文件详情</span><br><span class="line">	RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true);</span><br><span class="line"></span><br><span class="line">	while (listFiles.hasNext()) &#123;</span><br><span class="line">		LocatedFileStatus fileStatus = listFiles.next();</span><br><span class="line"></span><br><span class="line">		System.out.println(&quot;========&quot; + fileStatus.getPath() + &quot;=========&quot;);</span><br><span class="line">		System.out.println(fileStatus.getPermission());</span><br><span class="line">		System.out.println(fileStatus.getOwner());</span><br><span class="line">		System.out.println(fileStatus.getGroup());</span><br><span class="line">		System.out.println(fileStatus.getLen());</span><br><span class="line">		System.out.println(fileStatus.getModificationTime());</span><br><span class="line">		System.out.println(fileStatus.getReplication());</span><br><span class="line">		System.out.println(fileStatus.getBlockSize());</span><br><span class="line">		System.out.println(fileStatus.getPath().getName());</span><br><span class="line"></span><br><span class="line">		// 获取块信息</span><br><span class="line">		BlockLocation[] blockLocations = fileStatus.getBlockLocations();</span><br><span class="line">		System.out.println(Arrays.toString(blockLocations));</span><br><span class="line">	&#125;</span><br><span class="line">	// 3 关闭资源</span><br><span class="line">	fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-2-6-HDFS文件和文件夹判断"><a href="#3-2-6-HDFS文件和文件夹判断" class="headerlink" title="3.2.6 HDFS文件和文件夹判断"></a>3.2.6 HDFS文件和文件夹判断</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testListStatus() throws IOException, InterruptedException, URISyntaxException&#123;</span><br><span class="line"></span><br><span class="line">    // 1 获取文件配置信息</span><br><span class="line">    Configuration configuration = new Configuration();</span><br><span class="line">    FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:8020&quot;), configuration, &quot;atguigu&quot;);</span><br><span class="line"></span><br><span class="line">    // 2 判断是文件还是文件夹</span><br><span class="line">    FileStatus[] listStatus = fs.listStatus(new Path(&quot;/&quot;));</span><br><span class="line"></span><br><span class="line">    for (FileStatus fileStatus : listStatus) &#123;</span><br><span class="line"></span><br><span class="line">        // 如果是文件</span><br><span class="line">        if (fileStatus.isFile()) &#123;</span><br><span class="line">            System.out.println(&quot;f:&quot;+fileStatus.getPath().getName());</span><br><span class="line">        &#125;else &#123;</span><br><span class="line">            System.out.println(&quot;d:&quot;+fileStatus.getPath().getName());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 3 关闭资源</span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="第4章-HDFS的读写流程（面试重点）"><a href="#第4章-HDFS的读写流程（面试重点）" class="headerlink" title="第4章 HDFS的读写流程（面试重点）"></a>第4章 HDFS的读写流程（面试重点）</h1><h2 id="4-1-HDFS写数据流程"><a href="#4-1-HDFS写数据流程" class="headerlink" title="4.1 HDFS写数据流程"></a>4.1 HDFS写数据流程</h2><h3 id="4-1-1-剖析文件写入"><a href="#4-1-1-剖析文件写入" class="headerlink" title="4.1.1 剖析文件写入"></a>4.1.1 剖析文件写入</h3><p><img src="https://image.3001.net/images/20221104/16675345945070.png#crop=0&crop=0&crop=1&crop=1&id=jNVPz&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>（1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。</p>
<p>（2）NameNode返回是否可以上传。</p>
<p>（3）客户端请求第一个 Block上传到哪几个DataNode服务器上。</p>
<p>（4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。</p>
<p>（5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。</p>
<p>（6）dn1、dn2、dn3逐级应答客户端。</p>
<p>（7）客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。</p>
<p>（8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。</p>
<h3 id="4-1-2-网络拓扑-节点距离计算"><a href="#4-1-2-网络拓扑-节点距离计算" class="headerlink" title="4.1.2 网络拓扑-节点距离计算"></a>4.1.2 网络拓扑-节点距离计算</h3><p>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。那么这个最近距离怎么计算呢？</p>
<p>节点距离：两个节点到达最近的共同祖先的距离总和。</p>
<p><img src="https://image.3001.net/images/20221104/16675346113291.png#crop=0&crop=0&crop=1&crop=1&id=EX50K&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>例如，假设有数据中心d1机架r1中的节点n1。该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述。</p>
<p>大家算一算每两个节点之间的距离。</p>
<p>     <img src="https://image.3001.net/images/20221104/16675346281744.png#crop=0&crop=0&crop=1&crop=1&id=AUOcw&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h3 id="4-1-3-机架感知（副本存储节点选择）"><a href="#4-1-3-机架感知（副本存储节点选择）" class="headerlink" title="4.1.3 机架感知（副本存储节点选择）"></a>4.1.3 机架感知（副本存储节点选择）</h3><p><strong>1）</strong>机架感知说明</p>
<p>（1）官方说明</p>
<p><a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication">http://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">For the common case, when the replication factor is three, HDFS’s placement policy is to put one replica on the local machine if the writer is on a datanode, otherwise on a random datanode, another replica on a node in a different (remote) rack, and the last on a different node in the same remote rack. This policy cuts the inter-rack write traffic which generally improves write performance. The chance of rack failure is far less than that of node failure; this policy does not impact data reliability and availability guarantees. However, it does reduce the aggregate network bandwidth used when reading data since a block is placed in only two unique racks rather than three. With this policy, the replicas of a file do not evenly distribute across the racks. One third of replicas are on one node, two thirds of replicas are on one rack, and the other third are evenly distributed across the remaining racks. This policy improves write performance without compromising data reliability or read performance.</span><br></pre></td></tr></table></figure>

<p>（2）源码说明</p>
<p>Crtl + n 查找BlockPlacementPolicyDefault，在该类中查找chooseTargetInOrder方法。</p>
<p><strong>2</strong>）Hadoop3.1.3副本节点选择</p>
<p><img src="https://image.3001.net/images/20221104/16675346651497.png#crop=0&crop=0&crop=1&crop=1&id=mrtlo&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h2 id="4-2-HDFS读数据流程"><a href="#4-2-HDFS读数据流程" class="headerlink" title="4.2 HDFS读数据流程"></a>4.2 HDFS读数据流程</h2><p><img src="https://image.3001.net/images/20221104/16675346771864.png#crop=0&crop=0&crop=1&crop=1&id=aCtWf&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>（1）客户端通过DistributedFileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。</p>
<p>（2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。</p>
<p>（3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。</p>
<p>（4）客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</p>
<h1 id="第5章-NameNode和SecondaryNameNode"><a href="#第5章-NameNode和SecondaryNameNode" class="headerlink" title="第5章 NameNode和SecondaryNameNode"></a>第5章 NameNode和SecondaryNameNode</h1><h2 id="5-1-NN和2NN工作机制"><a href="#5-1-NN和2NN工作机制" class="headerlink" title="5.1 NN和2NN工作机制"></a>5.1 NN和2NN工作机制</h2><pre><code>    思考：NameNode中的元数据是存储在哪里的？

    首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage。

    这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件（只进行追加操作，效率很高）。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。

    但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并。
</code></pre>
<p><img src="https://image.3001.net/images/20221104/16675347128421.png#crop=0&crop=0&crop=1&crop=1&id=PQ0cK&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>1）第一阶段：NameNode启动</p>
<p>（1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</p>
<p>（2）客户端对元数据进行增删改的请求。</p>
<p>（3）NameNode记录操作日志，更新滚动日志。</p>
<p>（4）NameNode在内存中对元数据进行增删改。</p>
<p>2）第二阶段：Secondary NameNode工作</p>
<p>（1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。</p>
<p>（2）Secondary NameNode请求执行CheckPoint。</p>
<p>（3）NameNode滚动正在写的Edits日志。</p>
<p>（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。</p>
<p>（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</p>
<p>（6）生成新的镜像文件fsimage.chkpoint。</p>
<p>（7）拷贝fsimage.chkpoint到NameNode。</p>
<p>（8）NameNode将fsimage.chkpoint重新命名成fsimage。</p>
<h2 id="5-2-Fsimage和Edits解析"><a href="#5-2-Fsimage和Edits解析" class="headerlink" title="5.2 Fsimage和Edits解析"></a>5.2 Fsimage和Edits解析</h2><p><img src="https://image.3001.net/images/20221104/16675347415963.png#crop=0&crop=0&crop=1&crop=1&id=qtLlj&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>1）oiv查看Fsimage文件</p>
<p>（1）查看oiv和oev命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 current]$ hdfs</span><br><span class="line"></span><br><span class="line">oiv      apply the offline fsimage viewer to an fsimage</span><br><span class="line">oev      apply the offline edits viewer to an edits file</span><br></pre></td></tr></table></figure>

<p>（2）基本语法</p>
<p>hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径</p>
<p>（3）案例实操</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 current]$ pwd</span><br><span class="line">/opt/module/hadoop-3.1.3/data/dfs/name/current</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 current]$ hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-3.1.3/fsimage.xml</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 current]$ cat /opt/module/hadoop-3.1.3/fsimage.xml</span><br></pre></td></tr></table></figure>

<p>将显示的xml文件内容拷贝到Idea中创建的xml文件中，并格式化。部分显示结果如下。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">&lt;inode&gt;</span><br><span class="line">	&lt;id&gt;16386&lt;/id&gt;</span><br><span class="line">	&lt;type&gt;DIRECTORY&lt;/type&gt;</span><br><span class="line">	&lt;name&gt;user&lt;/name&gt;</span><br><span class="line">	&lt;mtime&gt;1512722284477&lt;/mtime&gt;</span><br><span class="line">	&lt;permission&gt;atguigu:supergroup:rwxr-xr-x&lt;/permission&gt;</span><br><span class="line">	&lt;nsquota&gt;-1&lt;/nsquota&gt;</span><br><span class="line">	&lt;dsquota&gt;-1&lt;/dsquota&gt;</span><br><span class="line">&lt;/inode&gt;</span><br><span class="line">&lt;inode&gt;</span><br><span class="line">	&lt;id&gt;16387&lt;/id&gt;</span><br><span class="line">	&lt;type&gt;DIRECTORY&lt;/type&gt;</span><br><span class="line">	&lt;name&gt;atguigu&lt;/name&gt;</span><br><span class="line">	&lt;mtime&gt;1512790549080&lt;/mtime&gt;</span><br><span class="line">	&lt;permission&gt;atguigu:supergroup:rwxr-xr-x&lt;/permission&gt;</span><br><span class="line">	&lt;nsquota&gt;-1&lt;/nsquota&gt;</span><br><span class="line">	&lt;dsquota&gt;-1&lt;/dsquota&gt;</span><br><span class="line">&lt;/inode&gt;</span><br><span class="line">&lt;inode&gt;</span><br><span class="line">	&lt;id&gt;16389&lt;/id&gt;</span><br><span class="line">	&lt;type&gt;FILE&lt;/type&gt;</span><br><span class="line">	&lt;name&gt;wc.input&lt;/name&gt;</span><br><span class="line">	&lt;replication&gt;3&lt;/replication&gt;</span><br><span class="line">	&lt;mtime&gt;1512722322219&lt;/mtime&gt;</span><br><span class="line">	&lt;atime&gt;1512722321610&lt;/atime&gt;</span><br><span class="line">	&lt;perferredBlockSize&gt;134217728&lt;/perferredBlockSize&gt;</span><br><span class="line">	&lt;permission&gt;atguigu:supergroup:rw-r--r--&lt;/permission&gt;</span><br><span class="line">	&lt;blocks&gt;</span><br><span class="line">		&lt;block&gt;</span><br><span class="line">			&lt;id&gt;1073741825&lt;/id&gt;</span><br><span class="line">			&lt;genstamp&gt;1001&lt;/genstamp&gt;</span><br><span class="line">			&lt;numBytes&gt;59&lt;/numBytes&gt;</span><br><span class="line">		&lt;/block&gt;</span><br><span class="line">	&lt;/blocks&gt;</span><br><span class="line">&lt;/inode &gt;</span><br></pre></td></tr></table></figure>

<p>思考：可以看出，Fsimage中没有记录块所对应DataNode，为什么？</p>
<p>在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报。</p>
<p><strong>2</strong>）oev查看Edits文件</p>
<p>（1）基本语法</p>
<p>hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径</p>
<p>（2）案例实操</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-3.1.3/edits.xml</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 current]$ cat /opt/module/hadoop-3.1.3/edits.xml</span><br></pre></td></tr></table></figure>

<p>将显示的xml文件内容拷贝到Idea中创建的xml文件中，并格式化。显示结果如下。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;EDITS&gt;</span><br><span class="line">	&lt;EDITS_VERSION&gt;-63&lt;/EDITS_VERSION&gt;</span><br><span class="line">	&lt;RECORD&gt;</span><br><span class="line">		&lt;OPCODE&gt;OP_START_LOG_SEGMENT&lt;/OPCODE&gt;</span><br><span class="line">		&lt;DATA&gt;</span><br><span class="line">			&lt;TXID&gt;129&lt;/TXID&gt;</span><br><span class="line">		&lt;/DATA&gt;</span><br><span class="line">	&lt;/RECORD&gt;</span><br><span class="line">	&lt;RECORD&gt;</span><br><span class="line">		&lt;OPCODE&gt;OP_ADD&lt;/OPCODE&gt;</span><br><span class="line">		&lt;DATA&gt;</span><br><span class="line">			&lt;TXID&gt;130&lt;/TXID&gt;</span><br><span class="line">			&lt;LENGTH&gt;0&lt;/LENGTH&gt;</span><br><span class="line">			&lt;INODEID&gt;16407&lt;/INODEID&gt;</span><br><span class="line">			&lt;PATH&gt;/hello7.txt&lt;/PATH&gt;</span><br><span class="line">			&lt;REPLICATION&gt;2&lt;/REPLICATION&gt;</span><br><span class="line">			&lt;MTIME&gt;1512943607866&lt;/MTIME&gt;</span><br><span class="line">			&lt;ATIME&gt;1512943607866&lt;/ATIME&gt;</span><br><span class="line">			&lt;BLOCKSIZE&gt;134217728&lt;/BLOCKSIZE&gt;</span><br><span class="line">			&lt;CLIENT_NAME&gt;DFSClient_NONMAPREDUCE_-1544295051_1&lt;/CLIENT_NAME&gt;</span><br><span class="line">			&lt;CLIENT_MACHINE&gt;192.168.10.102&lt;/CLIENT_MACHINE&gt;</span><br><span class="line">			&lt;OVERWRITE&gt;true&lt;/OVERWRITE&gt;</span><br><span class="line">			&lt;PERMISSION_STATUS&gt;</span><br><span class="line">				&lt;USERNAME&gt;atguigu&lt;/USERNAME&gt;</span><br><span class="line">				&lt;GROUPNAME&gt;supergroup&lt;/GROUPNAME&gt;</span><br><span class="line">				&lt;MODE&gt;420&lt;/MODE&gt;</span><br><span class="line">			&lt;/PERMISSION_STATUS&gt;</span><br><span class="line">			&lt;RPC_CLIENTID&gt;908eafd4-9aec-4288-96f1-e8011d181561&lt;/RPC_CLIENTID&gt;</span><br><span class="line">			&lt;RPC_CALLID&gt;0&lt;/RPC_CALLID&gt;</span><br><span class="line">		&lt;/DATA&gt;</span><br><span class="line">	&lt;/RECORD&gt;</span><br><span class="line">	&lt;RECORD&gt;</span><br><span class="line">		&lt;OPCODE&gt;OP_ALLOCATE_BLOCK_ID&lt;/OPCODE&gt;</span><br><span class="line">		&lt;DATA&gt;</span><br><span class="line">			&lt;TXID&gt;131&lt;/TXID&gt;</span><br><span class="line">			&lt;BLOCK_ID&gt;1073741839&lt;/BLOCK_ID&gt;</span><br><span class="line">		&lt;/DATA&gt;</span><br><span class="line">	&lt;/RECORD&gt;</span><br><span class="line">	&lt;RECORD&gt;</span><br><span class="line">		&lt;OPCODE&gt;OP_SET_GENSTAMP_V2&lt;/OPCODE&gt;</span><br><span class="line">		&lt;DATA&gt;</span><br><span class="line">			&lt;TXID&gt;132&lt;/TXID&gt;</span><br><span class="line">			&lt;GENSTAMPV2&gt;1016&lt;/GENSTAMPV2&gt;</span><br><span class="line">		&lt;/DATA&gt;</span><br><span class="line">	&lt;/RECORD&gt;</span><br><span class="line">	&lt;RECORD&gt;</span><br><span class="line">		&lt;OPCODE&gt;OP_ADD_BLOCK&lt;/OPCODE&gt;</span><br><span class="line">		&lt;DATA&gt;</span><br><span class="line">			&lt;TXID&gt;133&lt;/TXID&gt;</span><br><span class="line">			&lt;PATH&gt;/hello7.txt&lt;/PATH&gt;</span><br><span class="line">			&lt;BLOCK&gt;</span><br><span class="line">				&lt;BLOCK_ID&gt;1073741839&lt;/BLOCK_ID&gt;</span><br><span class="line">				&lt;NUM_BYTES&gt;0&lt;/NUM_BYTES&gt;</span><br><span class="line">				&lt;GENSTAMP&gt;1016&lt;/GENSTAMP&gt;</span><br><span class="line">			&lt;/BLOCK&gt;</span><br><span class="line">			&lt;RPC_CLIENTID&gt;&lt;/RPC_CLIENTID&gt;</span><br><span class="line">			&lt;RPC_CALLID&gt;-2&lt;/RPC_CALLID&gt;</span><br><span class="line">		&lt;/DATA&gt;</span><br><span class="line">	&lt;/RECORD&gt;</span><br><span class="line">	&lt;RECORD&gt;</span><br><span class="line">		&lt;OPCODE&gt;OP_CLOSE&lt;/OPCODE&gt;</span><br><span class="line">		&lt;DATA&gt;</span><br><span class="line">			&lt;TXID&gt;134&lt;/TXID&gt;</span><br><span class="line">			&lt;LENGTH&gt;0&lt;/LENGTH&gt;</span><br><span class="line">			&lt;INODEID&gt;0&lt;/INODEID&gt;</span><br><span class="line">			&lt;PATH&gt;/hello7.txt&lt;/PATH&gt;</span><br><span class="line">			&lt;REPLICATION&gt;2&lt;/REPLICATION&gt;</span><br><span class="line">			&lt;MTIME&gt;1512943608761&lt;/MTIME&gt;</span><br><span class="line">			&lt;ATIME&gt;1512943607866&lt;/ATIME&gt;</span><br><span class="line">			&lt;BLOCKSIZE&gt;134217728&lt;/BLOCKSIZE&gt;</span><br><span class="line">			&lt;CLIENT_NAME&gt;&lt;/CLIENT_NAME&gt;</span><br><span class="line">			&lt;CLIENT_MACHINE&gt;&lt;/CLIENT_MACHINE&gt;</span><br><span class="line">			&lt;OVERWRITE&gt;false&lt;/OVERWRITE&gt;</span><br><span class="line">			&lt;BLOCK&gt;</span><br><span class="line">				&lt;BLOCK_ID&gt;1073741839&lt;/BLOCK_ID&gt;</span><br><span class="line">				&lt;NUM_BYTES&gt;25&lt;/NUM_BYTES&gt;</span><br><span class="line">				&lt;GENSTAMP&gt;1016&lt;/GENSTAMP&gt;</span><br><span class="line">			&lt;/BLOCK&gt;</span><br><span class="line">			&lt;PERMISSION_STATUS&gt;</span><br><span class="line">				&lt;USERNAME&gt;atguigu&lt;/USERNAME&gt;</span><br><span class="line">				&lt;GROUPNAME&gt;supergroup&lt;/GROUPNAME&gt;</span><br><span class="line">				&lt;MODE&gt;420&lt;/MODE&gt;</span><br><span class="line">			&lt;/PERMISSION_STATUS&gt;</span><br><span class="line">		&lt;/DATA&gt;</span><br><span class="line">	&lt;/RECORD&gt;</span><br><span class="line">&lt;/EDITS &gt;</span><br></pre></td></tr></table></figure>

<p>思考：NameNode如何确定下次开机启动的时候合并哪些Edits？</p>
<h2 id="5-3-CheckPoint时间设置"><a href="#5-3-CheckPoint时间设置" class="headerlink" title="5.3 CheckPoint时间设置"></a>5.3 CheckPoint时间设置</h2><p>1）通常情况下，SecondaryNameNode每隔一小时执行一次。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;3600s&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>2）一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1000000&lt;/value&gt;</span><br><span class="line">&lt;description&gt;操作动作次数&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;60s&lt;/value&gt;</span><br><span class="line">&lt;description&gt; 1分钟检查一次操作次数&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h1 id="第6章-DataNode"><a href="#第6章-DataNode" class="headerlink" title="第6章 DataNode"></a>第6章 DataNode</h1><h2 id="6-1-DataNode工作机制"><a href="#6-1-DataNode工作机制" class="headerlink" title="6.1 DataNode工作机制"></a>6.1 DataNode工作机制</h2><p><img src="https://image.3001.net/images/20221104/16675348894178.png#crop=0&crop=0&crop=1&crop=1&id=loQHL&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>（1）一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。</p>
<p>（2）DataNode启动后向NameNode注册，通过后，周期性（6小时）的向NameNode上报所有的块信息。</p>
<p>DN向NN汇报当前解读信息的时间间隔，默认6小时；</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;dfs.blockreport.intervalMsec&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;21600000&lt;/value&gt;</span><br><span class="line">	&lt;description&gt;Determines block reporting interval in milliseconds.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>DN扫描自己节点块信息列表的时间，默认6小时</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;dfs.datanode.directoryscan.interval&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;21600s&lt;/value&gt;</span><br><span class="line">	&lt;description&gt;Interval in seconds for Datanode to scan data directories and reconcile the difference between blocks in memory and on the disk.</span><br><span class="line">	Support multiple time unit suffix(case insensitive), as described</span><br><span class="line">	in dfs.heartbeat.interval.</span><br><span class="line">	&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>（3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。</p>
<p>（4）集群运行中可以安全加入和退出一些机器。</p>
<h2 id="6-2-数据完整性"><a href="#6-2-数据完整性" class="headerlink" title="6.2 数据完整性"></a>6.2 数据完整性</h2><p>思考：如果电脑磁盘里面存储的数据是控制高铁信号灯的红灯信号（1）和绿灯信号（0），但是存储该数据的磁盘坏了，一直显示是绿灯，是否很危险？同理DataNode节点上的数据损坏了，却没有发现，是否也很危险，那么如何解决呢？</p>
<p>如下是DataNode节点保证数据完整性的方法。</p>
<p>（1）当DataNode读取Block的时候，它会计算CheckSum。</p>
<p>（2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。</p>
<p>（3）Client读取其他DataNode上的Block。</p>
<p>（4）常见的校验算法crc（32），md5（128），sha1（160）</p>
<p>（5）DataNode在其文件创建后周期验证CheckSum。</p>
<p><img src="https://image.3001.net/images/20221104/1667534937148.png#crop=0&crop=0&crop=1&crop=1&id=aGQqu&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h2 id="6-3-掉线时限参数设置"><a href="#6-3-掉线时限参数设置" class="headerlink" title="6.3 掉线时限参数设置"></a>6.3 掉线时限参数设置</h2><p><img src="https://image.3001.net/images/20221104/16675349527892.png#crop=0&crop=0&crop=1&crop=1&id=cLQ2j&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;300000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yangmour.github.io/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/02_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88%E5%85%A5%E9%97%A8%EF%BC%89V3.3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E5%A4%B4%E5%83%8F.gif">
      <meta itemprop="name" content="希文">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="希文的个人博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/02_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88%E5%85%A5%E9%97%A8%EF%BC%89V3.3/" class="post-title-link" itemprop="url">02_尚硅谷大数据技术之Hadoop（入门）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-11-02 10:06:17" itemprop="dateCreated datePublished" datetime="2022-11-02T10:06:17+08:00">2022-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-10 17:32:34" itemprop="dateModified" datetime="2022-11-10T17:32:34+08:00">2022-11-10</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>25k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>23 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<p>尚硅谷大数据技术之Hadoop（入门）</p>
<p>（作者：尚硅谷大数据研发部）</p>
<p>版本：V3.3</p>
<h1 id="第1章-Hadoop概述"><a href="#第1章-Hadoop概述" class="headerlink" title="第1章 Hadoop概述"></a>第1章 Hadoop概述</h1><h2 id="1-1-Hadoop是什么"><a href="#1-1-Hadoop是什么" class="headerlink" title="1.1 Hadoop是什么"></a>1.1 Hadoop是什么</h2><p>                               <img src="https://image.3001.net/images/20221031/1667194025732.png#crop=0&crop=0&crop=1&crop=1&id=j5iH0&originHeight=885&originWidth=1598&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h2 id="1-2-Hadoop发展历史（了解）"><a href="#1-2-Hadoop发展历史（了解）" class="headerlink" title="1.2 Hadoop发展历史（了解）"></a>1.2 Hadoop发展历史（了解）</h2><p><img src="https://image.3001.net/images/20221031/1667194037334.png#crop=0&crop=0&crop=1&crop=1&id=evRgg&originHeight=888&originWidth=1545&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p><img src="https://image.3001.net/images/20221031/16671940571077.png#crop=0&crop=0&crop=1&crop=1&id=PXRXd&originHeight=887&originWidth=1579&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h2 id="1-3-Hadoop三大发行版本（了解）"><a href="#1-3-Hadoop三大发行版本（了解）" class="headerlink" title="1.3 Hadoop三大发行版本（了解）"></a>1.3 Hadoop三大发行版本（了解）</h2><p>Hadoop三大发行版本：Apache、Cloudera、Hortonworks。</p>
<p>Apache版本最原始（最基础）的版本，对于入门学习最好。2006</p>
<p>Cloudera内部集成了很多大数据框架，对应产品CDH。2008</p>
<p>Hortonworks文档较好，对应产品HDP。2011</p>
<p>Hortonworks现在已经被Cloudera公司收购，推出新的品牌CDP。</p>
<p><img src="https://image.3001.net/images/20221031/16671941257461.png#crop=0&crop=0&crop=1&crop=1&id=jfarw&originHeight=680&originWidth=1483&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p><img src="https://image.3001.net/images/20221031/16671941366585.png#crop=0&crop=0&crop=1&crop=1&id=tHCdw&originHeight=342&originWidth=1502&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p><strong>1</strong>）<strong>Apache Hadoop</strong></p>
<p>官网地址：<a target="_blank" rel="noopener" href="http://hadoop.apache.org/">http://hadoop.apache.org</a></p>
<p>下载地址：<a target="_blank" rel="noopener" href="https://hadoop.apache.org/releases.html">https://hadoop.apache.org/releases.html</a></p>
<p><strong>2</strong>）<strong>Cloudera Hadoop</strong></p>
<p>官网地址：<a target="_blank" rel="noopener" href="https://www.cloudera.com/downloads/cdh">https://www.cloudera.com/downloads/cdh</a></p>
<p>下载地址：<a target="_blank" rel="noopener" href="https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_cdh_6_download.html">https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_cdh_6_download.html</a></p>
<p>（1）2008年成立的Cloudera是最早将Hadoop商用的公司，为合作伙伴提供Hadoop的商用解决方案，主要是包括支持、咨询服务、培训。</p>
<p><strong>（2）</strong> <strong>2009</strong> 年<strong>Hadoop</strong>的创始人<strong>Doug Cutting</strong>也加盟<strong>Cloudera</strong>公司。Cloudera产品主要为CDH，Cloudera Manager，Cloudera Support</p>
<p>（3）CDH是Cloudera的Hadoop发行版，完全开源，比Apache Hadoop在兼容性，安全性，稳定性上有所增强。Cloudera的标价为每年每个节点<strong>10000</strong>美元。</p>
<p>（4）Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。</p>
<p><strong>3</strong>）<strong>Hortonworks Hadoop</strong></p>
<p>官网地址：<a target="_blank" rel="noopener" href="https://hortonworks.com/products/data-center/hdp/">https://hortonworks.com/products/data-center/hdp/</a></p>
<p>下载地址：<a target="_blank" rel="noopener" href="https://hortonworks.com/downloads/#data-platform">https://hortonworks.com/downloads/#data-platform</a></p>
<p>（1）2011年成立的Hortonworks是雅虎与硅谷风投公司Benchmark Capital合资组建。</p>
<p><strong>（2）</strong>公司成立之初就吸纳了大约<strong>25</strong>名至<strong>30</strong>名专门研究<strong>Hadoop</strong>的雅虎工程师，上述工程师均在<strong>2005</strong>年开始协助雅虎开发<strong>Hadoop</strong>，贡献了**Hadoop80%**的代码。</p>
<p>（3）Hortonworks的主打产品是Hortonworks Data Platform（HDP），也同样是100%开源的产品，HDP除常见的项目外还包括了<strong>Ambari</strong>，一款开源的安装和管理系统。</p>
<p>（4）2018年Hortonworks目前已经被<strong>Cloudera</strong>公司收购。</p>
<h2 id="1-4-Hadoop优势（4高）"><a href="#1-4-Hadoop优势（4高）" class="headerlink" title="1.4 Hadoop优势（4高）"></a>1.4 Hadoop优势（4高）</h2><p><img src="https://image.3001.net/images/20221031/16671943599655.png#crop=0&crop=0&crop=1&crop=1&id=e0K1k&originHeight=857&originWidth=1561&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p><img src="https://image.3001.net/images/20221031/16671943728508.png#crop=0&crop=0&crop=1&crop=1&id=EiMSY&originHeight=894&originWidth=1579&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h2 id="1-5-Hadoop组成（面试重点）"><a href="#1-5-Hadoop组成（面试重点）" class="headerlink" title="1.5 Hadoop组成（面试重点）"></a>1.5 Hadoop组成（面试重点）</h2><p><img src="https://image.3001.net/images/20221031/16671943927639.png#crop=0&crop=0&crop=1&crop=1&id=aqNTg&originHeight=768&originWidth=1576&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h3 id="1-5-1-HDFS架构概述"><a href="#1-5-1-HDFS架构概述" class="headerlink" title="1.5.1 HDFS架构概述"></a>1.5.1 HDFS架构概述</h3><p>Hadoop Distributed File System，简称_HDFS_，是一个分布式文件系统。</p>
<p><img src="https://image.3001.net/images/20221031/1667194405929.png#crop=0&crop=0&crop=1&crop=1&id=cBR9g&originHeight=854&originWidth=1577&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h3 id="1-5-2-YARN架构概述"><a href="#1-5-2-YARN架构概述" class="headerlink" title="1.5.2 YARN架构概述"></a>1.5.2 YARN架构概述</h3><p>Yet Another Resource Negotiator简称YARN ，另一种资源协调者，是Hadoop的资源管理器。</p>
<p><img src="https://image.3001.net/images/20221031/1667194420824.png#crop=0&crop=0&crop=1&crop=1&id=pSzkM&originHeight=861&originWidth=1564&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h3 id="1-5-3-MapReduce架构概述"><a href="#1-5-3-MapReduce架构概述" class="headerlink" title="1.5.3 MapReduce架构概述"></a>1.5.3 MapReduce架构概述</h3><p>MapReduce将计算过程分为两个阶段：Map和Reduce</p>
<p>1）Map阶段并行处理输入数据</p>
<p>2）Reduce阶段对Map结果进行汇总</p>
<p><img src="https://image.3001.net/images/20221031/16671944389897.png#crop=0&crop=0&crop=1&crop=1&id=xieVc&originHeight=864&originWidth=1602&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h3 id="1-5-4-HDFS、YARN、MapReduce三者关系"><a href="#1-5-4-HDFS、YARN、MapReduce三者关系" class="headerlink" title="1.5.4 HDFS、YARN、MapReduce三者关系"></a>1.5.4 HDFS、YARN、MapReduce三者关系</h3><p><img src="https://image.3001.net/images/20221031/16671944656951.png#crop=0&crop=0&crop=1&crop=1&id=OHgg4&originHeight=803&originWidth=1478&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h2 id="1-6-大数据技术生态体系"><a href="#1-6-大数据技术生态体系" class="headerlink" title="1.6 大数据技术生态体系"></a>1.6 大数据技术生态体系</h2><p><img src="https://image.3001.net/images/20221031/16671944773962.png#crop=0&crop=0&crop=1&crop=1&id=VtmhW&originHeight=792&originWidth=1432&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>图中涉及的技术名词解释如下：</p>
<p>1）Sqoop：Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库（MySQL）间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。</p>
<p>2）Flume：Flume是一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；</p>
<p>3）Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统；</p>
<p>4）Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。</p>
<p>5）Flink：Flink是当前最流行的开源大数据内存计算框架。用于实时计算的场景较多。</p>
<p>6）Oozie：Oozie是一个管理Hadoop作业（job）的工作流程调度管理系统。</p>
<p>7）Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。</p>
<p>8）Hive：Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</p>
<p>9）ZooKeeper：它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。</p>
<h2 id="1-7-推荐系统框架图"><a href="#1-7-推荐系统框架图" class="headerlink" title="1.7 推荐系统框架图"></a>1.7 推荐系统框架图</h2><p><img src="https://image.3001.net/images/20221031/16671945032468.png#crop=0&crop=0&crop=1&crop=1&id=KxPbX&originHeight=796&originWidth=1457&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h1 id="第2章-Hadoop运行环境搭建（开发重点）"><a href="#第2章-Hadoop运行环境搭建（开发重点）" class="headerlink" title="第2章 Hadoop运行环境搭建（开发重点）"></a>第2章 Hadoop运行环境搭建（开发重点）</h1><h2 id="2-1-模板虚拟机环境准备"><a href="#2-1-模板虚拟机环境准备" class="headerlink" title="2.1 模板虚拟机环境准备"></a>2.1 模板虚拟机环境准备</h2><p><strong>0）安装模板虚拟机，</strong>IP<strong>地址</strong>192.168.10.100<strong>、主机名称</strong>hadoop100<strong>、内存</strong>4G<strong>、硬盘</strong>50G**</p>
<p>02.1_尚硅谷大数据技术之模板虚拟机环境准备</p>
<p><strong>1</strong>）<strong>hadoop100</strong>虚拟机配置要求如下（本文<strong>Linux</strong>系统全部以<strong>CentOS-7.5-x86-1804</strong>为例）</p>
<p>（1）使用yum安装需要虚拟机可以正常上网，yum安装前可以先测试下虚拟机联网情况</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# ping www.baidu.com</span><br><span class="line">PING www.baidu.com (14.215.177.39) 56(84) bytes of data.</span><br><span class="line">64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=1 ttl=128 time=8.60 ms</span><br><span class="line">64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=2 ttl=128 time=7.72 ms</span><br></pre></td></tr></table></figure>

<p>（2）安装epel-release</p>
<p>注：Extra Packages for Enterprise Linux是为“红帽系”的操作系统提供额外的软件包，适用于RHEL、CentOS和Scientific Linux。相当于是一个软件仓库，大多数rpm包在官方 repository 中是找不到的）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# yum install -y epel-release</span><br></pre></td></tr></table></figure>

<p>（3）注意：如果Linux安装的是最小系统版，还需要安装如下工具；如果安装的是Linux桌面标准版，不需要执行如下操作</p>
<p>Ø net-tool：工具包集合，包含ifconfig等命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# yum install -y net-tools</span><br></pre></td></tr></table></figure>

<p>Ø vim：编辑器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# yum install -y vim</span><br></pre></td></tr></table></figure>

<p><strong>2</strong>）关闭防火墙，关闭防火墙开机自启</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# systemctl stop firewalld</span><br><span class="line">[root@hadoop100 ~]# systemctl disable firewalld.service</span><br></pre></td></tr></table></figure>

<p>    注意：在企业开发时，通常单个服务器的防火墙时关闭的。公司整体对外会设置非常安全的防火墙</p>
<p><strong>3）创建atguigu用户，并修改atguigu用户的密码</strong></p>
<p>[root<a href="/hadoop100">@hadoop100 </a> ~]# useradd atguigu </p>
<p>[root<a href="/hadoop100">@hadoop100 </a> ~]# passwd atguigu </p>
<p><strong>4</strong>）配置<strong>atguigu</strong>用户具有<strong>root</strong>权限，方便后期加<strong>sudo</strong>执行<strong>root</strong>权限的命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# vim /etc/sudoers</span><br></pre></td></tr></table></figure>

<p>修改/etc/sudoers文件，在%wheel这行下面添加一行，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">## Allow root to run any commands anywhere</span><br><span class="line">root    ALL=(ALL)     ALL</span><br><span class="line"></span><br><span class="line">## Allows people in group wheel to run all commands</span><br><span class="line">%wheel  ALL=(ALL)       ALL</span><br><span class="line">atguigu   ALL=(ALL)     NOPASSWD:ALL</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>：atguigu这一行不要直接放到root行下面，因为所有用户都属于wheel组，你先配置了atguigu具有免密功能，但是程序执行到%wheel行时，该功能又被覆盖回需要密码。所以atguigu要放到%wheel这行下面。</p>
<p><strong>5</strong>）在**/opt<strong>目录下创建文件夹，并修改所属主和所属组</strong></p>
<p>（1）在/opt目录下创建module、software文件夹</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# mkdir /opt/module</span><br><span class="line">[root@hadoop100 ~]# mkdir /opt/software</span><br></pre></td></tr></table></figure>

<p>    （2）修改module、software文件夹的所有者和所属组均为atguigu用户</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# chown atguigu:atguigu /opt/module </span><br><span class="line">[root@hadoop100 ~]# chown atguigu:atguigu /opt/software</span><br></pre></td></tr></table></figure>

<p>（3）查看module、software文件夹的所有者和所属组</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# cd /opt/</span><br><span class="line">[root@hadoop100 opt]# ll</span><br><span class="line">总用量 12</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu 4096 5月 28 17:18 module</span><br><span class="line">drwxr-xr-x. 2 root  root  4096 9月  7 2017 rh</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu 4096 5月 28 17:18 software</span><br></pre></td></tr></table></figure>

<p><strong>6</strong>）卸载虚拟机自带的JDK</p>
<p>    注意：如果你的虚拟机是最小化安装不需要执行这一步。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# rpm -qa | grep -i java | xargs -n1 rpm -e --nodeps</span><br></pre></td></tr></table></figure>

<p>Ø rpm -qa：查询所安装的所有rpm软件包</p>
<p>Ø grep -i：忽略大小写</p>
<p>Ø xargs -n1：表示每次只传递一个参数</p>
<p>Ø rpm -e –nodeps：强制卸载软件</p>
<p><strong>7</strong>）重启虚拟机</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# reboot</span><br></pre></td></tr></table></figure>

<h2 id="2-2-克隆虚拟机"><a href="#2-2-克隆虚拟机" class="headerlink" title="2.2 克隆虚拟机"></a>2.2 克隆虚拟机</h2><p><strong>1</strong>）利用模板机hadoop100**，克隆三台虚拟机：hadoop102 hadoop103 hadoop104</p>
<p>    注意：克隆时，要先关闭hadoop100</p>
<p><strong>2）修改克隆机IP，以下以hadoop102举例说明</strong></p>
<p>（1）修改克隆虚拟机的静态IP</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br></pre></td></tr></table></figure>

<p>改成</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=ens33</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">ONBOOT=yes</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">NAME=&quot;ens33&quot;</span><br><span class="line">IPADDR=192.168.10.102</span><br><span class="line">PREFIX=24</span><br><span class="line">GATEWAY=192.168.10.2</span><br><span class="line">DNS1=192.168.10.2</span><br></pre></td></tr></table></figure>

<p>（2）查看Linux虚拟机的虚拟网络编辑器，编辑-&gt;虚拟网络编辑器-&gt;VMnet8</p>
<p>     <img src="https://image.3001.net/images/20221031/16671951899900.png#crop=0&crop=0&crop=1&crop=1&id=HSsXD&originHeight=458&originWidth=527&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p><img src="https://image.3001.net/images/20221031/16671952029218.png#crop=0&crop=0&crop=1&crop=1&id=N4xIT&originHeight=485&originWidth=457&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>（3）查看Windows系统适配器VMware Network Adapter VMnet8的IP地址</p>
<p>     <img src="https://image.3001.net/images/20221031/16671952098350.png#crop=0&crop=0&crop=1&crop=1&id=P3sFx&originHeight=581&originWidth=461&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>（4）保证Linux系统ifcfg-ens33文件中IP地址、虚拟网络编辑器地址和Windows系统VM8网络IP地址相同。</p>
<p><strong>3）修改克隆机主机名，以下以hadoop102举例说明</strong></p>
<p>    （1）修改主机名称</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# vim /etc/hostname</span><br><span class="line">hadoop102</span><br></pre></td></tr></table></figure>

<p>（2）配置Linux克隆机主机名称映射hosts文件，打开/etc/hosts</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# vim /etc/hosts</span><br></pre></td></tr></table></figure>

<p>添加如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">192.168.10.100 hadoop100</span><br><span class="line">192.168.10.101 hadoop101</span><br><span class="line">192.168.10.102 hadoop102</span><br><span class="line">192.168.10.103 hadoop103</span><br><span class="line">192.168.10.104 hadoop104</span><br><span class="line">192.168.10.105 hadoop105</span><br><span class="line">192.168.10.106 hadoop106</span><br><span class="line">192.168.10.107 hadoop107</span><br><span class="line">192.168.10.108 hadoop108</span><br></pre></td></tr></table></figure>

<p><strong>4</strong>）重启克隆机hadoop102</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# reboot</span><br></pre></td></tr></table></figure>

<p><strong>5</strong>）修改<strong>windows</strong>的主机映射文件（<strong>hosts</strong>文件）</p>
<p>（1）如果操作系统是window7，可以直接修改</p>
<p>    （a）进入<strong>C:\Windows\System32\drivers\etc</strong>路径</p>
<p>    （b）打开hosts文件并添加如下内容，然后保存</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">192.168.10.100 hadoop100</span><br><span class="line">192.168.10.101 hadoop101</span><br><span class="line">192.168.10.102 hadoop102</span><br><span class="line">192.168.10.103 hadoop103</span><br><span class="line">192.168.10.104 hadoop104</span><br><span class="line">192.168.10.105 hadoop105</span><br><span class="line">192.168.10.106 hadoop106</span><br><span class="line">192.168.10.107 hadoop107</span><br><span class="line">192.168.10.108 hadoop108</span><br></pre></td></tr></table></figure>

<p>（2）如果操作系统是window10，先拷贝出来，修改保存以后，再覆盖即可</p>
<p>（a）进入<strong>C:\Windows\System32\drivers\etc</strong>路径</p>
<p>（b）拷贝hosts文件到桌面</p>
<p>（c）打开桌面hosts文件并添加如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">192.168.10.100 hadoop100</span><br><span class="line">192.168.10.101 hadoop101</span><br><span class="line">192.168.10.102 hadoop102</span><br><span class="line">192.168.10.103 hadoop103</span><br><span class="line">192.168.10.104 hadoop104</span><br><span class="line">192.168.10.105 hadoop105</span><br><span class="line">192.168.10.106 hadoop106</span><br><span class="line">192.168.10.107 hadoop107</span><br><span class="line">192.168.10.108 hadoop108</span><br></pre></td></tr></table></figure>

<p>（d）将桌面hosts文件覆盖<strong>C:\Windows\System32\drivers\etc</strong>路径hosts文件</p>
<h2 id="2-3-在hadoop102安装JDK"><a href="#2-3-在hadoop102安装JDK" class="headerlink" title="2.3 在hadoop102安装JDK"></a>2.3 在hadoop102安装JDK</h2><p><strong>1</strong>）卸载现有<strong>JDK</strong></p>
<p>注意：安装JDK前，一定确保提前删除了虚拟机自带的JDK。详细步骤见问文档3.1节中卸载JDK步骤。</p>
<p><strong>2</strong>）用XShell传输工具将JDK导入到op目录下面的software文件夹下面**</p>
<p>     <img src="https://image.3001.net/images/20221031/1667195379816.png#crop=0&crop=0&crop=1&crop=1&id=dEdTT&originHeight=845&originWidth=1234&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p><strong>3</strong>）在Linux系统下的opt目录中查看软件包是否导入成功</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ ls /opt/software/</span><br></pre></td></tr></table></figure>

<p>看到如下结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jdk-8u212-linux-x64.tar.gz</span><br></pre></td></tr></table></figure>

<p><strong>4</strong>）解压JDK到/opt/module目录下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p><strong>5</strong>）配置JDK环境变量</p>
<p>    （1）新建/etc/profile.d/my_env.sh文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>添加如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#JAVA_HOME</span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure>

<p>    （2）保存后退出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:wq</span><br></pre></td></tr></table></figure>

<p>    （3）source一下/etc/profile文件，让新的环境变量PATH生效</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ source /etc/profile</span><br></pre></td></tr></table></figure>

<p><strong>6）测试JDK是否安装成功</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ java -version</span><br></pre></td></tr></table></figure>

<p>如果能看到以下结果，则代表Java安装成功。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java version &quot;1.8.0_212&quot;</span><br></pre></td></tr></table></figure>

<p>注意：重启（如果java -version可以用就不用重启）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo reboot</span><br></pre></td></tr></table></figure>

<h2 id="2-4-在hadoop102安装Hadoop"><a href="#2-4-在hadoop102安装Hadoop" class="headerlink" title="2.4 在hadoop102安装Hadoop"></a>2.4 在hadoop102安装Hadoop</h2><p>Hadoop下载地址：<a target="_blank" rel="noopener" href="https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/">https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/</a></p>
<p><strong>1</strong>）用XShell文件传输工具将hadoop-3.1.3.tar.gz导入到opt目录下面的software文件夹下面</p>
<p>     <img src="https://image.3001.net/images/20221104/16675228951696.png#crop=0&crop=0&crop=1&crop=1&id=K6Rig&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p><strong>2</strong>）进入到Hadoop安装包路径下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ cd /opt/software/</span><br></pre></td></tr></table></figure>

<p><strong>3</strong>）解压安装文件到/opt/module下面</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p><strong>4</strong>）查看是否解压成功</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ ls /opt/module/</span><br><span class="line">hadoop-3.1.3</span><br></pre></td></tr></table></figure>

<p><strong>5</strong>）将Hadoop添加到环境变量</p>
<p>    （1）获取Hadoop安装路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ pwd</span><br><span class="line">/opt/module/hadoop-3.1.3</span><br></pre></td></tr></table></figure>

<p>    （2）打开/etc/profile.d/my_env.sh文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ sudo vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>Ø 在my_env.sh文件末尾添加如下内容：（shift+g）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#HADOOP_HOME</span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<p>Ø 保存并退出： :wq</p>
<p>    （3）让修改后的文件生效</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ source /etc/profile</span><br></pre></td></tr></table></figure>

<p><strong>6</strong>）测试是否安装成功</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop version</span><br><span class="line">Hadoop 3.1.3</span><br></pre></td></tr></table></figure>

<p><strong>7</strong>）重启（如果Hadoop命令不能用再重启虚拟机）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ sudo reboot</span><br></pre></td></tr></table></figure>

<h2 id="2-5-Hadoop目录结构"><a href="#2-5-Hadoop目录结构" class="headerlink" title="2.5 Hadoop目录结构"></a>2.5 Hadoop目录结构</h2><p><strong>1</strong>）查看Hadoop目录结构</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ ll</span><br><span class="line">总用量 52</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 bin</span><br><span class="line">drwxr-xr-x. 3 atguigu atguigu 4096 5月 22 2017 etc</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 include</span><br><span class="line">drwxr-xr-x. 3 atguigu atguigu 4096 5月 22 2017 lib</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 libexec</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu 15429 5月 22 2017 LICENSE.txt</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu  101 5月 22 2017 NOTICE.txt</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu 1366 5月 22 2017 README.txt</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 sbin</span><br><span class="line">drwxr-xr-x. 4 atguigu atguigu 4096 5月 22 2017 share</span><br></pre></td></tr></table></figure>

<p><strong>2</strong>）重要目录</p>
<p>（1）bin目录：存放对Hadoop相关服务（hdfs，yarn，mapred）进行操作的脚本</p>
<p>（2）etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件</p>
<p>（3）lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能）</p>
<p>（4）sbin目录：存放启动或停止Hadoop相关服务的脚本</p>
<p>（5）share目录：存放Hadoop的依赖jar包、文档、和官方案例</p>
<h1 id="第3章-Hadoop运行模式"><a href="#第3章-Hadoop运行模式" class="headerlink" title="第3章 Hadoop运行模式"></a>第3章 Hadoop运行模式</h1><p>1）Hadoop官方网站：<a target="_blank" rel="noopener" href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></p>
<p>2）Hadoop运行模式包括：<strong>本地模式</strong>、<strong>伪分布式模式</strong>以及<strong>完全分布式模式</strong>。</p>
<p>Ø <strong>本地模式</strong>：单机运行，只是用来演示一下官方案例。生产环境不用。</p>
<p>Ø <strong>伪分布式模式：</strong>也是单机运行，但是具备Hadoop集群的所有功能，一台服务器模拟一个分布式的环境。个别缺钱的公司用来测试，生产环境不用。</p>
<p>Ø <strong>完全分布式模式：</strong>多台服务器组成分布式环境。生产环境使用。</p>
<h2 id="3-1-本地运行模式（官方WordCount）"><a href="#3-1-本地运行模式（官方WordCount）" class="headerlink" title="3.1 本地运行模式（官方WordCount）"></a>3.1 本地运行模式（官方WordCount）</h2><p><strong>1</strong>）创建在hadoop-3.1.3文件下面创建一个wcinput文件夹</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ mkdir wcinput</span><br></pre></td></tr></table></figure>

<p><strong>2</strong>）在wcinput文件下创建一个word.txt文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ cd wcinput</span><br></pre></td></tr></table></figure>

<p><strong>3</strong>）编辑word.txt文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 wcinput]$ vim word.txt</span><br></pre></td></tr></table></figure>

<p>Ø 在文件中输入如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop yarn</span><br><span class="line">hadoop mapreduce</span><br><span class="line">atguigu</span><br><span class="line">atguigu</span><br></pre></td></tr></table></figure>

<p>Ø 保存退出：:wq</p>
<p><strong>4</strong>）回到Hadoop目录/opt/module/hadoop-3.1.3</p>
<p><strong>5</strong>）执行程序</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput wcoutput</span><br></pre></td></tr></table></figure>

<p><strong>6</strong>）查看结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ cat wcoutput/part-r-00000</span><br></pre></td></tr></table></figure>

<p>看到如下结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">atguigu 2</span><br><span class="line">hadoop 2</span><br><span class="line">mapreduce    1</span><br><span class="line">yarn  1</span><br></pre></td></tr></table></figure>

<h2 id="3-2-完全分布式运行模式（开发重点）"><a href="#3-2-完全分布式运行模式（开发重点）" class="headerlink" title="3.2 完全分布式运行模式（开发重点）"></a>3.2 完全分布式运行模式（开发重点）</h2><p>分析：</p>
<p>    1）准备3台客户机（关闭防火墙、静态IP、主机名称）</p>
<p>    2）安装JDK</p>
<p>    3）配置环境变量</p>
<p>    4）安装Hadoop</p>
<p>    5）配置环境变量</p>
<pre><code>6）配置集群
</code></pre>
<p>    7）单点启动</p>
<p>    8）配置ssh</p>
<p>    9）群起并测试集群</p>
<h3 id="3-2-1-虚拟机准备"><a href="#3-2-1-虚拟机准备" class="headerlink" title="3.2.1 虚拟机准备"></a>3.2.1 虚拟机准备</h3><p>详见2.1、2.2两节。</p>
<h3 id="3-2-2-编写集群分发脚本xsync"><a href="#3-2-2-编写集群分发脚本xsync" class="headerlink" title="3.2.2 编写集群分发脚本xsync"></a>3.2.2 编写集群分发脚本xsync</h3><p><strong>1</strong>）scp（secure copy）安全拷贝</p>
<p>（1）scp定义</p>
<p>scp可以实现服务器与服务器之间的数据拷贝。（from server1 to server2）</p>
<p>    （2）基本语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp   -r    $pdir/$fname       $user@$host:$pdir/$fname</span><br></pre></td></tr></table></figure>

<p>命令  递归   要拷贝的文件路径/名称  目的地用户@主机:目的地路径/名称</p>
<p>（3）案例实操</p>
<pre><code>Ø 前提：在hadoop102、hadoop103、hadoop104都已经创建好的/opt/module、      /opt/software两个目录，并且已经把这两个目录修改为atguigu:atguigu
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo chown atguigu:atguigu -R /opt/module</span><br></pre></td></tr></table></figure>

<pre><code>（a）在hadoop102上，将hadoop102中/opt/module/jdk1.8.0_212目录拷贝到hadoop103上。
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ scp -r /opt/module/jdk1.8.0_212 atguigu@hadoop103:/opt/module</span><br></pre></td></tr></table></figure>

<pre><code>（b）在hadoop103上，将hadoop102中/opt/module/hadoop-3.1.3目录拷贝到hadoop103上。
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ scp -r atguigu@hadoop102:/opt/module/hadoop-3.1.3 /opt/module/</span><br></pre></td></tr></table></figure>

<pre><code>（c）在hadoop103上操作，将hadoop102中/opt/module目录下所有目录拷贝到hadoop104上。
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 opt]$ scp -r atguigu@hadoop102:/opt/module/* atguigu@hadoop104:/opt/module</span><br></pre></td></tr></table></figure>

<p><strong>2</strong>）rsync远程同步工具</p>
<p>rsync主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。</p>
<p>rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去。</p>
<p>    （1）基本语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rsync   -av    $pdir/$fname       $user@$host:$pdir/$fname</span><br></pre></td></tr></table></figure>

<p>命令  选项参数  要拷贝的文件路径/名称  目的地用户@主机:目的地路径/名称</p>
<p>     选项参数说明</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>归档拷贝</td>
</tr>
<tr>
<td>-v</td>
<td>显示复制过程</td>
</tr>
</tbody></table>
<p>（2）案例实操</p>
<p>    （a）删除hadoop103中/opt/module/hadoop-3.1.3/wcinput</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ rm -rf wcinput/</span><br></pre></td></tr></table></figure>

<p>    （b）同步hadoop102中的/opt/module/hadoop-3.1.3到hadoop103</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ rsync -av hadoop-3.1.3/ atguigu@hadoop103:/opt/module/hadoop-3.1.3/</span><br></pre></td></tr></table></figure>

<p><strong>3</strong>）xsync集群分发脚本</p>
<p>（1）需求：循环复制文件到所有节点的相同目录下</p>
<p>（2）需求分析：</p>
<p>（a）rsync命令原始拷贝：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rsync -av   /opt/module     atguigu@hadoop103:/opt/</span><br></pre></td></tr></table></figure>

<p>（b）期望脚本：</p>
<p>xsync要同步的文件名称</p>
<p>（c）期望脚本在任何路径都能使用（脚本放在声明了全局环境变量的路径）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ echo $PATH</span><br><span class="line">/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/atguigu/.local/bin:/home/atguigu/bin:/opt/module/jdk1.8.0_212/bin</span><br></pre></td></tr></table></figure>

<p>（3）脚本实现</p>
<p>（a）在/home/atguigu/bin目录下创建xsync文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 opt]$ cd /home/atguigu</span><br><span class="line">[atguigu@hadoop102 ~]$ mkdir bin</span><br><span class="line">[atguigu@hadoop102 ~]$ cd bin</span><br><span class="line">[atguigu@hadoop102 bin]$ vim xsync</span><br></pre></td></tr></table></figure>

<p>在该文件中编写如下代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">#1. 判断参数个数</span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo Not Enough Arguement!</span><br><span class="line">    exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#2. 遍历集群所有机器</span><br><span class="line">for host in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">    echo ====================  $host  ====================</span><br><span class="line">    #3. 遍历所有目录，挨个发送</span><br><span class="line"></span><br><span class="line">    for file in $@</span><br><span class="line">    do</span><br><span class="line">        #4. 判断文件是否存在</span><br><span class="line">        if [ -e $file ]</span><br><span class="line">            then</span><br><span class="line">                #5. 获取父目录</span><br><span class="line">                pdir=$(cd -P $(dirname $file); pwd)</span><br><span class="line"></span><br><span class="line">                #6. 获取当前文件的名称</span><br><span class="line">                fname=$(basename $file)</span><br><span class="line">                ssh $host &quot;mkdir -p $pdir&quot;</span><br><span class="line">                rsync -av $pdir/$fname $host:$pdir</span><br><span class="line">            else</span><br><span class="line">                echo $file does not exists!</span><br><span class="line">        fi</span><br><span class="line">    done</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>（b）修改脚本 xsync 具有执行权限</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ chmod +x xsync</span><br></pre></td></tr></table></figure>

<p>（c）测试脚本</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ xsync /home/atguigu/bin</span><br></pre></td></tr></table></figure>

<p>（d）将脚本复制到/bin中，以便全局调用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ sudo cp xsync /bin/</span><br></pre></td></tr></table></figure>

<p>（e）同步环境变量配置（root所有者）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo ./bin/xsync /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>注意：如果用了sudo，那么xsync一定要给它的路径补全。</p>
<p>让环境变量生效</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 bin]$ source /etc/profile</span><br><span class="line">[atguigu@hadoop104 opt]$ source /etc/profile</span><br></pre></td></tr></table></figure>

<h3 id="3-2-3-SSH无密登录配置"><a href="#3-2-3-SSH无密登录配置" class="headerlink" title="3.2.3 SSH无密登录配置"></a>3.2.3 SSH无密登录配置</h3><p><strong>1</strong>）配置ssh</p>
<p>（1）基本语法</p>
<p>ssh另一台电脑的IP地址</p>
<p>（2）ssh连接时出现Host key verification failed的解决方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ ssh hadoop103</span><br></pre></td></tr></table></figure>

<p>Ø 如果出现如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Are you sure you want to continue connecting (yes/no)?</span><br></pre></td></tr></table></figure>

<p>Ø 输入yes，并回车</p>
<p>（3）退回到hadoop102</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ exit</span><br></pre></td></tr></table></figure>

<p><strong>2）无密钥配置</strong></p>
<p>（1）免密登录原理</p>
<p><img src="https://image.3001.net/images/20221104/16675258082005.png#crop=0&crop=0&crop=1&crop=1&id=zfcux&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>（2）生成公钥和私钥</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 .ssh]$ pwd</span><br><span class="line">/home/atguigu/.ssh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>

<p>然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）</p>
<p>（3）将公钥拷贝到要免密登录的目标机器上</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop102</span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop103</span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop104</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<p>还需要在hadoop102上采用atguigu账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。</p>
<p>还需要在hadoop103上采用atguigu账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。</p>
<p>还需要在hadoop104上采用atguigu账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。</p>
<p>还需要在hadoop102上采用root账号，配置一下无密登录到hadoop102、hadoop103、hadoop104；</p>
<p><strong>3</strong>）.ssh文件夹下（~/.ssh）的文件功能解释**</p>
<table>
<thead>
<tr>
<th>known_hosts</th>
<th>记录ssh访问过计算机的公钥（public  key）</th>
</tr>
</thead>
<tbody><tr>
<td>id_rsa</td>
<td>生成的私钥</td>
</tr>
<tr>
<td>id_rsa.pub</td>
<td>生成的公钥</td>
</tr>
<tr>
<td>authorized_keys</td>
<td>存放授权过的无密登录服务器公钥</td>
</tr>
</tbody></table>
<h3 id="3-2-4-集群配置"><a href="#3-2-4-集群配置" class="headerlink" title="3.2.4 集群配置"></a>3.2.4 集群配置</h3><p><strong>1）集群部署规划</strong></p>
<p>     注意：</p>
<p>Ø NameNode和SecondaryNameNode不要安装在同一台服务器</p>
<p>Ø ResourceManager也很消耗内存，不要和NameNode、SecondaryNameNode配置在同一台机器上。</p>
<table>
<thead>
<tr>
<th></th>
<th>hadoop102</th>
<th>hadoop103</th>
<th>hadoop104</th>
</tr>
</thead>
<tbody><tr>
<td>HDFS</td>
<td>NameNode  DataNode</td>
<td>DataNode</td>
<td>SecondaryNameNode  DataNode</td>
</tr>
<tr>
<td>YARN</td>
<td>NodeManager</td>
<td>ResourceManager  NodeManager</td>
<td>NodeManager</td>
</tr>
</tbody></table>
<p><strong>2</strong>）配置文件说明</p>
<p>Hadoop配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。</p>
<p>（1）默认配置文件：</p>
<table>
<thead>
<tr>
<th>要获取的默认文件</th>
<th>文件存放在Hadoop的jar包中的位置</th>
</tr>
</thead>
<tbody><tr>
<td>[core-default.xml]</td>
<td>hadoop-common-3.1.3.jar/core-default.xml</td>
</tr>
<tr>
<td>[hdfs-default.xml]</td>
<td>hadoop-hdfs-3.1.3.jar/hdfs-default.xml</td>
</tr>
<tr>
<td>[yarn-default.xml]</td>
<td>hadoop-yarn-common-3.1.3.jar/yarn-default.xml</td>
</tr>
<tr>
<td>[mapred-default.xml]</td>
<td>hadoop-mapreduce-client-core-3.1.3.jar/mapred-default.xml</td>
</tr>
</tbody></table>
<p>（2）自定义配置文件：</p>
<p>    <strong>core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml</strong>四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置。</p>
<p><strong>3）配置集群</strong></p>
<p>（1）核心配置文件</p>
<p>配置core-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ cd $HADOOP_HOME/etc/hadoop</span><br><span class="line">[atguigu@hadoop102 hadoop]$ vim core-site.xml</span><br></pre></td></tr></table></figure>

<p>文件内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定NameNode的地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop102:8020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 指定hadoop数据的存储目录 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/opt/module/hadoop-3.1.3/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>（2）HDFS配置文件</p>
<p>配置hdfs-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ vim hdfs-site.xml</span><br></pre></td></tr></table></figure>

<p>文件内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;!-- nn web端访问地址--&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop102:9870&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">	&lt;!-- 2nn web端访问地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop104:9868&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>（3）YARN配置文件</p>
<p>配置yarn-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ vim yarn-site.xml</span><br></pre></td></tr></table></figure>

<p>文件内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定MR走shuffle --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 指定ResourceManager的地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop103&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 环境变量的继承 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>（4）MapReduce配置文件</p>
<p>配置mapred-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ vim mapred-site.xml</span><br></pre></td></tr></table></figure>

<p>文件内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p><strong>4</strong>）在集群上分发配置好的Hadoop配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ xsync /opt/module/hadoop-3.1.3/etc/hadoop/</span><br></pre></td></tr></table></figure>

<p><strong>5</strong>）去103和104上查看文件分发情况</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ cat /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml</span><br><span class="line">[atguigu@hadoop104 ~]$ cat /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml</span><br></pre></td></tr></table></figure>

<h3 id="3-2-5-群起集群"><a href="#3-2-5-群起集群" class="headerlink" title="3.2.5 群起集群"></a>3.2.5 群起集群</h3><p><strong>1</strong>）配置workers</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ vim /opt/module/hadoop-3.1.3/etc/hadoop/workers</span><br></pre></td></tr></table></figure>

<p>在该文件中增加如下内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure>

<p>注意：该文件中添加的内容结尾不允许有空格，文件中不允许有空行。</p>
<p>同步所有节点配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ xsync /opt/module/hadoop-3.1.3/etc</span><br></pre></td></tr></table></figure>

<p><strong>2</strong>）启动集群</p>
<p>    （1）<strong>如果集群是第一次启动</strong>，需要在hadoop102节点格式化NameNode（注意：格式化NameNode，会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到已往数据。如果集群在运行过程中报错，需要重新格式化NameNode的话，一定要先停止namenode和datanode进程，并且要删除所有机器的data和logs目录，然后再进行格式化。）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hdfs namenode -format</span><br></pre></td></tr></table></figure>

<p>（2）启动HDFS</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<p>（3）在配置了ResourceManager的节点（hadoop103）启动YARN</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<p>（4）Web端查看HDFS的NameNode</p>
<p>（a）浏览器中输入：<a target="_blank" rel="noopener" href="http://hadoop102:9870/">http://hadoop102:9870</a></p>
<p>        （b）查看HDFS上存储的数据信息</p>
<p>（5）Web端查看YARN的ResourceManager</p>
<p>（a）浏览器中输入：<a target="_blank" rel="noopener" href="http://hadoop103:8088/">http://hadoop103:8088</a></p>
<p>    （b）查看YARN上运行的Job信息</p>
<p><strong>3</strong>）集群基本测试</p>
<p>（1）上传文件到集群</p>
<p>Ø 上传小文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ hadoop fs -mkdir /input</span><br><span class="line">[atguigu@hadoop102 ~]$ hadoop fs -put $HADOOP_HOME/wcinput/word.txt /input</span><br></pre></td></tr></table></figure>

<p>Ø  上传大文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ hadoop fs -put  /opt/software/jdk-8u212-linux-x64.tar.gz  /</span><br></pre></td></tr></table></figure>

<p>（2）上传文件后查看文件存放在什么位置</p>
<p>Ø 查看HDFS文件存储路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 subdir0]$ pwd</span><br><span class="line">/opt/module/hadoop-3.1.3/data/dfs/data/current/BP-1436128598-192.168.10.102-1610603650062/current/finalized/subdir0/subdir0</span><br></pre></td></tr></table></figure>

<p>Ø 查看HDFS在磁盘存储文件内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 subdir0]$ cat blk_1073741825</span><br><span class="line">hadoop yarn</span><br><span class="line">hadoop mapreduce </span><br><span class="line">atguigu</span><br><span class="line">atguigu</span><br></pre></td></tr></table></figure>

<p>（3）拼接</p>
<p>-rw-rw-r–. 1 atguigu atguigu 134217728 5月 23 16:01 <strong>blk_1073741836</strong></p>
<p>-rw-rw-r–. 1 atguigu atguigu  1048583 5月 23 16:01 blk_1073741836_1012.meta</p>
<p>-rw-rw-r–. 1 atguigu atguigu 63439959 5月 23 16:01 <strong>blk_1073741837</strong></p>
<p>-rw-rw-r–. 1 atguigu atguigu  495635 5月 23 16:01 blk_1073741837_1013.meta</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 subdir0]$ cat blk_1073741836&gt;&gt;tmp.tar.gz</span><br><span class="line">[atguigu@hadoop102 subdir0]$ cat blk_1073741837&gt;&gt;tmp.tar.gz</span><br><span class="line">[atguigu@hadoop102 subdir0]$ tar -zxvf tmp.tar.gz</span><br></pre></td></tr></table></figure>

<p>（4）下载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 software]$ hadoop fs -get /jdk-8u212-linux-x64.tar.gz ./</span><br></pre></td></tr></table></figure>

<p>（5）执行wordcount程序</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output</span><br></pre></td></tr></table></figure>

<h3 id="3-2-6-配置历史服务器"><a href="#3-2-6-配置历史服务器" class="headerlink" title="3.2.6 配置历史服务器"></a>3.2.6 配置历史服务器</h3><p>为了查看程序的历史运行情况，需要配置一下历史服务器。具体配置步骤如下：</p>
<p><strong>1</strong>）配置mapred-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ vim mapred-site.xml</span><br></pre></td></tr></table></figure>

<p>在该文件里面增加如下配置。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 历史服务器端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop102:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 历史服务器web端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop102:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p><strong>2</strong>）分发配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ xsync $HADOOP_HOME/etc/hadoop/mapred-site.xml</span><br></pre></td></tr></table></figure>

<p><strong>3</strong>）在hadoop102启动历史服务器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ mapred --daemon start historyserver</span><br></pre></td></tr></table></figure>

<p><strong>4</strong>）查看历史服务器是否启动</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ jps</span><br></pre></td></tr></table></figure>

<p><strong>5</strong>）查看JobHistory</p>
<p><a target="_blank" rel="noopener" href="http://hadoop102:19888/jobhistory">http://hadoop102:19888/jobhistory</a></p>
<h3 id="3-2-7-配置日志的聚集"><a href="#3-2-7-配置日志的聚集" class="headerlink" title="3.2.7 配置日志的聚集"></a>3.2.7 配置日志的聚集</h3><p>日志聚集概念：应用运行完成以后，将程序运行日志信息上传到HDFS系统上。</p>
<p>     <img src="https://image.3001.net/images/20221104/16675324111724.png#crop=0&crop=0&crop=1&crop=1&id=UW5mV&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>日志聚集功能好处：可以方便的查看到程序运行详情，方便开发调试。</p>
<pre><code>注意：开启日志聚集功能，需要重新启动NodeManager 、ResourceManager和HistoryServer。
</code></pre>
<p>开启日志聚集功能具体步骤如下：</p>
<p><strong>1</strong>）配置yarn-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ vim yarn-site.xml</span><br></pre></td></tr></table></figure>

<p>在该文件里面增加如下配置。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 开启日志聚集功能 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 设置日志聚集服务器地址 --&gt;</span><br><span class="line">&lt;property&gt;  </span><br><span class="line">    &lt;name&gt;yarn.log.server.url&lt;/name&gt;  </span><br><span class="line">    &lt;value&gt;http://hadoop102:19888/jobhistory/logs&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 设置日志保留时间为7天 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p><strong>2</strong>）分发配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ xsync $HADOOP_HOME/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure>

<p><strong>3</strong>）关闭NodeManager、ResourceManager和HistoryServer</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ sbin/stop-yarn.sh</span><br><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ mapred --daemon stop historyserver</span><br></pre></td></tr></table></figure>

<p><strong>4</strong>）启动NodeManager 、ResourceManage和HistoryServer</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ start-yarn.sh</span><br><span class="line">[atguigu@hadoop102 ~]$ mapred --daemon start historyserver</span><br></pre></td></tr></table></figure>

<p><strong>5</strong>）删除HDFS上已经存在的输出文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ hadoop fs -rm -r /output</span><br></pre></td></tr></table></figure>

<p><strong>6</strong>）执行WordCount程序</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output</span><br></pre></td></tr></table></figure>

<p><strong>7</strong>）查看日志</p>
<p>    （1）历史服务器地址</p>
<p><a target="_blank" rel="noopener" href="http://hadoop102:19888/jobhistory">http://hadoop102:19888/jobhistory</a></p>
<p>    （2）历史任务列表</p>
<p>     <img src="https://image.3001.net/images/20221104/16675325708222.png#crop=0&crop=0&crop=1&crop=1&id=L7UIu&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>    （3）查看任务运行日志</p>
<p>     <img src="https://image.3001.net/images/20221104/16675325798812.png#crop=0&crop=0&crop=1&crop=1&id=rnxrT&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>    （4）运行日志详情</p>
<p>     <img src="https://image.3001.net/images/20221104/16675325889825.png#crop=0&crop=0&crop=1&crop=1&id=ZpUju&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h3 id="3-2-8-集群启动-停止方式总结"><a href="#3-2-8-集群启动-停止方式总结" class="headerlink" title="3.2.8 集群启动/停止方式总结"></a>3.2.8 集群启动/停止方式总结</h3><p><strong>1</strong>）各个模块分开启动/停止（配置ssh是前提）常用</p>
<p>    （1）整体启动/停止HDFS</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh/stop-dfs.sh</span><br></pre></td></tr></table></figure>

<p>    （2）整体启动/停止YARN</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh/stop-yarn.sh</span><br></pre></td></tr></table></figure>

<p><strong>2）各个服务组件逐一启动/停止</strong></p>
<p>    （1）分别启动/停止HDFS组件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs --daemon start/stop namenode/datanode/secondarynamenode</span><br></pre></td></tr></table></figure>

<p>    （2）启动/停止YARN</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn --daemon start/stop resourcemanager/nodemanager</span><br></pre></td></tr></table></figure>

<h3 id="3-2-9-编写Hadoop集群常用脚本"><a href="#3-2-9-编写Hadoop集群常用脚本" class="headerlink" title="3.2.9 编写Hadoop集群常用脚本"></a>3.2.9 编写Hadoop集群常用脚本</h3><p><strong>1</strong>）Hadoop集群启停脚本（包含HDFS，Yarn，Historyserver）：myhadoop.sh</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ cd /home/atguigu/bin</span><br><span class="line">[atguigu@hadoop102 bin]$ vim myhadoop.sh</span><br></pre></td></tr></table></figure>

<p>Ø 输入如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; =================== 启动 hadoop集群 ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot; --------------- 启动 hdfs ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/sbin/start-dfs.sh&quot;</span><br><span class="line">        echo &quot; --------------- 启动 yarn ---------------&quot;</span><br><span class="line">        ssh hadoop103 &quot;/opt/module/hadoop-3.1.3/sbin/start-yarn.sh&quot;</span><br><span class="line">        echo &quot; --------------- 启动 historyserver ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; =================== 关闭 hadoop集群 ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot; --------------- 关闭 historyserver ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver&quot;</span><br><span class="line">        echo &quot; --------------- 关闭 yarn ---------------&quot;</span><br><span class="line">        ssh hadoop103 &quot;/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh&quot;</span><br><span class="line">        echo &quot; --------------- 关闭 hdfs ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>Ø 保存后退出，然后赋予脚本执行权限</p>
<p>[atguigu<a href="/hadoop102">@hadoop102 </a> bin]$ chmod +x myhadoop.sh </p>
<p><strong>2）查看三台服务器Java进程脚本：jpsall</strong></p>
<p>[atguigu<a href="/hadoop102">@hadoop102 </a> ~]$ cd /home/atguigu/bin </p>
<p>[atguigu<a href="/hadoop102">@hadoop102 </a> bin]$ vim jpsall </p>
<p>Ø 输入如下内容</p>
<p>#!/bin/bash</p>
<p>for host in hadoop102 hadoop103 hadoop104</p>
<p>do</p>
<p>    echo =============== $host ===============</p>
<p>    ssh $host jps</p>
<p>done</p>
<p>Ø 保存后退出，然后赋予脚本执行权限</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ chmod +x jpsall</span><br></pre></td></tr></table></figure>

<p><strong>3</strong>）分发/home/atguigu/bin目录，保证自定义脚本在三台机器上都可以使用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ xsync /home/atguigu/bin/</span><br></pre></td></tr></table></figure>

<h3 id="3-2-10-常用端口号说明"><a href="#3-2-10-常用端口号说明" class="headerlink" title="3.2.10 常用端口号说明"></a>3.2.10 常用端口号说明</h3><table>
<thead>
<tr>
<th>端口名称</th>
<th>Hadoop2.x</th>
<th>Hadoop3.x</th>
</tr>
</thead>
<tbody><tr>
<td>NameNode内部通信端口</td>
<td>8020 / 9000</td>
<td>8020 /  9000/9820</td>
</tr>
<tr>
<td>NameNode HTTP UI</td>
<td>50070</td>
<td>9870</td>
</tr>
<tr>
<td>MapReduce查看执行任务端口</td>
<td>8088</td>
<td>8088</td>
</tr>
<tr>
<td>历史服务器通信端口</td>
<td>19888</td>
<td>19888</td>
</tr>
</tbody></table>
<h3 id="3-2-11-集群时间同步"><a href="#3-2-11-集群时间同步" class="headerlink" title="3.2.11 集群时间同步"></a>3.2.11 集群时间同步</h3><p>如果服务器在公网环境（能连接外网），可以不采用集群时间同步，因为服务器会定期和公网时间进行校准；</p>
<p>如果服务器在内网环境，必须要配置集群时间同步，否则时间久了，会产生时间偏差，导致集群执行任务时间不同步。</p>
<p><strong>1</strong>）需求</p>
<p>找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，生产环境根据任务对时间的准确程度要求周期同步。测试环境为了尽快看到效果，采用1分钟同步一次。</p>
<p><strong>2</strong>）时间服务器配置（必须root用户）</p>
<p>（1）查看所有节点ntpd服务状态和开机自启动状态</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo systemctl status ntpd</span><br><span class="line">[atguigu@hadoop102 ~]$ sudo systemctl start ntpd</span><br><span class="line">[atguigu@hadoop102 ~]$ sudo systemctl is-enabled ntpd</span><br></pre></td></tr></table></figure>

<p>（2）修改hadoop102的ntp.conf配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo vim /etc/ntp.conf</span><br></pre></td></tr></table></figure>

<p>修改内容如下</p>
<p>（a）修改1（授权192.168.10.0-192.168.10.255网段上的所有机器可以从这台机器上查询和同步时间）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#restrict 192.168.10.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line">为restrict 192.168.10.0 mask 255.255.255.0 nomodify notrap</span><br></pre></td></tr></table></figure>

<p>    （b）修改2（集群在局域网中，不使用其他互联网上的时间）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">server 0.centos.pool.ntp.org iburst</span><br><span class="line">server 1.centos.pool.ntp.org iburst</span><br><span class="line">server 2.centos.pool.ntp.org iburst</span><br><span class="line">server 3.centos.pool.ntp.org iburst</span><br></pre></td></tr></table></figure>

<p>为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#server 0.centos.pool.ntp.org iburst</span><br><span class="line">#server 1.centos.pool.ntp.org iburst</span><br><span class="line">#server 2.centos.pool.ntp.org iburst</span><br><span class="line">#server 3.centos.pool.ntp.org iburst</span><br></pre></td></tr></table></figure>

<p>（c）添加3（当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure>

<p>（3）修改hadoop102的/etc/sysconfig/ntpd 文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo vim /etc/sysconfig/ntpd</span><br></pre></td></tr></table></figure>

<p>增加内容如下（让硬件时间与系统时间一起同步）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SYNC_HWCLOCK=yes</span><br></pre></td></tr></table></figure>

<p>（4）重新启动ntpd服务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo systemctl start ntpd</span><br></pre></td></tr></table></figure>

<p>（5）设置ntpd服务开机启动</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo systemctl enable ntpd</span><br></pre></td></tr></table></figure>

<p><strong>3</strong>）其他机器配置（必须root用户）</p>
<p>（1）关闭所有节点上ntp服务和自启动</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ sudo systemctl stop ntpd</span><br><span class="line">[atguigu@hadoop103 ~]$ sudo systemctl disable ntpd</span><br><span class="line">[atguigu@hadoop104 ~]$ sudo systemctl stop ntpd</span><br><span class="line">[atguigu@hadoop104 ~]$ sudo systemctl disable ntpd</span><br></pre></td></tr></table></figure>

<p>（2）在其他机器配置1分钟与时间服务器同步一次</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ sudo crontab -e</span><br></pre></td></tr></table></figure>

<p>编写定时任务如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*/1 * * * * /usr/sbin/ntpdate hadoop102</span><br></pre></td></tr></table></figure>

<p>（3）修改任意机器时间</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ sudo date -s &quot;2021-9-11 11:11:11&quot;</span><br></pre></td></tr></table></figure>

<p>（4）1分钟后查看机器是否与时间服务器同步</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ sudo date</span><br></pre></td></tr></table></figure>

<h1 id="第4章-常见错误及解决方案"><a href="#第4章-常见错误及解决方案" class="headerlink" title="第4章 常见错误及解决方案"></a>第4章 常见错误及解决方案</h1><p>1）防火墙没关闭、或者没有启动YARN</p>
<ul>
<li>INFO client.RMProxy: Connecting to ResourceManager at hadoop108/192.168.10.108:8032*</li>
</ul>
<p>2）主机名称配置错误</p>
<p>3）IP地址配置错误</p>
<p>4）ssh没有配置好</p>
<p>5）root用户和atguigu两个用户启动集群不统一</p>
<p>6）配置文件修改不细心</p>
<p>7）不识别主机名称</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">java.net.UnknownHostException: hadoop102: hadoop102</span><br><span class="line">        at java.net.InetAddress.getLocalHost(InetAddress.java:1475)</span><br><span class="line">        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:146)</span><br><span class="line">        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)</span><br><span class="line">        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)</span><br><span class="line">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">at javax.security.auth.Subject.doAs(Subject.java:415)</span><br></pre></td></tr></table></figure>

<p>解决办法：</p>
<pre><code>（1）在/etc/hosts文件中添加192.168.10.102 hadoop102
</code></pre>
<p>    （2）主机名称不要起hadoop hadoop000等特殊名称</p>
<p>8）DataNode和NameNode进程同时只能工作一个。</p>
<p><img src="https://image.3001.net/images/20221104/16675333856288.png#crop=0&crop=0&crop=1&crop=1&id=izPch&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>9）执行命令不生效，粘贴Word中命令时，遇到-和长–没区分开。导致命令失效</p>
<pre><code>    解决办法：尽量不要粘贴Word中代码。
</code></pre>
<p>10）jps发现进程已经没有，但是重新启动集群，提示进程已经开启。</p>
<pre><code>    原因是在Linux的根目录下/tmp目录中存在启动的进程临时文件，将集群相关进程删除掉，再重新启动集群。
</code></pre>
<p>11）jps不生效</p>
<pre><code>    原因：全局变量hadoop java没有生效。解决办法：需要source /etc/profile文件。
</code></pre>
<p>12）8088端口连接不上</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 桌面]$ cat /etc/hosts</span><br></pre></td></tr></table></figure>

<p>注释掉如下代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#127.0.0.1  localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">#::1     hadoop102</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yangmour.github.io/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/05_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88Yarn%EF%BC%89V3.3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E5%A4%B4%E5%83%8F.gif">
      <meta itemprop="name" content="希文">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="希文的个人博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/05_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88Yarn%EF%BC%89V3.3/" class="post-title-link" itemprop="url">05_尚硅谷大数据技术之Hadoop（Yarn）V3.3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-11-02 10:06:17" itemprop="dateCreated datePublished" datetime="2022-11-02T10:06:17+08:00">2022-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-10 14:04:10" itemprop="dateModified" datetime="2022-11-10T14:04:10+08:00">2022-11-10</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>26k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>23 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<p>尚硅谷大数据技术之Hadoop（Yarn）</p>
<p>（作者：尚硅谷大数据研发部）</p>
<p>版本：V3.3</p>
<h1 id="第1章-Yarn资源调度器"><a href="#第1章-Yarn资源调度器" class="headerlink" title="第1章 Yarn资源调度器"></a>第1章 Yarn资源调度器</h1><p>思考：</p>
<p>1）如何管理集群资源？</p>
<p>2）如何给任务合理分配资源？</p>
<p>​        <img src="https://image.3001.net/images/20221110/16680591616165.png" alt="image-20221110134558345">                       </p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/05_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88Yarn%EF%BC%89V3.3/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yangmour.github.io/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/06_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88%E7%94%9F%E4%BA%A7%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%EF%BC%89V3.3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E5%A4%B4%E5%83%8F.gif">
      <meta itemprop="name" content="希文">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="希文的个人博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/06_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88%E7%94%9F%E4%BA%A7%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%EF%BC%89V3.3/" class="post-title-link" itemprop="url">06_尚硅谷大数据技术之Hadoop（生产调优手册）V3.3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-11-02 10:06:17" itemprop="dateCreated datePublished" datetime="2022-11-02T10:06:17+08:00">2022-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-10 15:41:19" itemprop="dateModified" datetime="2022-11-10T15:41:19+08:00">2022-11-10</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>41k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>37 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<p>尚硅谷大数据技术之Hadoop（生产调优手册）</p>
<p>（作者：尚硅谷大数据研发部）</p>
<p>版本：V3.3</p>
<h1 id="第1章-HDFS—核心参数"><a href="#第1章-HDFS—核心参数" class="headerlink" title="第1章 HDFS—核心参数"></a>第1章 HDFS—核心参数</h1><h2 id="1-1-NameNode内存生产配置"><a href="#1-1-NameNode内存生产配置" class="headerlink" title="1.1 NameNode内存生产配置"></a>1.1 NameNode内存生产配置</h2><p>1）NameNode内存计算</p>
<p>​    每个文件块大概占用150byte，一台服务器128G内存为例，能存储多少文件块呢？</p>
<p>​    128 * 1024 * 1024 * 1024 / 150Byte ≈ 9.1亿</p>
<p>​    G    MB    KB  Byte</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/06_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88%E7%94%9F%E4%BA%A7%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C%EF%BC%89V3.3/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yangmour.github.io/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/04_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88MapReduce%EF%BC%89V3.3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E5%A4%B4%E5%83%8F.gif">
      <meta itemprop="name" content="希文">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="希文的个人博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/04_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88MapReduce%EF%BC%89V3.3/" class="post-title-link" itemprop="url">04_尚硅谷大数据技术之Hadoop（MapReduce）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-11-02 10:06:17" itemprop="dateCreated datePublished" datetime="2022-11-02T10:06:17+08:00">2022-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-10 15:45:31" itemprop="dateModified" datetime="2022-11-10T15:45:31+08:00">2022-11-10</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>58k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>53 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<p>尚硅谷大数据技术之Hadoop（MapReduce）</p>
<p>（作者：尚硅谷大数据研发部）</p>
<p>版本：V3.3</p>
<h1 id="第1章-MapReduce概述"><a href="#第1章-MapReduce概述" class="headerlink" title="第1章 MapReduce概述"></a>第1章 MapReduce概述</h1><h2 id="1-1-MapReduce定义"><a href="#1-1-MapReduce定义" class="headerlink" title="1.1 MapReduce定义"></a>1.1 MapReduce定义</h2><p>​    MapReduce是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架。</p>
<p>​    MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/04_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88MapReduce%EF%BC%89V3.3/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yangmour.github.io/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/07_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%89V3.3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E5%A4%B4%E5%83%8F.gif">
      <meta itemprop="name" content="希文">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="希文的个人博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/07_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%89V3.3/" class="post-title-link" itemprop="url">07_尚硅谷大数据技术之Hadoop（源码解析）V3.3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-11-02 10:06:17" itemprop="dateCreated datePublished" datetime="2022-11-02T10:06:17+08:00">2022-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-10 17:37:31" itemprop="dateModified" datetime="2022-11-10T17:37:31+08:00">2022-11-10</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>118k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1:47</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<p>尚硅谷大数据技术之Hadoop源码解析</p>
<p>（作者：尚硅谷大数据研发部）</p>
<p>版本：V3.3</p>
<h1 id="第0章-RPC通信原理解析"><a href="#第0章-RPC通信原理解析" class="headerlink" title="第0章 RPC通信原理解析"></a>第0章 RPC通信原理解析</h1><p>0）回顾</p>
<p><img src="https://image.3001.net/images/20221031/16671862014641.png#alt=image-20221031111634272">1)需求：</p>
<p>    模拟RPC的客户端、服务端、通信协议三者如何工作的</p>
<p>     <img src="https://image.3001.net/images/20221031/16671862275760.png#alt=image-20221031111700911"></p>
<p>2）代码编写：</p>
<p>（1）在HDFSClient项目基础上创建包名com.atguigu.rpc</p>
<p>（2）创建RPC协议</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.rpc;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">RPCProtocol</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> versionID = <span class="number">666</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">mkdirs</span><span class="params">(String path)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（3）创建RPC服务端</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.rpc;</span><br><span class="line"> </span><br><span class="line"> import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> import org.apache.hadoop.ipc.RPC;</span><br><span class="line"> import org.apache.hadoop.ipc.Server;</span><br><span class="line"> </span><br><span class="line"> import java.io.IOException;</span><br><span class="line"> </span><br><span class="line"> public class NNServer implements RPCProtocol&#123;</span><br><span class="line"> </span><br><span class="line">   @Override</span><br><span class="line">   public void mkdirs(String path) &#123;</span><br><span class="line">     System.out.println(&quot;服务端，创建路径&quot; + path);</span><br><span class="line">   &#125;</span><br><span class="line"> </span><br><span class="line">   public static void main(String[] args) throws IOException &#123;</span><br><span class="line"> </span><br><span class="line">     Server server = new RPC.Builder(new Configuration())</span><br><span class="line">         .setBindAddress(&quot;localhost&quot;)</span><br><span class="line">         .setPort(8888)</span><br><span class="line">         .setProtocol(RPCProtocol.class)</span><br><span class="line">         .setInstance(new NNServer())</span><br><span class="line">         .build();</span><br><span class="line"> </span><br><span class="line">     System.out.println(&quot;服务器开始工作&quot;);</span><br><span class="line"> </span><br><span class="line">     server.start();</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>（4）创建RPC客户端</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.rpc;</span><br><span class="line"> </span><br><span class="line"> import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> import org.apache.hadoop.ipc.RPC;</span><br><span class="line"> </span><br><span class="line"> import java.io.IOException;</span><br><span class="line"> import java.net.InetSocketAddress;</span><br><span class="line"> </span><br><span class="line"> public class HDFSClient &#123;</span><br><span class="line"> </span><br><span class="line">   public static void main(String[] args) throws IOException &#123;</span><br><span class="line">     RPCProtocol client = RPC.getProxy(</span><br><span class="line">         RPCProtocol.class,</span><br><span class="line">         RPCProtocol.versionID,</span><br><span class="line">         new InetSocketAddress(&quot;localhost&quot;, 8888),</span><br><span class="line">         new Configuration());</span><br><span class="line"> </span><br><span class="line">     System.out.println(&quot;我是客户端&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">     client.mkdirs(&quot;/input&quot;);</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>3）测试</p>
<p>    （1）启动服务端</p>
<p>观察控制台打印：服务器开始工作</p>
<p>在控制台Terminal窗口输入，jps，查看到NNServer服务</p>
<p>    （2）启动客户端</p>
<p>        观察客户端控制台打印：我是客户端</p>
<p>        观察服务端控制台打印：服务端，创建路径/input</p>
<p>4）总结</p>
<p>    RPC的客户端调用通信协议方法，方法的执行在服务端；</p>
<p>    通信协议就是接口规范。</p>
<h1 id="第1章-NameNode启动源码解析"><a href="#第1章-NameNode启动源码解析" class="headerlink" title="第1章 NameNode启动源码解析"></a>第1章 NameNode启动源码解析</h1><p><img src="https://image.3001.net/images/20221031/16671863278361.png#alt=image-20221031111840714"></p>
<p><img src="https://image.3001.net/images/20221031/16671863452272.png#alt=image-20221031111858595"></p>
<p>0）在pom.xml中增加如下依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>1）ctrl + n 全局查找namenode，进入NameNode.java</p>
<p>    NameNode官方说明</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NameNode serves as both directory namespace manager and &quot;inode table&quot; for the Hadoop DFS. There is a single NameNode running in any DFS deployment. (Well, except when there is a second backup/failover NameNode, or when using federated NameNodes.) The NameNode controls two critical tables: 1) filename-&gt;blocksequence (namespace) 2) block-&gt;machinelist (&quot;inodes&quot;) The first table is stored on disk and is very precious. The second table is rebuilt every time the NameNode comes up. &#x27;NameNode&#x27; refers to both this class as well as the &#x27;NameNode server&#x27;. The &#x27;FSNamesystem&#x27; class actually performs most of the filesystem management. The majority of the &#x27;NameNode&#x27; class itself is concerned with exposing the IPC interface and the HTTP server to the outside world, plus some configuration management. NameNode implements the ClientProtocol interface, which allows clients to ask for DFS services. ClientProtocol is not designed for direct use by authors of DFS client code. End-users should instead use the FileSystem class. NameNode also implements the DatanodeProtocol interface, used by DataNodes that actually store DFS data blocks. These methods are invoked repeatedly and automatically by all the DataNodes in a DFS deployment. NameNode also implements the NamenodeProtocol interface, used by secondary namenodes or rebalancing processes to get partial NameNode state, for example partial blocksMap etc.</span><br></pre></td></tr></table></figure>

<p>2）ctrl + f，查找main方法</p>
<p>NameNode.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public static void main(String argv[]) throws Exception &#123;</span><br><span class="line">	if (DFSUtil.parseHelpArgument(argv, NameNode.USAGE, System.out, true)) &#123;</span><br><span class="line">		System.exit(0);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	try &#123;</span><br><span class="line">		StringUtils.startupShutdownMessage(NameNode.class, argv, LOG);</span><br><span class="line">		// 创建NameNode</span><br><span class="line">		NameNode namenode = createNameNode(argv, null);</span><br><span class="line">		if (namenode != null) &#123;</span><br><span class="line">			namenode.join();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; catch (Throwable e) &#123;</span><br><span class="line">		LOG.error(&quot;Failed to start namenode.&quot;, e);</span><br><span class="line">		terminate(1, e);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击createNameNode</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> NameNode <span class="title">createNameNode</span><span class="params">(String argv[], Configuration conf)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  … …</span><br><span class="line">  StartupOption startOpt = parseArguments(argv);</span><br><span class="line">  <span class="keyword">if</span> (startOpt == <span class="keyword">null</span>) &#123;</span><br><span class="line">    printUsage(System.err);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  setStartupOption(conf, startOpt);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">boolean</span> aborted = <span class="keyword">false</span>;</span><br><span class="line">  <span class="keyword">switch</span> (startOpt) &#123;</span><br><span class="line">  <span class="keyword">case</span> FORMAT:</span><br><span class="line">    aborted = format(conf, startOpt.getForceFormat(),</span><br><span class="line">        startOpt.getInteractiveFormat());</span><br><span class="line">    terminate(aborted ? <span class="number">1</span> : <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>; <span class="comment">// avoid javac warning</span></span><br><span class="line">  <span class="keyword">case</span> GENCLUSTERID:</span><br><span class="line">    … …</span><br><span class="line">  <span class="keyword">default</span>:</span><br><span class="line">    DefaultMetricsSystem.initialize(<span class="string">&quot;NameNode&quot;</span>);</span><br><span class="line">	<span class="comment">// 创建NameNode对象</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> NameNode(conf);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击NameNode</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public NameNode(Configuration conf) throws IOException &#123;</span><br><span class="line">  this(conf, NamenodeRole.NAMENODE);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">protected NameNode(Configuration conf, NamenodeRole role)</span><br><span class="line">    throws IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  try &#123;</span><br><span class="line">    initializeGenericKeys(conf, nsId, namenodeId);</span><br><span class="line">    initialize(getConf());</span><br><span class="line">    ... ...</span><br><span class="line">  &#125; catch (IOException e) &#123;</span><br><span class="line">    this.stopAtException(e);</span><br><span class="line">    throw e;</span><br><span class="line">  &#125; catch (HadoopIllegalArgumentException e) &#123;</span><br><span class="line">    this.stopAtException(e);</span><br><span class="line">    throw e;</span><br><span class="line">  &#125;</span><br><span class="line">  this.started.set(true);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击initialize</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">protected void initialize(Configuration conf) throws IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  if (NamenodeRole.NAMENODE == role) &#123;</span><br><span class="line">	// 启动HTTP服务端（9870）</span><br><span class="line">    startHttpServer(conf);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // 加载镜像文件和编辑日志到内存</span><br><span class="line">  loadNamesystem(conf);</span><br><span class="line">  startAliasMapServerIfNecessary(conf);</span><br><span class="line"></span><br><span class="line">  // 创建NN的RPC服务端</span><br><span class="line">  rpcServer = createRpcServer(conf);</span><br><span class="line"></span><br><span class="line">  initReconfigurableBackoffKey();</span><br><span class="line"></span><br><span class="line">  if (clientNamenodeAddress == null) &#123;</span><br><span class="line">    // This is expected for MiniDFSCluster. Set it now using </span><br><span class="line">    // the RPC server&#x27;s bind address.</span><br><span class="line">    clientNamenodeAddress = </span><br><span class="line">        NetUtils.getHostPortString(getNameNodeAddress());</span><br><span class="line">    LOG.info(&quot;Clients are to use &quot; + clientNamenodeAddress + &quot; to access&quot;</span><br><span class="line">        + &quot; this namenode/service.&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">  if (NamenodeRole.NAMENODE == role) &#123;</span><br><span class="line">    httpServer.setNameNodeAddress(getNameNodeAddress());</span><br><span class="line">    httpServer.setFSImage(getFSImage());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // NN启动资源检查</span><br><span class="line">  startCommonServices(conf);</span><br><span class="line">  startMetricsLogger(conf);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="1-1-启动9870端口服务"><a href="#1-1-启动9870端口服务" class="headerlink" title="1.1 启动9870端口服务"></a>1.1 启动9870端口服务</h2><p>1）点击startHttpServer</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">private void startHttpServer(final Configuration conf) throws IOException &#123;</span><br><span class="line">	httpServer = new NameNodeHttpServer(conf, this, getHttpServerBindAddress(conf));</span><br><span class="line">	httpServer.start();</span><br><span class="line">	httpServer.setStartupProgress(startupProgress);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">protected InetSocketAddress getHttpServerBindAddress(Configuration conf) &#123;</span><br><span class="line">  InetSocketAddress bindAddress = getHttpServerAddress(conf);</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line">  return bindAddress;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">protected InetSocketAddress getHttpServerAddress(Configuration conf) &#123;</span><br><span class="line">  return getHttpAddress(conf);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public static InetSocketAddress getHttpAddress(Configuration conf) &#123;</span><br><span class="line">	return  NetUtils.createSocketAddr(</span><br><span class="line">      conf.getTrimmed(DFS_NAMENODE_HTTP_ADDRESS_KEY, DFS_NAMENODE_HTTP_ADDRESS_DEFAULT));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public static final String  DFS_NAMENODE_HTTP_ADDRESS_DEFAULT = &quot;0.0.0.0:&quot; + DFS_NAMENODE_HTTP_PORT_DEFAULT;</span><br><span class="line"></span><br><span class="line">public static final int     DFS_NAMENODE_HTTP_PORT_DEFAULT =</span><br><span class="line">HdfsClientConfigKeys.DFS_NAMENODE_HTTP_PORT_DEFAULT;</span><br><span class="line"></span><br><span class="line">int  DFS_NAMENODE_HTTP_PORT_DEFAULT = 9870;</span><br></pre></td></tr></table></figure>

<p>2）点击startHttpServer方法中的httpServer.<strong>start</strong>();</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">void start() throws IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  // Hadoop自己封装了HttpServer，形成自己的HttpServer2</span><br><span class="line">  HttpServer2.Builder builder = DFSUtil.httpServerTemplateForNNAndJN(conf,</span><br><span class="line">      httpAddr, httpsAddr, &quot;hdfs&quot;,</span><br><span class="line">      DFSConfigKeys.DFS_NAMENODE_KERBEROS_INTERNAL_SPNEGO_PRINCIPAL_KEY,</span><br><span class="line">      DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY);</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  httpServer = builder.build();</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  httpServer.setAttribute(NAMENODE_ATTRIBUTE_KEY, nn);</span><br><span class="line">  httpServer.setAttribute(JspHelper.CURRENT_CONF, conf);</span><br><span class="line">  setupServlets(httpServer, conf);</span><br><span class="line">  httpServer.start();</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>    点击setupServlets</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">private static void setupServlets(HttpServer2 httpServer, Configuration conf) &#123;</span><br><span class="line">	httpServer.addInternalServlet(&quot;startupProgress&quot;,</span><br><span class="line">		StartupProgressServlet.PATH_SPEC, StartupProgressServlet.class);</span><br><span class="line">	httpServer.addInternalServlet(&quot;fsck&quot;, &quot;/fsck&quot;, FsckServlet.class,</span><br><span class="line">		true);</span><br><span class="line">	httpServer.addInternalServlet(&quot;imagetransfer&quot;, ImageServlet.PATH_SPEC,</span><br><span class="line">      ImageServlet.class, true);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="1-2-加载镜像文件和编辑日志"><a href="#1-2-加载镜像文件和编辑日志" class="headerlink" title="1.2 加载镜像文件和编辑日志"></a>1.2 加载镜像文件和编辑日志</h2><p>1）点击loadNamesystem</p>
<p>NameNode.java</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">loadNamesystem</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">	<span class="keyword">this</span>.namesystem = FSNamesystem.loadFromDisk(conf);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> FSNamesystem <span class="title">loadFromDisk</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">  checkConfiguration(conf);</span><br><span class="line"></span><br><span class="line">  FSImage fsImage = <span class="keyword">new</span> FSImage(conf,</span><br><span class="line">      FSNamesystem.getNamespaceDirs(conf),</span><br><span class="line">      FSNamesystem.getNamespaceEditsDirs(conf));</span><br><span class="line"></span><br><span class="line">  FSNamesystem namesystem = <span class="keyword">new</span> FSNamesystem(conf, fsImage, <span class="keyword">false</span>);</span><br><span class="line">  StartupOption startOpt = NameNode.getStartupOption(conf);</span><br><span class="line">  <span class="keyword">if</span> (startOpt == StartupOption.RECOVER) &#123;</span><br><span class="line">    namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">long</span> loadStart = monotonicNow();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    namesystem.loadFSImage(startOpt);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (IOException ioe) &#123;</span><br><span class="line">    LOG.warn(<span class="string">&quot;Encountered exception loading fsimage&quot;</span>, ioe);</span><br><span class="line">    fsImage.close();</span><br><span class="line">    <span class="keyword">throw</span> ioe;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">long</span> timeTakenToLoadFSImage = monotonicNow() - loadStart;</span><br><span class="line">  LOG.info(<span class="string">&quot;Finished loading FSImage in &quot;</span> + timeTakenToLoadFSImage + <span class="string">&quot; msecs&quot;</span>);</span><br><span class="line">  NameNodeMetrics nnMetrics = NameNode.getNameNodeMetrics();</span><br><span class="line">  <span class="keyword">if</span> (nnMetrics != <span class="keyword">null</span>) &#123;</span><br><span class="line">    nnMetrics.setFsImageLoadTime((<span class="keyword">int</span>) timeTakenToLoadFSImage);</span><br><span class="line">  &#125;</span><br><span class="line">  namesystem.getFSDirectory().createReservedStatuses(namesystem.getCTime());</span><br><span class="line">  <span class="keyword">return</span> namesystem;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="1-3-初始化NN的RPC服务端"><a href="#1-3-初始化NN的RPC服务端" class="headerlink" title="1.3 初始化NN的RPC服务端"></a>1.3 初始化NN的RPC服务端</h2><p>1）点击createRpcServer</p>
<p>NameNode.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">protected NameNodeRpcServer createRpcServer(Configuration conf) throws IOException &#123;</span><br><span class="line"> return new NameNodeRpcServer(conf, this);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>NameNodeRpcServer.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public NameNodeRpcServer(Configuration conf, NameNode nn)</span><br><span class="line">      throws IOException &#123;</span><br><span class="line">	... ....	</span><br><span class="line">    serviceRpcServer = new RPC.Builder(conf)</span><br><span class="line">        .setProtocol(</span><br><span class="line">            org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB.class)</span><br><span class="line">        .setInstance(clientNNPbService)</span><br><span class="line">        .setBindAddress(bindHost)</span><br><span class="line">        .setPort(serviceRpcAddr.getPort())</span><br><span class="line">        .setNumHandlers(serviceHandlerCount)</span><br><span class="line">        .setVerbose(false)</span><br><span class="line">        .setSecretManager(namesystem.getDelegationTokenSecretManager())</span><br><span class="line">        .build();</span><br><span class="line">	... ....	</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="1-4-NN启动资源检查"><a href="#1-4-NN启动资源检查" class="headerlink" title="1.4 NN启动资源检查"></a>1.4 NN启动资源检查</h2><p>1）点击startCommonServices</p>
<p>NameNode.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">private void startCommonServices(Configuration conf) throws IOException &#123;</span><br><span class="line"></span><br><span class="line">  namesystem.startCommonServices(conf, haContext);</span><br><span class="line"></span><br><span class="line">  registerNNSMXBean();</span><br><span class="line">  if (NamenodeRole.NAMENODE != role) &#123;</span><br><span class="line">    startHttpServer(conf);</span><br><span class="line">    httpServer.setNameNodeAddress(getNameNodeAddress());</span><br><span class="line">    httpServer.setFSImage(getFSImage());</span><br><span class="line">  &#125;</span><br><span class="line">  rpcServer.start();</span><br><span class="line">  try &#123;</span><br><span class="line">    plugins = conf.getInstances(DFS_NAMENODE_PLUGINS_KEY,</span><br><span class="line">        ServicePlugin.class);</span><br><span class="line">  &#125; catch (RuntimeException e) &#123;</span><br><span class="line">    String pluginsValue = conf.get(DFS_NAMENODE_PLUGINS_KEY);</span><br><span class="line">    LOG.error(&quot;Unable to load NameNode plugins. Specified list of plugins: &quot; +</span><br><span class="line">        pluginsValue, e);</span><br><span class="line">    throw e;</span><br><span class="line">  &#125;</span><br><span class="line">  … …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2）点击startCommonServices</p>
<p>    FSNamesystem.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">void startCommonServices(Configuration conf, HAContext haContext) throws IOException &#123;</span><br><span class="line">  this.registerMBean(); // register the MBean for the FSNamesystemState</span><br><span class="line">  writeLock();</span><br><span class="line">  this.haContext = haContext;</span><br><span class="line">  try &#123;</span><br><span class="line">    nnResourceChecker = new NameNodeResourceChecker(conf);</span><br><span class="line">    // 检查是否有足够的磁盘存储元数据（fsimage（默认100m） editLog（默认100m））</span><br><span class="line">    checkAvailableResources();</span><br><span class="line"></span><br><span class="line">    assert !blockManager.isPopulatingReplQueues();</span><br><span class="line">    StartupProgress prog = NameNode.getStartupProgress();</span><br><span class="line">    prog.beginPhase(Phase.SAFEMODE);</span><br><span class="line">long completeBlocksTotal = getCompleteBlocksTotal();</span><br><span class="line"></span><br><span class="line">    // 安全模式</span><br><span class="line">    prog.setTotal(Phase.SAFEMODE, STEP_AWAITING_REPORTED_BLOCKS,</span><br><span class="line">        completeBlocksTotal);</span><br><span class="line"></span><br><span class="line">    // 启动块服务</span><br><span class="line">    blockManager.activate(conf, completeBlocksTotal);</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    writeUnlock(&quot;startCommonServices&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  registerMXBean();</span><br><span class="line">  DefaultMetricsSystem.instance().register(this);</span><br><span class="line">  if (inodeAttributeProvider != null) &#123;</span><br><span class="line">    inodeAttributeProvider.start();</span><br><span class="line">    dir.setINodeAttributeProvider(inodeAttributeProvider);</span><br><span class="line">  &#125;</span><br><span class="line">  snapshotManager.registerMXBean();</span><br><span class="line">  InetSocketAddress serviceAddress = NameNode.getServiceAddress(conf, true);</span><br><span class="line">  this.nameNodeHostName = (serviceAddress != null) ?</span><br><span class="line">      serviceAddress.getHostName() : &quot;&quot;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>    点击NameNodeResourceChecker</p>
<p>NameNodeResourceChecker.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">public NameNodeResourceChecker(Configuration conf) throws IOException &#123;</span><br><span class="line">  this.conf = conf;</span><br><span class="line">  volumes = new HashMap&lt;String, CheckedVolume&gt;();</span><br><span class="line">  </span><br><span class="line">  // dfs.namenode.resource.du.reserved默认值 1024 * 1024 * 100 =》100m</span><br><span class="line">  duReserved = conf.getLong(DFSConfigKeys.DFS_NAMENODE_DU_RESERVED_KEY,</span><br><span class="line">      DFSConfigKeys.DFS_NAMENODE_DU_RESERVED_DEFAULT);</span><br><span class="line">  </span><br><span class="line">  Collection&lt;URI&gt; extraCheckedVolumes = Util.stringCollectionAsURIs(conf</span><br><span class="line">      .getTrimmedStringCollection(DFSConfigKeys.DFS_NAMENODE_CHECKED_VOLUMES_KEY));</span><br><span class="line">  </span><br><span class="line">  Collection&lt;URI&gt; localEditDirs = Collections2.filter(</span><br><span class="line">      FSNamesystem.getNamespaceEditsDirs(conf),</span><br><span class="line">      new Predicate&lt;URI&gt;() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        public boolean apply(URI input) &#123;</span><br><span class="line">          if (input.getScheme().equals(NNStorage.LOCAL_URI_SCHEME)) &#123;</span><br><span class="line">            return true;</span><br><span class="line">          &#125;</span><br><span class="line">          return false;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">  // 对所有路径进行资源检查</span><br><span class="line">  for (URI editsDirToCheck : localEditDirs) &#123;</span><br><span class="line">    addDirToCheck(editsDirToCheck,</span><br><span class="line">        FSNamesystem.getRequiredNamespaceEditsDirs(conf).contains(</span><br><span class="line">            editsDirToCheck));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // All extra checked volumes are marked &quot;required&quot;</span><br><span class="line">  for (URI extraDirToCheck : extraCheckedVolumes) &#123;</span><br><span class="line">    addDirToCheck(extraDirToCheck, true);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  minimumRedundantVolumes = conf.getInt(</span><br><span class="line">      DFSConfigKeys.DFS_NAMENODE_CHECKED_VOLUMES_MINIMUM_KEY,</span><br><span class="line">      DFSConfigKeys.DFS_NAMENODE_CHECKED_VOLUMES_MINIMUM_DEFAULT);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>    点击checkAvailableResources</p>
<p>    FNNamesystem.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">void checkAvailableResources() &#123;</span><br><span class="line">	long resourceCheckTime = monotonicNow();</span><br><span class="line">	Preconditions.checkState(nnResourceChecker != null,</span><br><span class="line">		&quot;nnResourceChecker not initialized&quot;);</span><br><span class="line"></span><br><span class="line">	// 判断资源是否足够，不够返回false</span><br><span class="line">	hasResourcesAvailable = nnResourceChecker.hasAvailableDiskSpace();</span><br><span class="line"></span><br><span class="line">	resourceCheckTime = monotonicNow() - resourceCheckTime;</span><br><span class="line">	NameNode.getNameNodeMetrics().addResourceCheckTime(resourceCheckTime);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>NameNodeResourceChecker.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public boolean hasAvailableDiskSpace() &#123;</span><br><span class="line">	return NameNodeResourcePolicy.areResourcesAvailable(volumes.values(),</span><br><span class="line">      minimumRedundantVolumes);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>NameNodeResourcePolicy.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">static boolean areResourcesAvailable(</span><br><span class="line">    Collection&lt;? extends CheckableNameNodeResource&gt; resources,</span><br><span class="line">    int minimumRedundantResources) &#123;</span><br><span class="line"></span><br><span class="line">  // TODO: workaround:</span><br><span class="line">  // - during startup, if there are no edits dirs on disk, then there is</span><br><span class="line">  // a call to areResourcesAvailable() with no dirs at all, which was</span><br><span class="line">  // previously causing the NN to enter safemode</span><br><span class="line">  if (resources.isEmpty()) &#123;</span><br><span class="line">    return true;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  int requiredResourceCount = 0;</span><br><span class="line">  int redundantResourceCount = 0;</span><br><span class="line">  int disabledRedundantResourceCount = 0;</span><br><span class="line"></span><br><span class="line">  // 判断资源是否充足</span><br><span class="line">  for (CheckableNameNodeResource resource : resources) &#123;</span><br><span class="line">    if (!resource.isRequired()) &#123;</span><br><span class="line">      redundantResourceCount++;</span><br><span class="line">      if (!resource.isResourceAvailable()) &#123;</span><br><span class="line">        disabledRedundantResourceCount++;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      requiredResourceCount++;</span><br><span class="line">      if (!resource.isResourceAvailable()) &#123;</span><br><span class="line">        // Short circuit - a required resource is not available. 不充足返回false</span><br><span class="line">        return false;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  if (redundantResourceCount == 0) &#123;</span><br><span class="line">    // If there are no redundant resources, return true if there are any</span><br><span class="line">    // required resources available.</span><br><span class="line">    return requiredResourceCount &gt; 0;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    return redundantResourceCount - disabledRedundantResourceCount &gt;=</span><br><span class="line">        minimumRedundantResources;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">interface CheckableNameNodeResource &#123;</span><br><span class="line">  </span><br><span class="line">  public boolean isResourceAvailable();</span><br><span class="line">  </span><br><span class="line">  public boolean isRequired();</span><br></pre></td></tr></table></figure>

<p>ctrl + h，查找实现类CheckedVolume</p>
<p>NameNodeResourceChecker.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public boolean isResourceAvailable() &#123;</span><br><span class="line"></span><br><span class="line">  // 获取当前目录的空间大小</span><br><span class="line">  long availableSpace = df.getAvailable();</span><br><span class="line"></span><br><span class="line">  if (LOG.isDebugEnabled()) &#123;</span><br><span class="line">    LOG.debug(&quot;Space available on volume &#x27;&quot; + volume + &quot;&#x27; is &quot;</span><br><span class="line">        + availableSpace);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // 如果当前空间大小，小于100m，返回false</span><br><span class="line">  if (availableSpace &lt; duReserved) &#123;</span><br><span class="line">    LOG.warn(&quot;Space available on volume &#x27;&quot; + volume + &quot;&#x27; is &quot;</span><br><span class="line">        + availableSpace +</span><br><span class="line">        &quot;, which is below the configured reserved amount &quot; + duReserved);</span><br><span class="line">    return false;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    return true;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="1-5-NN对心跳超时判断"><a href="#1-5-NN对心跳超时判断" class="headerlink" title="1.5 NN对心跳超时判断"></a>1.5 NN对心跳超时判断</h2><p>Ctrl + n 搜索namenode，ctrl + f搜索startCommonServices</p>
<p>点击namesystem.startCommonServices(conf, haContext);</p>
<p>点击blockManager.activate(conf, completeBlocksTotal);</p>
<p>点击datanodeManager.activate(conf);</p>
<p>    DatanodeManager.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">void activate(final Configuration conf) &#123;</span><br><span class="line"></span><br><span class="line"> datanodeAdminManager.activate(conf);</span><br><span class="line"></span><br><span class="line"> heartbeatManager.activate();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>    DatanodeManager.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line">void activate() &#123;</span><br><span class="line">  // 启动的线程，搜索run方法</span><br><span class="line">  heartbeatThread.start();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void run() &#123;</span><br><span class="line">  while(namesystem.isRunning()) &#123;</span><br><span class="line">    restartHeartbeatStopWatch();</span><br><span class="line">    try &#123;</span><br><span class="line">      final long now = Time.monotonicNow();</span><br><span class="line">      if (lastHeartbeatCheck + heartbeatRecheckInterval &lt; now) &#123;</span><br><span class="line">		// 心跳检查</span><br><span class="line">        heartbeatCheck();</span><br><span class="line">        lastHeartbeatCheck = now;</span><br><span class="line">      &#125;</span><br><span class="line">      if (blockManager.shouldUpdateBlockKey(now - lastBlockKeyUpdate)) &#123;</span><br><span class="line">        synchronized(HeartbeatManager.this) &#123;</span><br><span class="line">          for(DatanodeDescriptor d : datanodes) &#123;</span><br><span class="line">            d.setNeedKeyUpdate(true);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        lastBlockKeyUpdate = now;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; catch (Exception e) &#123;</span><br><span class="line">      LOG.error(&quot;Exception while checking heartbeat&quot;, e);</span><br><span class="line">    &#125;</span><br><span class="line">    try &#123;</span><br><span class="line">      Thread.sleep(5000);  // 5 seconds</span><br><span class="line">    &#125; catch (InterruptedException ignored) &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    // avoid declaring nodes dead for another cycle if a GC pause lasts</span><br><span class="line">    // longer than the node recheck interval</span><br><span class="line">    if (shouldAbortHeartbeatCheck(-5000)) &#123;</span><br><span class="line">      LOG.warn(&quot;Skipping next heartbeat scan due to excessive pause&quot;);</span><br><span class="line">      lastHeartbeatCheck = Time.monotonicNow();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void heartbeatCheck() &#123;</span><br><span class="line">  final DatanodeManager dm = blockManager.getDatanodeManager();</span><br><span class="line"></span><br><span class="line">  boolean allAlive = false;</span><br><span class="line">  while (!allAlive) &#123;</span><br><span class="line">    // locate the first dead node.</span><br><span class="line">    DatanodeDescriptor dead = null;</span><br><span class="line"></span><br><span class="line">    // locate the first failed storage that isn&#x27;t on a dead node.</span><br><span class="line">    DatanodeStorageInfo failedStorage = null;</span><br><span class="line"></span><br><span class="line">    // check the number of stale nodes</span><br><span class="line">    int numOfStaleNodes = 0;</span><br><span class="line">    int numOfStaleStorages = 0;</span><br><span class="line">    synchronized(this) &#123;</span><br><span class="line">      for (DatanodeDescriptor d : datanodes) &#123;</span><br><span class="line">        // check if an excessive GC pause has occurred</span><br><span class="line">        if (shouldAbortHeartbeatCheck(0)) &#123;</span><br><span class="line">          return;</span><br><span class="line">        &#125;</span><br><span class="line">		// 判断DN节点是否挂断</span><br><span class="line">        if (dead == null &amp;&amp; dm.isDatanodeDead(d)) &#123;</span><br><span class="line">          stats.incrExpiredHeartbeats();</span><br><span class="line">          dead = d;</span><br><span class="line">        &#125;</span><br><span class="line">        if (d.isStale(dm.getStaleInterval())) &#123;</span><br><span class="line">          numOfStaleNodes++;</span><br><span class="line">        &#125;</span><br><span class="line">        DatanodeStorageInfo[] storageInfos = d.getStorageInfos();</span><br><span class="line">        for(DatanodeStorageInfo storageInfo : storageInfos) &#123;</span><br><span class="line">          if (storageInfo.areBlockContentsStale()) &#123;</span><br><span class="line">            numOfStaleStorages++;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          if (failedStorage == null &amp;&amp;</span><br><span class="line">              storageInfo.areBlocksOnFailedStorage() &amp;&amp;</span><br><span class="line">              d != dead) &#123;</span><br><span class="line">            failedStorage = storageInfo;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      </span><br><span class="line">      // Set the number of stale nodes in the DatanodeManager</span><br><span class="line">      dm.setNumStaleNodes(numOfStaleNodes);</span><br><span class="line">      dm.setNumStaleStorages(numOfStaleStorages);</span><br><span class="line">    &#125;</span><br><span class="line">    ... ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">boolean isDatanodeDead(DatanodeDescriptor node) &#123;</span><br><span class="line">  return (node.getLastUpdateMonotonic() &lt;</span><br><span class="line">          (monotonicNow() - heartbeatExpireInterval));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private long heartbeatExpireInterval;</span><br><span class="line">// 10分钟 + 30秒</span><br><span class="line">this.heartbeatExpireInterval = 2 * heartbeatRecheckInterval + 10 * 1000 * heartbeatIntervalSeconds;</span><br><span class="line"></span><br><span class="line">private volatile int heartbeatRecheckInterval;</span><br><span class="line">heartbeatRecheckInterval = conf.getInt(</span><br><span class="line">        DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY, </span><br><span class="line">        DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_DEFAULT); // 5 minutes</span><br><span class="line"></span><br><span class="line">private volatile long heartbeatIntervalSeconds;</span><br><span class="line">heartbeatIntervalSeconds = conf.getTimeDuration(</span><br><span class="line">        DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,</span><br><span class="line">        DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT, TimeUnit.SECONDS);</span><br><span class="line">public static final long    DFS_HEARTBEAT_INTERVAL_DEFAULT = 3;</span><br></pre></td></tr></table></figure>


<h2 id="1-6-安全模式"><a href="#1-6-安全模式" class="headerlink" title="1.6 安全模式"></a>1.6 安全模式</h2><p>    FSNamesystem.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">void startCommonServices(Configuration conf, HAContext haContext) throws IOException &#123;</span><br><span class="line">  this.registerMBean(); // register the MBean for the FSNamesystemState</span><br><span class="line">  writeLock();</span><br><span class="line">  this.haContext = haContext;</span><br><span class="line">  try &#123;</span><br><span class="line">    nnResourceChecker = new NameNodeResourceChecker(conf);</span><br><span class="line">    // 检查是否有足够的磁盘存储元数据（fsimage（默认100m） editLog（默认100m））</span><br><span class="line">    checkAvailableResources();</span><br><span class="line"></span><br><span class="line">    assert !blockManager.isPopulatingReplQueues();</span><br><span class="line">    StartupProgress prog = NameNode.getStartupProgress();</span><br><span class="line"></span><br><span class="line">    // 开始进入安全模式</span><br><span class="line">    prog.beginPhase(Phase.SAFEMODE);</span><br><span class="line"></span><br><span class="line">    // 获取所有可以正常使用的block</span><br><span class="line">long completeBlocksTotal = getCompleteBlocksTotal();</span><br><span class="line"></span><br><span class="line">    prog.setTotal(Phase.SAFEMODE, STEP_AWAITING_REPORTED_BLOCKS,</span><br><span class="line">        completeBlocksTotal);</span><br><span class="line"></span><br><span class="line">    // 启动块服务</span><br><span class="line">    blockManager.activate(conf, completeBlocksTotal);</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    writeUnlock(&quot;startCommonServices&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  registerMXBean();</span><br><span class="line">  DefaultMetricsSystem.instance().register(this);</span><br><span class="line">  if (inodeAttributeProvider != null) &#123;</span><br><span class="line">    inodeAttributeProvider.start();</span><br><span class="line">    dir.setINodeAttributeProvider(inodeAttributeProvider);</span><br><span class="line">  &#125;</span><br><span class="line">  snapshotManager.registerMXBean();</span><br><span class="line">  InetSocketAddress serviceAddress = NameNode.getServiceAddress(conf, true);</span><br><span class="line">  this.nameNodeHostName = (serviceAddress != null) ?</span><br><span class="line">      serviceAddress.getHostName() : &quot;&quot;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击getCompleteBlocksTotal</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public long getCompleteBlocksTotal() &#123;</span><br><span class="line">  // Calculate number of blocks under construction</span><br><span class="line">  long numUCBlocks = 0;</span><br><span class="line">  readLock();</span><br><span class="line">  try &#123;</span><br><span class="line">    // 获取正在构建的block</span><br><span class="line">    numUCBlocks = leaseManager.getNumUnderConstructionBlocks();</span><br><span class="line">	// 获取所有的块 - 正在构建的block = 可以正常使用的block</span><br><span class="line">    return getBlocksTotal() - numUCBlocks;</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    readUnlock(&quot;getCompleteBlocksTotal&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击activate</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public void activate(Configuration conf, long blockTotal) &#123;</span><br><span class="line">	pendingReconstruction.start();</span><br><span class="line">	datanodeManager.activate(conf);</span><br><span class="line"></span><br><span class="line">	this.redundancyThread.setName(&quot;RedundancyMonitor&quot;);</span><br><span class="line">	this.redundancyThread.start();</span><br><span class="line"></span><br><span class="line">	storageInfoDefragmenterThread.setName(&quot;StorageInfoMonitor&quot;);</span><br><span class="line">	storageInfoDefragmenterThread.start();</span><br><span class="line">	this.blockReportThread.start();</span><br><span class="line"></span><br><span class="line">	mxBeanName = MBeans.register(&quot;NameNode&quot;, &quot;BlockStats&quot;, this);</span><br><span class="line"></span><br><span class="line">	bmSafeMode.activate(blockTotal);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击activate</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">void activate(long total) &#123;</span><br><span class="line">  assert namesystem.hasWriteLock();</span><br><span class="line">  assert status == BMSafeModeStatus.OFF;</span><br><span class="line"></span><br><span class="line">  startTime = monotonicNow();</span><br><span class="line"></span><br><span class="line">  // 计算是否满足块个数的阈值</span><br><span class="line">  setBlockTotal(total);</span><br><span class="line"></span><br><span class="line">  // 判断DataNode节点和块信息是否达到退出安全模式标准</span><br><span class="line">  if (areThresholdsMet()) &#123;</span><br><span class="line">    boolean exitResult = leaveSafeMode(false);</span><br><span class="line">    Preconditions.checkState(exitResult, &quot;Failed to leave safe mode.&quot;);</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    // enter safe mode</span><br><span class="line">status = BMSafeModeStatus.PENDING_THRESHOLD;</span><br><span class="line"></span><br><span class="line">initializeReplQueuesIfNecessary();</span><br><span class="line"></span><br><span class="line">    reportStatus(&quot;STATE* Safe mode ON.&quot;, true);</span><br><span class="line">    lastStatusReport = monotonicNow();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击setBlockTotal</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">void setBlockTotal(long total) &#123;</span><br><span class="line">  assert namesystem.hasWriteLock();</span><br><span class="line">  synchronized (this) &#123;</span><br><span class="line">    this.blockTotal = total;</span><br><span class="line">	// 计算阈值：例如：1000个正常的块 * 0.999 = 999</span><br><span class="line">    this.blockThreshold = (long) (total * threshold);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  this.blockReplQueueThreshold = (long) (total * replQueueThreshold);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">this.threshold = conf.getFloat(DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_KEY,</span><br><span class="line">        DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_DEFAULT);</span><br><span class="line"></span><br><span class="line">public static final float   DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_DEFAULT = 0.999f;</span><br></pre></td></tr></table></figure>

<p>点击areThresholdsMet</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">private boolean areThresholdsMet() &#123;</span><br><span class="line">  assert namesystem.hasWriteLock();</span><br><span class="line">  // Calculating the number of live datanodes is time-consuming</span><br><span class="line">  // in large clusters. Skip it when datanodeThreshold is zero.</span><br><span class="line">  int datanodeNum = 0;</span><br><span class="line"></span><br><span class="line">  if (datanodeThreshold &gt; 0) &#123;</span><br><span class="line">    datanodeNum = blockManager.getDatanodeManager().getNumLiveDataNodes();</span><br><span class="line">  &#125;</span><br><span class="line">  synchronized (this) &#123;</span><br><span class="line">  // 已经正常注册的块数 》= 块的最小阈值 》=最小可用DataNode</span><br><span class="line">    return blockSafe &gt;= blockThreshold &amp;&amp; datanodeNum &gt;= datanodeThreshold;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h1 id="第2章-DataNode启动源码解析"><a href="#第2章-DataNode启动源码解析" class="headerlink" title="第2章 DataNode启动源码解析"></a>第2章 DataNode启动源码解析</h1><p><img src="https://image.3001.net/images/20221031/16671880101840.png#alt=image-20221031114643770"></p>
<p><img src="https://image.3001.net/images/20221031/16671880211111.png#alt=image-20221031114654562"></p>
<p>0）在pom.xml中增加如下依赖</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">		&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">		&lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">		&lt;version&gt;3.1.3&lt;/version&gt;</span><br><span class="line">	&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">		&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">		&lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;</span><br><span class="line">		&lt;version&gt;3.1.3&lt;/version&gt;</span><br><span class="line">	&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">		&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">		&lt;artifactId&gt;hadoop-hdfs-client&lt;/artifactId&gt;</span><br><span class="line">		&lt;version&gt;3.1.3&lt;/version&gt;</span><br><span class="line">		&lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">	&lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure>

<p>1）ctrl + n 全局查找datanode，进入DataNode.java</p>
<p>    DataNode官方说明</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataNode is a class (and program) that stores a set of blocks for a DFS deployment. A single deployment can have one or many DataNodes. Each DataNode communicates regularly with a single NameNode. It also communicates with client code and other DataNodes from time to time. DataNodes store a series of named blocks. The DataNode allows client code to read these blocks, or to write new block data. The DataNode may also, in response to instructions from its NameNode, delete blocks or copy blocks to/from other DataNodes. The DataNode maintains just one critical table: block-&gt; stream of bytes (of BLOCK_SIZE or less) This info is stored on a local disk. The DataNode reports the table&#x27;s contents to the NameNode upon startup and every so often afterwards. DataNodes spend their lives in an endless loop of asking the NameNode for something to do. A NameNode cannot connect to a DataNode directly; a NameNode simply returns values from functions invoked by a DataNode. DataNodes maintain an open server socket so that client code or other DataNodes can read/write data. The host/port for this server is reported to the NameNode, which then sends that information to clients or other DataNodes that might be interested.</span><br></pre></td></tr></table></figure>

<p>2）ctrl + f，查找main方法</p>
<p>DataNode.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line">public static void main(String args[]) &#123;</span><br><span class="line">  if (DFSUtil.parseHelpArgument(args, DataNode.USAGE, System.out, true)) &#123;</span><br><span class="line">    System.exit(0);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  secureMain(args, null);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public static void secureMain(String args[], SecureResources resources) &#123;</span><br><span class="line">  int errorCode = 0;</span><br><span class="line">  try &#123;</span><br><span class="line">    StringUtils.startupShutdownMessage(DataNode.class, args, LOG);</span><br><span class="line"></span><br><span class="line">    DataNode datanode = createDataNode(args, null, resources);</span><br><span class="line"></span><br><span class="line">    … …</span><br><span class="line">  &#125; catch (Throwable e) &#123;</span><br><span class="line">    LOG.error(&quot;Exception in secureMain&quot;, e);</span><br><span class="line">    terminate(1, e);</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    LOG.warn(&quot;Exiting Datanode&quot;);</span><br><span class="line">    terminate(errorCode);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public static DataNode createDataNode(String args[], Configuration conf,</span><br><span class="line">    SecureResources resources) throws IOException &#123;</span><br><span class="line">  // 初始化DN</span><br><span class="line">  DataNode dn = instantiateDataNode(args, conf, resources);</span><br><span class="line"></span><br><span class="line">  if (dn != null) &#123;</span><br><span class="line">    // 启动DN进程</span><br><span class="line">    dn.runDatanodeDaemon();</span><br><span class="line">  &#125;</span><br><span class="line">  return dn;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public static DataNode instantiateDataNode(String args [], Configuration conf,</span><br><span class="line">    SecureResources resources) throws IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  </span><br><span class="line">  return makeInstance(dataLocations, conf, resources);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static DataNode makeInstance(Collection&lt;StorageLocation&gt; dataDirs,</span><br><span class="line">    Configuration conf, SecureResources resources) throws IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  return new DataNode(conf, locations, storageLocationChecker, resources);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">DataNode(final Configuration conf,</span><br><span class="line">         final List&lt;StorageLocation&gt; dataDirs,</span><br><span class="line">         final StorageLocationChecker storageLocationChecker,</span><br><span class="line">         final SecureResources resources) throws IOException &#123;</span><br><span class="line">  super(conf);</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  try &#123;</span><br><span class="line">    hostName = getHostName(conf);</span><br><span class="line">    LOG.info(&quot;Configured hostname is &#123;&#125;&quot;, hostName);</span><br><span class="line">	// 启动DN</span><br><span class="line">    startDataNode(dataDirs, resources);</span><br><span class="line">  &#125; catch (IOException ie) &#123;</span><br><span class="line">    shutdown();</span><br><span class="line">    throw ie;</span><br><span class="line">  &#125;</span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void startDataNode(List&lt;StorageLocation&gt; dataDirectories,</span><br><span class="line">                   SecureResources resources</span><br><span class="line">                   ) throws IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  // 创建数据存储对象</span><br><span class="line">  storage = new DataStorage();</span><br><span class="line">  </span><br><span class="line">  // global DN settings</span><br><span class="line">  registerMXBean();</span><br><span class="line">  // 初始化DataXceiver</span><br><span class="line">  initDataXceiver();</span><br><span class="line">  </span><br><span class="line">  // 启动HttpServer</span><br><span class="line">  startInfoServer();</span><br><span class="line"></span><br><span class="line">  pauseMonitor = new JvmPauseMonitor();</span><br><span class="line">  pauseMonitor.init(getConf());</span><br><span class="line">  pauseMonitor.start();</span><br><span class="line"></span><br><span class="line">  // BlockPoolTokenSecretManager is required to create ipc server.</span><br><span class="line">  this.blockPoolTokenSecretManager = new BlockPoolTokenSecretManager();</span><br><span class="line"></span><br><span class="line">  // Login is done by now. Set the DN user name.</span><br><span class="line">  dnUserName = UserGroupInformation.getCurrentUser().getUserName();</span><br><span class="line">  LOG.info(&quot;dnUserName = &#123;&#125;&quot;, dnUserName);</span><br><span class="line">  LOG.info(&quot;supergroup = &#123;&#125;&quot;, supergroup);</span><br><span class="line">  </span><br><span class="line">  // 初始化RPC服务</span><br><span class="line">  initIpcServer();</span><br><span class="line"></span><br><span class="line">  metrics = DataNodeMetrics.create(getConf(), getDisplayName());</span><br><span class="line">  peerMetrics = dnConf.peerStatsEnabled ?</span><br><span class="line">      DataNodePeerMetrics.create(getDisplayName(), getConf()) : null;</span><br><span class="line">  metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);</span><br><span class="line"></span><br><span class="line">  ecWorker = new ErasureCodingWorker(getConf(), this);</span><br><span class="line">  blockRecoveryWorker = new BlockRecoveryWorker(this);</span><br><span class="line">  </span><br><span class="line">  // 创建BlockPoolManager</span><br><span class="line">  blockPoolManager = new BlockPoolManager(this);</span><br><span class="line">  // 心跳管理</span><br><span class="line">  blockPoolManager.refreshNamenodes(getConf());</span><br><span class="line"></span><br><span class="line">  // Create the ReadaheadPool from the DataNode context so we can</span><br><span class="line">  // exit without having to explicitly shutdown its thread pool.</span><br><span class="line">  readaheadPool = ReadaheadPool.getInstance();</span><br><span class="line">  saslClient = new SaslDataTransferClient(dnConf.getConf(),</span><br><span class="line">      dnConf.saslPropsResolver, dnConf.trustedChannelResolver);</span><br><span class="line">  saslServer = new SaslDataTransferServer(dnConf, blockPoolTokenSecretManager);</span><br><span class="line">  startMetricsLogger();</span><br><span class="line"></span><br><span class="line">  if (dnConf.diskStatsEnabled) &#123;</span><br><span class="line">    diskMetrics = new DataNodeDiskMetrics(this,</span><br><span class="line">        dnConf.outliersReportIntervalMs);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="2-1-初始化DataXceiverServer"><a href="#2-1-初始化DataXceiverServer" class="headerlink" title="2.1 初始化DataXceiverServer"></a>2.1 初始化DataXceiverServer</h2><p>点击initDataXceiver</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">private void initDataXceiver() throws IOException &#123;</span><br><span class="line">// dataXceiverServer是一个服务，DN用来接收客户端和其他DN发送过来的数据服务</span><br><span class="line">  this.dataXceiverServer = new Daemon(threadGroup, xserver);</span><br><span class="line">  this.threadGroup.setDaemon(true); // auto destroy when empty</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="2-2-初始化HTTP服务"><a href="#2-2-初始化HTTP服务" class="headerlink" title="2.2 初始化HTTP服务"></a>2.2 初始化HTTP服务</h2><p>点击startInfoServer();</p>
<p>    DataNode.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">private void startInfoServer()</span><br><span class="line">  throws IOException &#123;</span><br><span class="line">  // SecureDataNodeStarter will bind the privileged port to the channel if</span><br><span class="line">  // the DN is started by JSVC, pass it along.</span><br><span class="line">  ServerSocketChannel httpServerChannel = secureResources != null ?</span><br><span class="line">      secureResources.getHttpServerChannel() : null;</span><br><span class="line"></span><br><span class="line">  httpServer = new DatanodeHttpServer(getConf(), this, httpServerChannel);</span><br><span class="line">  httpServer.start();</span><br><span class="line">  if (httpServer.getHttpAddress() != null) &#123;</span><br><span class="line">    infoPort = httpServer.getHttpAddress().getPort();</span><br><span class="line">  &#125;</span><br><span class="line">  if (httpServer.getHttpsAddress() != null) &#123;</span><br><span class="line">    infoSecurePort = httpServer.getHttpsAddress().getPort();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>    DatanodeHttpServer.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public DatanodeHttpServer(final Configuration conf,</span><br><span class="line">    final DataNode datanode,</span><br><span class="line">    final ServerSocketChannel externalHttpChannel)</span><br><span class="line">  throws IOException &#123;</span><br><span class="line">  </span><br><span class="line">  ... ...</span><br><span class="line">  HttpServer2.Builder builder = new HttpServer2.Builder()</span><br><span class="line">      .setName(&quot;datanode&quot;)</span><br><span class="line">      .setConf(confForInfoServer)</span><br><span class="line">      .setACL(new AccessControlList(conf.get(DFS_ADMIN, &quot; &quot;)))</span><br><span class="line">      .hostName(getHostnameForSpnegoPrincipal(confForInfoServer))</span><br><span class="line">      .addEndpoint(URI.create(&quot;http://localhost:&quot; + proxyPort))</span><br><span class="line">      .setFindPort(true);</span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="2-3-初始化DN的RPC服务端"><a href="#2-3-初始化DN的RPC服务端" class="headerlink" title="2.3 初始化DN的RPC服务端"></a>2.3 初始化DN的RPC服务端</h2><p>点击initIpcServer</p>
<p>    DataNode.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">private void initIpcServer() throws IOException &#123;</span><br><span class="line">  InetSocketAddress ipcAddr = NetUtils.createSocketAddr(</span><br><span class="line">      getConf().getTrimmed(DFS_DATANODE_IPC_ADDRESS_KEY));</span><br><span class="line">  </span><br><span class="line">  ... ...</span><br><span class="line">  ipcServer = new RPC.Builder(getConf())</span><br><span class="line">      .setProtocol(ClientDatanodeProtocolPB.class)</span><br><span class="line">      .setInstance(service)</span><br><span class="line">      .setBindAddress(ipcAddr.getHostName())</span><br><span class="line">      .setPort(ipcAddr.getPort())</span><br><span class="line">      .setNumHandlers(</span><br><span class="line">          getConf().getInt(DFS_DATANODE_HANDLER_COUNT_KEY,</span><br><span class="line">              DFS_DATANODE_HANDLER_COUNT_DEFAULT)).setVerbose(false)</span><br><span class="line">      .setSecretManager(blockPoolTokenSecretManager).build();</span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="2-4-DN向NN注册"><a href="#2-4-DN向NN注册" class="headerlink" title="2.4 DN向NN注册"></a>2.4 DN向NN注册</h2><p>点击refreshNamenodes</p>
<p>    BlockPoolManager.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">void refreshNamenodes(Configuration conf)</span><br><span class="line">    throws IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  synchronized (refreshNamenodesLock) &#123;</span><br><span class="line">    doRefreshNamenodes(newAddressMap, newLifelineAddressMap);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private void doRefreshNamenodes(</span><br><span class="line">    Map&lt;String, Map&lt;String, InetSocketAddress&gt;&gt; addrMap,</span><br><span class="line">    Map&lt;String, Map&lt;String, InetSocketAddress&gt;&gt; lifelineAddrMap)</span><br><span class="line">    throws IOException &#123;</span><br><span class="line">  … ….</span><br><span class="line">  </span><br><span class="line">  synchronized (this) &#123;</span><br><span class="line">    … …</span><br><span class="line"></span><br><span class="line">    // Step 3. Start new nameservices</span><br><span class="line">    if (!toAdd.isEmpty()) &#123;</span><br><span class="line"></span><br><span class="line">      for (String nsToAdd : toAdd) &#123;</span><br><span class="line">        … …</span><br><span class="line">        BPOfferService bpos = createBPOS(nsToAdd, addrs, lifelineAddrs);</span><br><span class="line">        bpByNameserviceId.put(nsToAdd, bpos);</span><br><span class="line">        offerServices.add(bpos);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    startAll();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  … …</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">protected BPOfferService createBPOS(</span><br><span class="line">    final String nameserviceId,</span><br><span class="line">    List&lt;InetSocketAddress&gt; nnAddrs,</span><br><span class="line">    List&lt;InetSocketAddress&gt; lifelineNnAddrs) &#123;</span><br><span class="line">  // 根据NameNode个数创建对应的服务</span><br><span class="line">  return new BPOfferService(nameserviceId, nnAddrs, lifelineNnAddrs, dn);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击startAll()</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">synchronized void startAll() throws IOException &#123;</span><br><span class="line">  try &#123;</span><br><span class="line">    UserGroupInformation.getLoginUser().doAs(</span><br><span class="line">        new PrivilegedExceptionAction&lt;Object&gt;() &#123;</span><br><span class="line">          @Override</span><br><span class="line">          public Object run() throws Exception &#123;</span><br><span class="line">            for (BPOfferService bpos : offerServices) &#123;</span><br><span class="line">			  // 启动服务</span><br><span class="line">              bpos.start();</span><br><span class="line">            &#125;</span><br><span class="line">            return null;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  &#125; catch (InterruptedException ex) &#123;</span><br><span class="line">    ... ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击start ()</p>
<p>    BPOfferService.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">void start() &#123;</span><br><span class="line">  for (BPServiceActor actor : bpServices) &#123;</span><br><span class="line">    actor.start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击start ()</p>
<p>    BPServiceActor.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">void start() &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  bpThread = new Thread(this);</span><br><span class="line">  bpThread.setDaemon(true); // needed for JUnit testing</span><br><span class="line">// 表示开启一个线程，所有查找该线程的run方法</span><br><span class="line">  bpThread.start();</span><br><span class="line"></span><br><span class="line">  if (lifelineSender != null) &#123;</span><br><span class="line">    lifelineSender.start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>ctrl + f 搜索run方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">public void run() &#123;</span><br><span class="line">  LOG.info(this + &quot; starting to offer service&quot;);</span><br><span class="line"></span><br><span class="line">  try &#123;</span><br><span class="line">    while (true) &#123;</span><br><span class="line">      // init stuff</span><br><span class="line">      try &#123;</span><br><span class="line">        // setup storage</span><br><span class="line">		// 向NN 注册</span><br><span class="line">        connectToNNAndHandshake();</span><br><span class="line">        break;</span><br><span class="line">      &#125; catch (IOException ioe) &#123;</span><br><span class="line">        // Initial handshake, storage recovery or registration failed</span><br><span class="line">        runningState = RunningState.INIT_FAILED;</span><br><span class="line">        if (shouldRetryInit()) &#123;</span><br><span class="line">          // Retry until all namenode&#x27;s of BPOS failed initialization</span><br><span class="line">          LOG.error(&quot;Initialization failed for &quot; + this + &quot; &quot;</span><br><span class="line">              + ioe.getLocalizedMessage());</span><br><span class="line">		  // 注册失败，5s后重试</span><br><span class="line">          sleepAndLogInterrupts(5000, &quot;initializing&quot;);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          runningState = RunningState.FAILED;</span><br><span class="line">          LOG.error(&quot;Initialization failed for &quot; + this + &quot;. Exiting. &quot;, ioe);</span><br><span class="line">          return;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    … …</span><br><span class="line">    while (shouldRun()) &#123;</span><br><span class="line">      try &#123;</span><br><span class="line">        // 发送心跳</span><br><span class="line">        offerService();</span><br><span class="line">      &#125; catch (Exception ex) &#123;</span><br><span class="line">        ... ...</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private void connectToNNAndHandshake() throws IOException &#123;</span><br><span class="line">  // get NN proxy 获取NN的RPC客户端对象</span><br><span class="line">  bpNamenode = dn.connectToNN(nnAddr);</span><br><span class="line"></span><br><span class="line">  // First phase of the handshake with NN - get the namespace</span><br><span class="line">  // info.</span><br><span class="line">  NamespaceInfo nsInfo = retrieveNamespaceInfo();</span><br><span class="line"></span><br><span class="line">  // Verify that this matches the other NN in this HA pair.</span><br><span class="line">  // This also initializes our block pool in the DN if we are</span><br><span class="line">  // the first NN connection for this BP.</span><br><span class="line">  bpos.verifyAndSetNamespaceInfo(this, nsInfo);</span><br><span class="line"></span><br><span class="line">  /* set thread name again to include NamespaceInfo when it&#x27;s available. */</span><br><span class="line">  this.bpThread.setName(formatThreadName(&quot;heartbeating&quot;, nnAddr));</span><br><span class="line"></span><br><span class="line">  // 注册</span><br><span class="line">  register(nsInfo);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">DatanodeProtocolClientSideTranslatorPB connectToNN(</span><br><span class="line">    InetSocketAddress nnAddr) throws IOException &#123;</span><br><span class="line">  return new DatanodeProtocolClientSideTranslatorPB(nnAddr, getConf());</span><br><span class="line">&#125;</span><br><span class="line">DatanodeProtocolClientSideTranslatorPB.java</span><br><span class="line">public DatanodeProtocolClientSideTranslatorPB(InetSocketAddress nameNodeAddr,</span><br><span class="line">    Configuration conf) throws IOException &#123;</span><br><span class="line">  RPC.setProtocolEngine(conf, DatanodeProtocolPB.class,</span><br><span class="line">      ProtobufRpcEngine.class);</span><br><span class="line">  UserGroupInformation ugi = UserGroupInformation.getCurrentUser();</span><br><span class="line">  rpcProxy = createNamenode(nameNodeAddr, conf, ugi);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private static DatanodeProtocolPB createNamenode(</span><br><span class="line">    InetSocketAddress nameNodeAddr, Configuration conf,</span><br><span class="line">    UserGroupInformation ugi) throws IOException &#123;</span><br><span class="line">  return RPC.getProxy(DatanodeProtocolPB.class,</span><br><span class="line">      RPC.getProtocolVersion(DatanodeProtocolPB.class), nameNodeAddr, ugi,</span><br><span class="line">      conf, NetUtils.getSocketFactory(conf, DatanodeProtocolPB.class));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击register</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">void register(NamespaceInfo nsInfo) throws IOException &#123;</span><br><span class="line">  // 创建注册信息</span><br><span class="line">  DatanodeRegistration newBpRegistration = bpos.createRegistration();</span><br><span class="line"></span><br><span class="line">  LOG.info(this + &quot; beginning handshake with NN&quot;);</span><br><span class="line"></span><br><span class="line">  while (shouldRun()) &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      // Use returned registration from namenode with updated fields</span><br><span class="line">	  // 把注册信息发送给NN（DN调用接口方法，执行在NN）</span><br><span class="line">      newBpRegistration = bpNamenode.registerDatanode(newBpRegistration);</span><br><span class="line">      newBpRegistration.setNamespaceInfo(nsInfo);</span><br><span class="line">      bpRegistration = newBpRegistration;</span><br><span class="line">      break;</span><br><span class="line">    &#125; catch(EOFException e) &#123;  // namenode might have just restarted</span><br><span class="line">      LOG.info(&quot;Problem connecting to server: &quot; + nnAddr + &quot; :&quot;</span><br><span class="line">          + e.getLocalizedMessage());</span><br><span class="line">      sleepAndLogInterrupts(1000, &quot;connecting to server&quot;);</span><br><span class="line">    &#125; catch(SocketTimeoutException e) &#123;  // namenode is busy</span><br><span class="line">      LOG.info(&quot;Problem connecting to server: &quot; + nnAddr);</span><br><span class="line">      sleepAndLogInterrupts(1000, &quot;connecting to server&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  … …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>ctrl + n 搜索NameNodeRpcServer</p>
<p>    NameNodeRpcServer.java</p>
<p>ctrl + f 在NameNodeRpcServer.java中搜索registerDatanode</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public DatanodeRegistration registerDatanode(DatanodeRegistration nodeReg)</span><br><span class="line">    throws IOException &#123;</span><br><span class="line">  checkNNStartup();</span><br><span class="line">  verifySoftwareVersion(nodeReg);</span><br><span class="line">  // 注册DN</span><br><span class="line">  namesystem.registerDatanode(nodeReg);</span><br><span class="line"></span><br><span class="line">  return nodeReg;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>    FSNamesystem.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">void registerDatanode(DatanodeRegistration nodeReg) throws IOException &#123;</span><br><span class="line">  writeLock();</span><br><span class="line">  try &#123;</span><br><span class="line">    blockManager.registerDatanode(nodeReg);</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    writeUnlock(&quot;registerDatanode&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>    BlockManager.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">public void registerDatanode(DatanodeRegistration nodeReg)</span><br><span class="line">    throws IOException &#123;</span><br><span class="line">  assert namesystem.hasWriteLock();</span><br><span class="line">  datanodeManager.registerDatanode(nodeReg);</span><br><span class="line">  bmSafeMode.checkSafeMode();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void registerDatanode(DatanodeRegistration nodeReg)</span><br><span class="line">    throws DisallowedDatanodeException, UnresolvedTopologyException &#123;</span><br><span class="line">	... ...</span><br><span class="line">	// register new datanode 注册DN</span><br><span class="line">    addDatanode(nodeDescr);</span><br><span class="line">    blockManager.getBlockReportLeaseManager().register(nodeDescr);</span><br><span class="line">    // also treat the registration message as a heartbeat</span><br><span class="line">    // no need to update its timestamp</span><br><span class="line">    // because its is done when the descriptor is created</span><br><span class="line">	// 将DN添加到心跳管理</span><br><span class="line">    heartbeatManager.addDatanode(nodeDescr);</span><br><span class="line">    heartbeatManager.updateDnStat(nodeDescr);</span><br><span class="line">    incrementVersionCount(nodeReg.getSoftwareVersion());</span><br><span class="line">    startAdminOperationIfNecessary(nodeDescr);</span><br><span class="line">    success = true;</span><br><span class="line">	... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void addDatanode(final DatanodeDescriptor node) &#123;</span><br><span class="line">  // To keep host2DatanodeMap consistent with datanodeMap,</span><br><span class="line">  // remove  from host2DatanodeMap the datanodeDescriptor removed</span><br><span class="line">  // from datanodeMap before adding node to host2DatanodeMap.</span><br><span class="line">  synchronized(this) &#123;</span><br><span class="line">    host2DatanodeMap.remove(datanodeMap.put(node.getDatanodeUuid(), node));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  networktopology.add(node); // may throw InvalidTopologyException</span><br><span class="line">  host2DatanodeMap.add(node);</span><br><span class="line">  checkIfClusterIsNowMultiRack(node);</span><br><span class="line">  resolveUpgradeDomain(node);</span><br><span class="line"></span><br><span class="line">  … …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="2-5-向NN发送心跳"><a href="#2-5-向NN发送心跳" class="headerlink" title="2.5 向NN发送心跳"></a>2.5 向NN发送心跳</h2><p>点击BPServiceActor.java中的run方法中的offerService方法</p>
<p>    BPServiceActor.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">private void offerService() throws Exception &#123;</span><br><span class="line"></span><br><span class="line">	while (shouldRun()) &#123;</span><br><span class="line">        ... ...</span><br><span class="line">        HeartbeatResponse resp = null;</span><br><span class="line">        if (sendHeartbeat) &#123;</span><br><span class="line"></span><br><span class="line">          boolean requestBlockReportLease = (fullBlockReportLeaseId == 0) &amp;&amp;</span><br><span class="line">                  scheduler.isBlockReportDue(startTime);</span><br><span class="line">          if (!dn.areHeartbeatsDisabledForTests()) &#123;</span><br><span class="line">		    // 发送心跳信息</span><br><span class="line">            resp = sendHeartBeat(requestBlockReportLease);</span><br><span class="line">            assert resp != null;</span><br><span class="line">            if (resp.getFullBlockReportLeaseId() != 0) &#123;</span><br><span class="line">              if (fullBlockReportLeaseId != 0) &#123;</span><br><span class="line">				... ...</span><br><span class="line">              &#125;</span><br><span class="line">              fullBlockReportLeaseId = resp.getFullBlockReportLeaseId();</span><br><span class="line">            &#125;</span><br><span class="line">            ... ...</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">		... ...</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)</span><br><span class="line">    throws IOException &#123;</span><br><span class="line">	... ...</span><br><span class="line">	// 通过NN的RPC客户端发送给NN</span><br><span class="line">	HeartbeatResponse response = bpNamenode.sendHeartbeat(bpRegistration,</span><br><span class="line">        reports,</span><br><span class="line">        dn.getFSDataset().getCacheCapacity(),</span><br><span class="line">        dn.getFSDataset().getCacheUsed(),</span><br><span class="line">        dn.getXmitsInProgress(),</span><br><span class="line">        dn.getXceiverCount(),</span><br><span class="line">        numFailedVolumes,</span><br><span class="line">        volumeFailureSummary,</span><br><span class="line">        requestBlockReportLease,</span><br><span class="line">        slowPeers,</span><br><span class="line">        slowDisks);</span><br><span class="line">	... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>ctrl + n 搜索NameNodeRpcServer</p>
<p>    NameNodeRpcServer.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">public HeartbeatResponse sendHeartbeat(DatanodeRegistration nodeReg,</span><br><span class="line">    StorageReport[] report, long dnCacheCapacity, long dnCacheUsed,</span><br><span class="line">    int xmitsInProgress, int xceiverCount,</span><br><span class="line">    int failedVolumes, VolumeFailureSummary volumeFailureSummary,</span><br><span class="line">    boolean requestFullBlockReportLease,</span><br><span class="line">    @Nonnull SlowPeerReports slowPeers,</span><br><span class="line">@Nonnull SlowDiskReports slowDisks) throws IOException &#123;</span><br><span class="line"></span><br><span class="line">  checkNNStartup();</span><br><span class="line">  verifyRequest(nodeReg);</span><br><span class="line"></span><br><span class="line">  // 处理DN发送的心跳</span><br><span class="line">  return namesystem.handleHeartbeat(nodeReg, report,</span><br><span class="line">      dnCacheCapacity, dnCacheUsed, xceiverCount, xmitsInProgress,</span><br><span class="line">      failedVolumes, volumeFailureSummary, requestFullBlockReportLease,</span><br><span class="line">      slowPeers, slowDisks);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">HeartbeatResponse handleHeartbeat(DatanodeRegistration nodeReg,</span><br><span class="line">    StorageReport[] reports, long cacheCapacity, long cacheUsed,</span><br><span class="line">    int xceiverCount, int xmitsInProgress, int failedVolumes,</span><br><span class="line">    VolumeFailureSummary volumeFailureSummary,</span><br><span class="line">    boolean requestFullBlockReportLease,</span><br><span class="line">    @Nonnull SlowPeerReports slowPeers,</span><br><span class="line">    @Nonnull SlowDiskReports slowDisks) throws IOException &#123;</span><br><span class="line">  readLock();</span><br><span class="line">  try &#123;</span><br><span class="line">    //get datanode commands</span><br><span class="line">    final int maxTransfer = blockManager.getMaxReplicationStreams()</span><br><span class="line">        - xmitsInProgress;</span><br><span class="line">	// 处理DN发送过来的心跳</span><br><span class="line">    DatanodeCommand[] cmds = blockManager.getDatanodeManager().handleHeartbeat(</span><br><span class="line">        nodeReg, reports, getBlockPoolId(), cacheCapacity, cacheUsed,</span><br><span class="line">        xceiverCount, maxTransfer, failedVolumes, volumeFailureSummary,</span><br><span class="line">        slowPeers, slowDisks);</span><br><span class="line"></span><br><span class="line">    long blockReportLeaseId = 0;</span><br><span class="line">    if (requestFullBlockReportLease) &#123;</span><br><span class="line">      blockReportLeaseId =  blockManager.requestBlockReportLeaseId(nodeReg);</span><br><span class="line">    &#125;</span><br><span class="line">    //create ha status</span><br><span class="line">    final NNHAStatusHeartbeat haState = new NNHAStatusHeartbeat(</span><br><span class="line">        haContext.getState().getServiceState(),</span><br><span class="line">        getFSImage().getCorrectLastAppliedOrWrittenTxId());</span><br><span class="line"></span><br><span class="line">	// 响应DN的心跳</span><br><span class="line">    return new HeartbeatResponse(cmds, haState, rollingUpgradeInfo,</span><br><span class="line">        blockReportLeaseId);</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    readUnlock(&quot;handleHeartbeat&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击handleHeartbeat</p>
<p>    DatanodeManager.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public DatanodeCommand[] handleHeartbeat(DatanodeRegistration nodeReg,</span><br><span class="line">    StorageReport[] reports, final String blockPoolId,</span><br><span class="line">    long cacheCapacity, long cacheUsed, int xceiverCount, </span><br><span class="line">    int maxTransfers, int failedVolumes,</span><br><span class="line">    VolumeFailureSummary volumeFailureSummary,</span><br><span class="line">    @Nonnull SlowPeerReports slowPeers,</span><br><span class="line">    @Nonnull SlowDiskReports slowDisks) throws IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  heartbeatManager.updateHeartbeat(nodeinfo, reports, cacheCapacity,</span><br><span class="line">      cacheUsed, xceiverCount, failedVolumes, volumeFailureSummary);</span><br><span class="line">  ... ...  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>    HeartbeatManager.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">synchronized void updateHeartbeat(final DatanodeDescriptor node,</span><br><span class="line">    StorageReport[] reports, long cacheCapacity, long cacheUsed,</span><br><span class="line">    int xceiverCount, int failedVolumes,</span><br><span class="line">    VolumeFailureSummary volumeFailureSummary) &#123;</span><br><span class="line">  stats.subtract(node);</span><br><span class="line">  blockManager.updateHeartbeat(node, reports, cacheCapacity, cacheUsed,</span><br><span class="line">      xceiverCount, failedVolumes, volumeFailureSummary);</span><br><span class="line">  stats.add(node);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>    BlockManager.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">void updateHeartbeat(DatanodeDescriptor node, StorageReport[] reports,</span><br><span class="line">    long cacheCapacity, long cacheUsed, int xceiverCount, int failedVolumes,</span><br><span class="line">    VolumeFailureSummary volumeFailureSummary) &#123;</span><br><span class="line"></span><br><span class="line">  for (StorageReport report: reports) &#123;</span><br><span class="line">    providedStorageMap.updateStorage(node, report.getStorage());</span><br><span class="line">  &#125;</span><br><span class="line">  node.updateHeartbeat(reports, cacheCapacity, cacheUsed, xceiverCount,</span><br><span class="line">      failedVolumes, volumeFailureSummary);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>    DatanodeDescriptor.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">void updateHeartbeat(StorageReport[] reports, long cacheCapacity,</span><br><span class="line">    long cacheUsed, int xceiverCount, int volFailures,</span><br><span class="line">    VolumeFailureSummary volumeFailureSummary) &#123;</span><br><span class="line">  updateHeartbeatState(reports, cacheCapacity, cacheUsed, xceiverCount,</span><br><span class="line">      volFailures, volumeFailureSummary);</span><br><span class="line">  heartbeatedSinceRegistration = true;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void updateHeartbeatState(StorageReport[] reports, long cacheCapacity,</span><br><span class="line">    long cacheUsed, int xceiverCount, int volFailures,</span><br><span class="line">    VolumeFailureSummary volumeFailureSummary) &#123;</span><br><span class="line">  // 更新存储</span><br><span class="line">  updateStorageStats(reports, cacheCapacity, cacheUsed, xceiverCount,</span><br><span class="line">      volFailures, volumeFailureSummary);</span><br><span class="line">  // 更新心跳时间</span><br><span class="line">  setLastUpdate(Time.now());</span><br><span class="line">  setLastUpdateMonotonic(Time.monotonicNow());</span><br><span class="line">  rollBlocksScheduled(getLastUpdateMonotonic());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private void updateStorageStats(StorageReport[] reports, long cacheCapacity,</span><br><span class="line">    long cacheUsed, int xceiverCount, int volFailures,</span><br><span class="line">    VolumeFailureSummary volumeFailureSummary) &#123;</span><br><span class="line">  long totalCapacity = 0;</span><br><span class="line">  long totalRemaining = 0;</span><br><span class="line">  long totalBlockPoolUsed = 0;</span><br><span class="line">  long totalDfsUsed = 0;</span><br><span class="line">  long totalNonDfsUsed = 0;</span><br><span class="line">  … …</span><br><span class="line"></span><br><span class="line">  setCacheCapacity(cacheCapacity);</span><br><span class="line">  setCacheUsed(cacheUsed);</span><br><span class="line">  setXceiverCount(xceiverCount);</span><br><span class="line">  this.volumeFailures = volFailures;</span><br><span class="line">  this.volumeFailureSummary = volumeFailureSummary;</span><br><span class="line">  for (StorageReport report : reports) &#123;</span><br><span class="line"></span><br><span class="line">    DatanodeStorageInfo storage =</span><br><span class="line">        storageMap.get(report.getStorage().getStorageID());</span><br><span class="line">    if (checkFailedStorages) &#123;</span><br><span class="line">      failedStorageInfos.remove(storage);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    storage.receivedHeartbeat(report);</span><br><span class="line">    // skip accounting for capacity of PROVIDED storages!</span><br><span class="line">    if (StorageType.PROVIDED.equals(storage.getStorageType())) &#123;</span><br><span class="line">      continue;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    totalCapacity += report.getCapacity();</span><br><span class="line">    totalRemaining += report.getRemaining();</span><br><span class="line">    totalBlockPoolUsed += report.getBlockPoolUsed();</span><br><span class="line">    totalDfsUsed += report.getDfsUsed();</span><br><span class="line">    totalNonDfsUsed += report.getNonDfsUsed();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // Update total metrics for the node.</span><br><span class="line">  // 更新存储相关信息</span><br><span class="line">  setCapacity(totalCapacity);</span><br><span class="line">  setRemaining(totalRemaining);</span><br><span class="line">  setBlockPoolUsed(totalBlockPoolUsed);</span><br><span class="line">  setDfsUsed(totalDfsUsed);</span><br><span class="line">  setNonDfsUsed(totalNonDfsUsed);</span><br><span class="line">  if (checkFailedStorages) &#123;</span><br><span class="line">    updateFailedStorage(failedStorageInfos);</span><br><span class="line">  &#125;</span><br><span class="line">  long storageMapSize;</span><br><span class="line">  synchronized (storageMap) &#123;</span><br><span class="line">    storageMapSize = storageMap.size();</span><br><span class="line">  &#125;</span><br><span class="line">  if (storageMapSize != reports.length) &#123;</span><br><span class="line">    pruneStorageMap(reports);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h1 id="第3章-HDFS上传源码解析"><a href="#第3章-HDFS上传源码解析" class="headerlink" title="第3章 HDFS上传源码解析"></a>第3章 HDFS上传源码解析</h1><p><img src="https://image.3001.net/images/20221031/16671884819863.png#alt=image-20221031115434492"></p>
<p><img src="https://image.3001.net/images/20221031/16671884897129.png#alt=image-20221031115443262"></p>
<h2 id="3-1-create创建过程"><a href="#3-1-create创建过程" class="headerlink" title="3.1 create创建过程"></a>3.1 create创建过程</h2><p>添加依赖</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">		&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">		&lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">		&lt;version&gt;3.1.3&lt;/version&gt;</span><br><span class="line">	&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">		&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">		&lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;</span><br><span class="line">		&lt;version&gt;3.1.3&lt;/version&gt;</span><br><span class="line">	&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">		&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">		&lt;artifactId&gt;hadoop-hdfs-client&lt;/artifactId&gt;</span><br><span class="line">		&lt;version&gt;3.1.3&lt;/version&gt;</span><br><span class="line">		&lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">	&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">		&lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">		&lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">		&lt;version&gt;4.12&lt;/version&gt;</span><br><span class="line">	&lt;/dependency&gt;</span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">		&lt;groupId&gt;org.slf4j&lt;/groupId&gt;</span><br><span class="line">		&lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;</span><br><span class="line">		&lt;version&gt;1.7.30&lt;/version&gt;</span><br><span class="line">	&lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure>


<h3 id="3-1-1-DN向NN发起创建请求"><a href="#3-1-1-DN向NN发起创建请求" class="headerlink" title="3.1.1 DN向NN发起创建请求"></a>3.1.1 DN向NN发起创建请求</h3><p>用户自己写的代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testPut2() throws IOException &#123;</span><br><span class="line">	FSDataOutputStream fos = fs.create(new Path(&quot;/input&quot;));</span><br><span class="line"></span><br><span class="line">	fos.write(&quot;hello world&quot;.getBytes());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>FileSystem.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">public FSDataOutputStream create(Path f) throws IOException &#123;</span><br><span class="line">	return create(f, true);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public FSDataOutputStream create(Path f, boolean overwrite)</span><br><span class="line">  throws IOException &#123;</span><br><span class="line">  </span><br><span class="line">	return create(f, overwrite,</span><br><span class="line">			  getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,</span><br><span class="line">				  IO_FILE_BUFFER_SIZE_DEFAULT),</span><br><span class="line">			  getDefaultReplication(f),</span><br><span class="line">			  getDefaultBlockSize(f));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public FSDataOutputStream create(Path f,</span><br><span class="line">	boolean overwrite,</span><br><span class="line">	int bufferSize,</span><br><span class="line">	short replication,</span><br><span class="line">	long blockSize) throws IOException &#123;</span><br><span class="line">	</span><br><span class="line">	return create(f, overwrite, bufferSize, replication, blockSize, null);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public FSDataOutputStream create(Path f,</span><br><span class="line">	boolean overwrite,</span><br><span class="line">	int bufferSize,</span><br><span class="line">	short replication,</span><br><span class="line">	long blockSize,</span><br><span class="line">	Progressable progress</span><br><span class="line">	) throws IOException &#123;</span><br><span class="line">										</span><br><span class="line">	return this.create(f, FsCreateModes.applyUMask(</span><br><span class="line">	FsPermission.getFileDefault(), FsPermission.getUMask(getConf())),</span><br><span class="line">	overwrite, bufferSize, replication, blockSize, progress);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public abstract FSDataOutputStream create(Path f,</span><br><span class="line">	FsPermission permission,</span><br><span class="line">	boolean overwrite,</span><br><span class="line">	int bufferSize,</span><br><span class="line">	short replication,</span><br><span class="line">	long blockSize,</span><br><span class="line">	Progressable progress) throws IOException;</span><br></pre></td></tr></table></figure>

<p>选中create，点击ctrl+h，找到实现类DistributedFileSystem.java，查找create方法。</p>
<p>DistributedFileSystem.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public FSDataOutputStream create(Path f, FsPermission permission,</span><br><span class="line">  boolean overwrite, int bufferSize, short replication, long blockSize,</span><br><span class="line">  Progressable progress) throws IOException &#123;</span><br><span class="line">  </span><br><span class="line">	return this.create(f, permission,</span><br><span class="line">	overwrite ? EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)</span><br><span class="line">		: EnumSet.of(CreateFlag.CREATE), bufferSize, replication,</span><br><span class="line">	blockSize, progress, null);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public FSDataOutputStream create(final Path f, final FsPermission permission,</span><br><span class="line">  final EnumSet&lt;CreateFlag&gt; cflags, final int bufferSize,</span><br><span class="line">  final short replication, final long blockSize,</span><br><span class="line">  final Progressable progress, final ChecksumOpt checksumOpt)</span><br><span class="line">  throws IOException &#123;</span><br><span class="line">  </span><br><span class="line">	statistics.incrementWriteOps(1);</span><br><span class="line">	storageStatistics.incrementOpCounter(OpType.CREATE);</span><br><span class="line">	Path absF = fixRelativePart(f);</span><br><span class="line">	</span><br><span class="line">	return new FileSystemLinkResolver&lt;FSDataOutputStream&gt;() &#123;</span><br><span class="line"></span><br><span class="line">	  @Override</span><br><span class="line">	  public FSDataOutputStream doCall(final Path p) throws IOException &#123;</span><br><span class="line"></span><br><span class="line">		// 创建获取了一个输出流对象</span><br><span class="line">		final DFSOutputStream dfsos = dfs.create(getPathName(p), permission,</span><br><span class="line">			cflags, replication, blockSize, progress, bufferSize,</span><br><span class="line">			checksumOpt);</span><br><span class="line">		// 这里将上面创建的dfsos进行包装并返回</span><br><span class="line">		return dfs.createWrappedOutputStream(dfsos, statistics);</span><br><span class="line">	  &#125;</span><br><span class="line"></span><br><span class="line">	  @Override</span><br><span class="line">	  public FSDataOutputStream next(final FileSystem fs, final Path p)</span><br><span class="line">		  throws IOException &#123;</span><br><span class="line">		return fs.create(p, permission, cflags, bufferSize,</span><br><span class="line">			replication, blockSize, progress, checksumOpt);</span><br><span class="line">	  &#125;</span><br><span class="line">	&#125;.resolve(this, absF);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击create，进入DFSClient.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">public DFSOutputStream create(String src, FsPermission permission,</span><br><span class="line">  EnumSet&lt;CreateFlag&gt; flag, short replication, long blockSize,</span><br><span class="line">  Progressable progress, int buffersize, ChecksumOpt checksumOpt)</span><br><span class="line">  throws IOException &#123;</span><br><span class="line">  </span><br><span class="line">	return create(src, permission, flag, true,</span><br><span class="line">	replication, blockSize, progress, buffersize, checksumOpt, null);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public DFSOutputStream create(String src, FsPermission permission,</span><br><span class="line">  EnumSet&lt;CreateFlag&gt; flag, boolean createParent, short replication,</span><br><span class="line">  long blockSize, Progressable progress, int buffersize,</span><br><span class="line">  ChecksumOpt checksumOpt, InetSocketAddress[] favoredNodes)</span><br><span class="line">  throws IOException &#123;</span><br><span class="line">  </span><br><span class="line">	return create(src, permission, flag, createParent, replication, blockSize,</span><br><span class="line">	progress, buffersize, checksumOpt, favoredNodes, null);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public DFSOutputStream create(String src, FsPermission permission,</span><br><span class="line">  EnumSet&lt;CreateFlag&gt; flag, boolean createParent, short replication,</span><br><span class="line">  long blockSize, Progressable progress, int buffersize,</span><br><span class="line">  ChecksumOpt checksumOpt, InetSocketAddress[] favoredNodes,</span><br><span class="line">  String ecPolicyName) throws IOException &#123;</span><br><span class="line">  </span><br><span class="line">	checkOpen();</span><br><span class="line">	</span><br><span class="line">	final FsPermission masked = applyUMask(permission);</span><br><span class="line">	LOG.debug(&quot;&#123;&#125;: masked=&#123;&#125;&quot;, src, masked);</span><br><span class="line">	</span><br><span class="line">	final DFSOutputStream result = DFSOutputStream.newStreamForCreate(this,</span><br><span class="line">		src, masked, flag, createParent, replication, blockSize, progress,</span><br><span class="line">		dfsClientConf.createChecksum(checksumOpt),</span><br><span class="line">		getFavoredNodesStr(favoredNodes), ecPolicyName);</span><br><span class="line">		</span><br><span class="line">	beginFileLease(result.getFileId(), result);</span><br><span class="line">	</span><br><span class="line">	return result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击newStreamForCreate，进入DFSOutputStream.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,</span><br><span class="line">  FsPermission masked, EnumSet&lt;CreateFlag&gt; flag, boolean createParent,</span><br><span class="line">  short replication, long blockSize, Progressable progress,</span><br><span class="line">  DataChecksum checksum, String[] favoredNodes, String ecPolicyName)</span><br><span class="line">  throws IOException &#123;</span><br><span class="line">  </span><br><span class="line">	try (TraceScope ignored =</span><br><span class="line">			 dfsClient.newPathTraceScope(&quot;newStreamForCreate&quot;, src)) &#123;</span><br><span class="line">	  HdfsFileStatus stat = null;</span><br><span class="line"></span><br><span class="line">	  // Retry the create if we get a RetryStartFileException up to a maximum</span><br><span class="line">	  // number of times</span><br><span class="line">	  boolean shouldRetry = true;</span><br><span class="line">	  int retryCount = CREATE_RETRY_COUNT;</span><br><span class="line"></span><br><span class="line">	  while (shouldRetry) &#123;</span><br><span class="line">		shouldRetry = false;</span><br><span class="line">		try &#123;</span><br><span class="line">		  // DN将创建请求发送给NN（RPC）</span><br><span class="line">		  stat = dfsClient.namenode.create(src, masked, dfsClient.clientName,</span><br><span class="line">			  new EnumSetWritable&lt;&gt;(flag), createParent, replication,</span><br><span class="line">			  blockSize, SUPPORTED_CRYPTO_VERSIONS, ecPolicyName);</span><br><span class="line">		  break;</span><br><span class="line">		&#125; catch (RemoteException re) &#123;</span><br><span class="line">		  … ….</span><br><span class="line">		&#125;</span><br><span class="line">	  &#125;</span><br><span class="line">	  Preconditions.checkNotNull(stat, &quot;HdfsFileStatus should not be null!&quot;);</span><br><span class="line">	  final DFSOutputStream out;</span><br><span class="line"></span><br><span class="line">	  if(stat.getErasureCodingPolicy() != null) &#123;</span><br><span class="line">		out = new DFSStripedOutputStream(dfsClient, src, stat,</span><br><span class="line">			flag, progress, checksum, favoredNodes);</span><br><span class="line">	  &#125; else &#123;</span><br><span class="line">		out = new DFSOutputStream(dfsClient, src, stat,</span><br><span class="line">			flag, progress, checksum, favoredNodes, true);</span><br><span class="line">	  &#125;</span><br><span class="line"></span><br><span class="line">	  // 开启线程run，DataStreamer extends Daemon extends Thread</span><br><span class="line">	  out.start();</span><br><span class="line"></span><br><span class="line">	  return out;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3 id="3-1-2-NN处理DN的创建请求"><a href="#3-1-2-NN处理DN的创建请求" class="headerlink" title="3.1.2 NN处理DN的创建请求"></a>3.1.2 NN处理DN的创建请求</h3><p>1）点击create</p>
<p>    ClientProtocol.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HdfsFileStatus create(String src, FsPermission masked,</span><br><span class="line">    String clientName, EnumSetWritable&lt;CreateFlag&gt; flag,</span><br><span class="line">    boolean createParent, short replication, long blockSize,</span><br><span class="line">    CryptoProtocolVersion[] supportedVersions, String ecPolicyName)</span><br><span class="line">    throws IOException;</span><br></pre></td></tr></table></figure>

<p>2）Ctrl + h查找create实现类，点击NameNodeRpcServer，在NameNodeRpcServer.java中搜索create</p>
<p>    NameNodeRpcServer.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">public HdfsFileStatus create(String src, FsPermission masked,</span><br><span class="line">    String clientName, EnumSetWritable&lt;CreateFlag&gt; flag,</span><br><span class="line">    boolean createParent, short replication, long blockSize,</span><br><span class="line">    CryptoProtocolVersion[] supportedVersions, String ecPolicyName)</span><br><span class="line">    throws IOException &#123;</span><br><span class="line">  // 检查NN启动</span><br><span class="line">  checkNNStartup();</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  HdfsFileStatus status = null;</span><br><span class="line">  try &#123;</span><br><span class="line">    PermissionStatus perm = new PermissionStatus(getRemoteUser()</span><br><span class="line">        .getShortUserName(), null, masked);</span><br><span class="line">	// 重要</span><br><span class="line">    status = namesystem.startFile(src, perm, clientName, clientMachine,</span><br><span class="line">        flag.get(), createParent, replication, blockSize, supportedVersions,</span><br><span class="line">        ecPolicyName, cacheEntry != null);</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    RetryCache.setState(cacheEntry, status != null, status);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  metrics.incrFilesCreated();</span><br><span class="line">  metrics.incrCreateFileOps();</span><br><span class="line">  return status;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>FSNamesystem.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line">HdfsFileStatus startFile(String src, PermissionStatus permissions,</span><br><span class="line">    String holder, String clientMachine, EnumSet&lt;CreateFlag&gt; flag,</span><br><span class="line">    boolean createParent, short replication, long blockSize,</span><br><span class="line">    CryptoProtocolVersion[] supportedVersions, String ecPolicyName,</span><br><span class="line">    boolean logRetryCache) throws IOException &#123;</span><br><span class="line"></span><br><span class="line">  HdfsFileStatus status;</span><br><span class="line">  try &#123;</span><br><span class="line">    status = startFileInt(src, permissions, holder, clientMachine, flag,</span><br><span class="line">        createParent, replication, blockSize, supportedVersions, ecPolicyName,</span><br><span class="line">        logRetryCache);</span><br><span class="line">  &#125; catch (AccessControlException e) &#123;</span><br><span class="line">    logAuditEvent(false, &quot;create&quot;, src);</span><br><span class="line">    throw e;</span><br><span class="line">  &#125;</span><br><span class="line">  logAuditEvent(true, &quot;create&quot;, src, status);</span><br><span class="line">  return status;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private HdfsFileStatus startFileInt(String src,</span><br><span class="line">    PermissionStatus permissions, String holder, String clientMachine,</span><br><span class="line">    EnumSet&lt;CreateFlag&gt; flag, boolean createParent, short replication,</span><br><span class="line">    long blockSize, CryptoProtocolVersion[] supportedVersions,</span><br><span class="line">    String ecPolicyName, boolean logRetryCache) throws IOException &#123;       </span><br><span class="line">	... ...</span><br><span class="line">	stat = FSDirWriteFileOp.startFile(this, iip, permissions, holder,</span><br><span class="line">        clientMachine, flag, createParent, replication, blockSize, feInfo,</span><br><span class="line">        toRemoveBlocks, shouldReplicate, ecPolicyName, logRetryCache);</span><br><span class="line">	... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static HdfsFileStatus startFile(</span><br><span class="line">    ... ...)</span><br><span class="line">    throws IOException &#123;</span><br><span class="line">	</span><br><span class="line">  ... ...</span><br><span class="line">  FSDirectory fsd = fsn.getFSDirectory();</span><br><span class="line"></span><br><span class="line">  // 文件路径是否存在校验</span><br><span class="line">  if (iip.getLastINode() != null) &#123;</span><br><span class="line">    if (overwrite) &#123;</span><br><span class="line">      List&lt;INode&gt; toRemoveINodes = new ChunkedArrayList&lt;&gt;();</span><br><span class="line">      List&lt;Long&gt; toRemoveUCFiles = new ChunkedArrayList&lt;&gt;();</span><br><span class="line">      long ret = FSDirDeleteOp.delete(fsd, iip, toRemoveBlocks,</span><br><span class="line">                                      toRemoveINodes, toRemoveUCFiles, now());</span><br><span class="line">      if (ret &gt;= 0) &#123;</span><br><span class="line">        iip = INodesInPath.replace(iip, iip.length() - 1, null);</span><br><span class="line">        FSDirDeleteOp.incrDeletedFileCount(ret);</span><br><span class="line">        fsn.removeLeasesAndINodes(toRemoveUCFiles, toRemoveINodes, true);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      // If lease soft limit time is expired, recover the lease</span><br><span class="line">      fsn.recoverLeaseInternal(FSNamesystem.RecoverLeaseOp.CREATE_FILE, iip,</span><br><span class="line">                               src, holder, clientMachine, false);</span><br><span class="line">      throw new FileAlreadyExistsException(src + &quot; for client &quot; +</span><br><span class="line">          clientMachine + &quot; already exists&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  fsn.checkFsObjectLimit();</span><br><span class="line">  INodeFile newNode = null;</span><br><span class="line">  INodesInPath parent = FSDirMkdirOp.createAncestorDirectories(fsd, iip, permissions);</span><br><span class="line">  if (parent != null) &#123;</span><br><span class="line">    // 添加文件元数据信息</span><br><span class="line">    iip = addFile(fsd, parent, iip.getLastLocalName(), permissions,</span><br><span class="line">        replication, blockSize, holder, clientMachine, shouldReplicate,</span><br><span class="line">        ecPolicyName);</span><br><span class="line">    newNode = iip != null ? iip.getLastINode().asFile() : null;</span><br><span class="line">  &#125;</span><br><span class="line">  ... ...</span><br><span class="line">  setNewINodeStoragePolicy(fsd.getBlockManager(), iip, isLazyPersist);</span><br><span class="line">  fsd.getEditLog().logOpenFile(src, newNode, overwrite, logRetryEntry);</span><br><span class="line">  if (NameNode.stateChangeLog.isDebugEnabled()) &#123;</span><br><span class="line">    NameNode.stateChangeLog.debug(&quot;DIR* NameSystem.startFile: added &quot; +</span><br><span class="line">        src + &quot; inode &quot; + newNode.getId() + &quot; &quot; + holder);</span><br><span class="line">  &#125;</span><br><span class="line">  return FSDirStatAndListingOp.getFileInfo(fsd, iip, false, false);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private static INodesInPath addFile(</span><br><span class="line">    FSDirectory fsd, INodesInPath existing, byte[] localName,</span><br><span class="line">    PermissionStatus permissions, short replication, long preferredBlockSize,</span><br><span class="line">    String clientName, String clientMachine, boolean shouldReplicate,</span><br><span class="line">    String ecPolicyName) throws IOException &#123;</span><br><span class="line"></span><br><span class="line">  Preconditions.checkNotNull(existing);</span><br><span class="line">  long modTime = now();</span><br><span class="line">  INodesInPath newiip;</span><br><span class="line">  fsd.writeLock();</span><br><span class="line">  try &#123;</span><br><span class="line">    … …</span><br><span class="line"></span><br><span class="line">    newiip = fsd.addINode(existing, newNode, permissions.getPermission());</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    fsd.writeUnlock();</span><br><span class="line">  &#125;</span><br><span class="line">  ... ...</span><br><span class="line">  return newiip;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">INodesInPath addINode(INodesInPath existing, INode child,</span><br><span class="line">                      FsPermission modes)</span><br><span class="line">    throws QuotaExceededException, UnresolvedLinkException &#123;</span><br><span class="line">  cacheName(child);</span><br><span class="line">  writeLock();</span><br><span class="line">  try &#123;</span><br><span class="line">    // 将数据写入到INode的目录树中</span><br><span class="line">    return addLastINode(existing, child, modes, true);</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    writeUnlock();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3 id="3-1-3-DataStreamer启动流程"><a href="#3-1-3-DataStreamer启动流程" class="headerlink" title="3.1.3 DataStreamer启动流程"></a>3.1.3 DataStreamer启动流程</h3><p>NN处理完DN请求后，再次回到DN端，启动对应的线程</p>
<p>DFSOutputStream.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,</span><br><span class="line">  FsPermission masked, EnumSet&lt;CreateFlag&gt; flag, boolean createParent,</span><br><span class="line">  short replication, long blockSize, Progressable progress,</span><br><span class="line">  DataChecksum checksum, String[] favoredNodes, String ecPolicyName)</span><br><span class="line">  throws IOException &#123;</span><br><span class="line">	... ...</span><br><span class="line">	// DN将创建请求发送给NN（RPC）</span><br><span class="line">	stat = dfsClient.namenode.create(src, masked, dfsClient.clientName,</span><br><span class="line">	  new EnumSetWritable&lt;&gt;(flag), createParent, replication,</span><br><span class="line">	  blockSize, SUPPORTED_CRYPTO_VERSIONS, ecPolicyName);</span><br><span class="line">	... ...</span><br><span class="line">	</span><br><span class="line">	// 创建输出流</span><br><span class="line">	out = new DFSOutputStream(dfsClient, src, stat,</span><br><span class="line">            flag, progress, checksum, favoredNodes, true);</span><br><span class="line">	// 开启线程run，DataStreamer extends Daemon extends Thread</span><br><span class="line">	out.start();</span><br><span class="line"></span><br><span class="line">	return out;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击DFSOutputStream</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">protected DFSOutputStream(DFSClient dfsClient, String src,</span><br><span class="line">    HdfsFileStatus stat, EnumSet&lt;CreateFlag&gt; flag, Progressable progress,</span><br><span class="line">    DataChecksum checksum, String[] favoredNodes, boolean createStreamer) &#123;</span><br><span class="line">  this(dfsClient, src, flag, progress, stat, checksum);</span><br><span class="line">  this.shouldSyncBlock = flag.contains(CreateFlag.SYNC_BLOCK);</span><br><span class="line"></span><br><span class="line">  // Directory =&gt; File =&gt; Block(128M) =&gt; packet(64K) =&gt; chunk（chunk 512byte + chunksum 4byte）</span><br><span class="line">  computePacketChunkSize(dfsClient.getConf().getWritePacketSize(),</span><br><span class="line">      bytesPerChecksum);</span><br><span class="line"></span><br><span class="line">  if (createStreamer) &#123;</span><br><span class="line">    streamer = new DataStreamer(stat, null, dfsClient, src, progress,</span><br><span class="line">        checksum, cachingStrategy, byteArrayManager, favoredNodes,</span><br><span class="line">        addBlockFlags);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>1）点击newStreamForCreate方法中的out.start()，进入DFSOutputStream.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">protected synchronized void start() &#123;</span><br><span class="line">	getStreamer().start();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">protected DataStreamer getStreamer() &#123;</span><br><span class="line">	return streamer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击DataStreamer，进入DataStreamer.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class DataStreamer extends Daemon &#123;</span><br><span class="line"></span><br><span class="line">  。。。 。。。</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击Daemon，进入Daemon.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public class Daemon extends Thread &#123;</span><br><span class="line"></span><br><span class="line">  。。。 。。。</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>说明：out.start();实际是开启线程，点击DataStreamer，搜索run方法</p>
<p>DataStreamer.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void run() &#123;</span><br><span class="line"></span><br><span class="line">	long lastPacket = Time.monotonicNow();</span><br><span class="line">	TraceScope scope = null;</span><br><span class="line">	while (!streamerClosed &amp;&amp; dfsClient.clientRunning) &#123;</span><br><span class="line">	  // if the Responder encountered an error, shutdown Responder</span><br><span class="line">	  if (errorState.hasError()) &#123;</span><br><span class="line">		closeResponder();</span><br><span class="line">	  &#125;</span><br><span class="line"></span><br><span class="line">	  DFSPacket one;</span><br><span class="line">	  try &#123;</span><br><span class="line">		// process datanode IO errors if any</span><br><span class="line">		boolean doSleep = processDatanodeOrExternalError();</span><br><span class="line"></span><br><span class="line">		final int halfSocketTimeout = dfsClient.getConf().getSocketTimeout()/2;</span><br><span class="line">		synchronized (dataQueue) &#123;</span><br><span class="line">		  // wait for a packet to be sent.</span><br><span class="line">		  … …</span><br><span class="line">			try &#123;</span><br><span class="line">			  // 如果dataQueue里面没有数据，代码会阻塞在这儿</span><br><span class="line">			  dataQueue.wait(timeout);</span><br><span class="line">			&#125; catch (InterruptedException  e) &#123;</span><br><span class="line">			  LOG.warn(&quot;Caught exception&quot;, e);</span><br><span class="line">			&#125;</span><br><span class="line">			doSleep = false;</span><br><span class="line">			now = Time.monotonicNow();</span><br><span class="line">		  &#125;</span><br><span class="line">		  … …</span><br><span class="line">			//  队列不为空，从队列中取出packet</span><br><span class="line">			one = dataQueue.getFirst(); // regular data packet</span><br><span class="line">			SpanId[] parents = one.getTraceParents();</span><br><span class="line">			if (parents.length &gt; 0) &#123;</span><br><span class="line">			  scope = dfsClient.getTracer().</span><br><span class="line">				  newScope(&quot;dataStreamer&quot;, parents[0]);</span><br><span class="line">			  scope.getSpan().setParents(parents);</span><br><span class="line">			&#125;</span><br><span class="line">		  &#125;</span><br><span class="line">		&#125;</span><br><span class="line">		… …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="3-2-write上传过程"><a href="#3-2-write上传过程" class="headerlink" title="3.2 write上传过程"></a>3.2 write上传过程</h2><h3 id="3-1-1-向DataStreamer的队列里面写数据"><a href="#3-1-1-向DataStreamer的队列里面写数据" class="headerlink" title="3.1.1 向DataStreamer的队列里面写数据"></a>3.1.1 向DataStreamer的队列里面写数据</h3><p>1）用户写的代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line"></span><br><span class="line">public void testPut2() throws IOException &#123;</span><br><span class="line"></span><br><span class="line">  FSDataOutputStream fos = fs.create(new Path(&quot;/input&quot;));</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  fos.write(&quot;hello world&quot;.getBytes());</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2）点击write</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public void write(byte b[]) throws IOException &#123;</span><br><span class="line">    write(b, 0, b.length);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void write(byte b[], int off, int len) throws IOException &#123;</span><br><span class="line">    if ((off | len | (b.length - (len + off)) | (off + len)) &lt; 0)</span><br><span class="line">        throw new IndexOutOfBoundsException();</span><br><span class="line"></span><br><span class="line">    for (int i = 0 ; i &lt; len ; i++) &#123;</span><br><span class="line">        write(b[off + i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void write(int b) throws IOException &#123;</span><br><span class="line">    out.write(b);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>3）点击write</p>
<p>    OutputStream.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">public abstract void write(int b) throws IOException;</span><br></pre></td></tr></table></figure>

<p>ctrl + h 查找write实现类，选择FSOutputSummer.java，在该类中查找write</p>
<p>FSOutputSummer.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">public synchronized void write(int b) throws IOException &#123;</span><br><span class="line">  buf[count++] = (byte)b;</span><br><span class="line">  if(count == buf.length) &#123;</span><br><span class="line">    flushBuffer();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">protected synchronized void flushBuffer() throws IOException &#123;</span><br><span class="line">  flushBuffer(false, true);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">protected synchronized int flushBuffer(boolean keep,</span><br><span class="line">    boolean flushPartial) throws IOException &#123;</span><br><span class="line">  int bufLen = count;</span><br><span class="line">  int partialLen = bufLen % sum.getBytesPerChecksum();</span><br><span class="line">  int lenToFlush = flushPartial ? bufLen : bufLen - partialLen;</span><br><span class="line"></span><br><span class="line">  if (lenToFlush != 0) &#123;</span><br><span class="line">// 向队列中写数据   </span><br><span class="line">// Directory =&gt; File =&gt; Block(128M) =&gt; package(64K) =&gt; chunk（chunk 512byte + chunksum 4byte）</span><br><span class="line">writeChecksumChunks(buf, 0, lenToFlush);</span><br><span class="line"></span><br><span class="line">    if (!flushPartial || keep) &#123;</span><br><span class="line">      count = partialLen;</span><br><span class="line">      System.arraycopy(buf, bufLen - count, buf, 0, count);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      count = 0;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // total bytes left minus unflushed bytes left</span><br><span class="line">  return count - (bufLen - lenToFlush);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private void writeChecksumChunks(byte b[], int off, int len)</span><br><span class="line">throws IOException &#123;</span><br><span class="line"></span><br><span class="line">  // 计算chunk的校验和</span><br><span class="line">  sum.calculateChunkedSums(b, off, len, checksum, 0);</span><br><span class="line">  TraceScope scope = createWriteTraceScope();</span><br><span class="line"></span><br><span class="line">  // 按照chunk的大小遍历数据</span><br><span class="line">  try &#123;</span><br><span class="line">    for (int i = 0; i &lt; len; i += sum.getBytesPerChecksum()) &#123;</span><br><span class="line">      int chunkLen = Math.min(sum.getBytesPerChecksum(), len - i);</span><br><span class="line">      int ckOffset = i / sum.getBytesPerChecksum() * getChecksumSize();</span><br><span class="line"></span><br><span class="line">	  // 一个chunk一个chunk的将数据写入队列</span><br><span class="line">      writeChunk(b, off + i, chunkLen, checksum, ckOffset,</span><br><span class="line">          getChecksumSize());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    if (scope != null) &#123;</span><br><span class="line">      scope.close();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">protected abstract void writeChunk(byte[] b, int bOffset, int bLen,</span><br><span class="line">   byte[] checksum, int checksumOffset, int checksumLen) throws IOException;</span><br></pre></td></tr></table></figure>

<p>ctrl + h 查找writeChunk实现类DFSOutputStream.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">protected synchronized void writeChunk(byte[] b, int offset, int len,</span><br><span class="line">    byte[] checksum, int ckoff, int cklen) throws IOException &#123;</span><br><span class="line">  </span><br><span class="line">  writeChunkPrepare(len, ckoff, cklen);</span><br><span class="line"></span><br><span class="line">  // 往packet里面写chunk的校验和 4byte</span><br><span class="line">  currentPacket.writeChecksum(checksum, ckoff, cklen);</span><br><span class="line"></span><br><span class="line">  // 往packet里面写一个chunk 512 byte</span><br><span class="line">  currentPacket.writeData(b, offset, len);</span><br><span class="line"></span><br><span class="line">  // 记录写入packet中的chunk个数，累计到127个chuck，这个packet就满了</span><br><span class="line">  currentPacket.incNumChunks();</span><br><span class="line">  getStreamer().incBytesCurBlock(len);</span><br><span class="line"></span><br><span class="line">  // If packet is full, enqueue it for transmission</span><br><span class="line">  if (currentPacket.getNumChunks() == currentPacket.getMaxChunks() ||</span><br><span class="line">      getStreamer().getBytesCurBlock() == blockSize) &#123;</span><br><span class="line">    enqueueCurrentPacketFull();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">synchronized void enqueueCurrentPacketFull() throws IOException &#123;</span><br><span class="line">  LOG.debug(&quot;enqueue full &#123;&#125;, src=&#123;&#125;, bytesCurBlock=&#123;&#125;, blockSize=&#123;&#125;,&quot;</span><br><span class="line">          + &quot; appendChunk=&#123;&#125;, &#123;&#125;&quot;, currentPacket, src, getStreamer()</span><br><span class="line">          .getBytesCurBlock(), blockSize, getStreamer().getAppendChunk(),</span><br><span class="line">      getStreamer());</span><br><span class="line"></span><br><span class="line">  enqueueCurrentPacket();</span><br><span class="line"></span><br><span class="line">  adjustChunkBoundary();</span><br><span class="line"></span><br><span class="line">  endBlock();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void enqueueCurrentPacket() throws IOException &#123;</span><br><span class="line">  getStreamer().waitAndQueuePacket(currentPacket);</span><br><span class="line">  currentPacket = null;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void waitAndQueuePacket(DFSPacket packet) throws IOException &#123;</span><br><span class="line">  synchronized (dataQueue) &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">	  // 如果队列满了，等待</span><br><span class="line">      // If queue is full, then wait till we have enough space</span><br><span class="line">      boolean firstWait = true;</span><br><span class="line">      try &#123;</span><br><span class="line">        while (!streamerClosed &amp;&amp; dataQueue.size() + ackQueue.size() &gt;</span><br><span class="line">            dfsClient.getConf().getWriteMaxPackets()) &#123;</span><br><span class="line">          if (firstWait) &#123;</span><br><span class="line">            Span span = Tracer.getCurrentSpan();</span><br><span class="line">            if (span != null) &#123;</span><br><span class="line">              span.addTimelineAnnotation(&quot;dataQueue.wait&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">            firstWait = false;</span><br><span class="line">          &#125;</span><br><span class="line">          try &#123;</span><br><span class="line">            dataQueue.wait();</span><br><span class="line">          &#125; catch (InterruptedException e) &#123;</span><br><span class="line">            ... ...</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; finally &#123;</span><br><span class="line">        Span span = Tracer.getCurrentSpan();</span><br><span class="line">        if ((span != null) &amp;&amp; (!firstWait)) &#123;</span><br><span class="line">          span.addTimelineAnnotation(&quot;end.wait&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      checkClosed();</span><br><span class="line">	  // 如果队列没满，向队列中添加数据</span><br><span class="line">      queuePacket(packet);</span><br><span class="line">    &#125; catch (ClosedChannelException ignored) &#123;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>DataStreamer.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">void queuePacket(DFSPacket packet) &#123;</span><br><span class="line">  synchronized (dataQueue) &#123;</span><br><span class="line">    if (packet == null) return;</span><br><span class="line">    packet.addTraceParent(Tracer.getCurrentSpanId());</span><br><span class="line"></span><br><span class="line">	// 向队列中添加数据</span><br><span class="line">    dataQueue.addLast(packet);</span><br><span class="line"></span><br><span class="line">    lastQueuedSeqno = packet.getSeqno();</span><br><span class="line">    LOG.debug(&quot;Queued &#123;&#125;, &#123;&#125;&quot;, packet, this);</span><br><span class="line"></span><br><span class="line">	// 通知队列添加数据完成</span><br><span class="line">    dataQueue.notifyAll();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3 id="3-1-2-建立管道之机架感知（块存储位置）"><a href="#3-1-2-建立管道之机架感知（块存储位置）" class="headerlink" title="3.1.2 建立管道之机架感知（块存储位置）"></a>3.1.2 建立管道之机架感知（块存储位置）</h3><p>Ctrl + n全局查找DataStreamer，搜索run方法</p>
<p>DataStreamer.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void run() &#123;</span><br><span class="line"></span><br><span class="line">	long lastPacket = Time.monotonicNow();</span><br><span class="line">	TraceScope scope = null;</span><br><span class="line">	while (!streamerClosed &amp;&amp; dfsClient.clientRunning) &#123;</span><br><span class="line">	  // if the Responder encountered an error, shutdown Responder</span><br><span class="line">	  if (errorState.hasError()) &#123;</span><br><span class="line">		closeResponder();</span><br><span class="line">	  &#125;</span><br><span class="line"></span><br><span class="line">	  DFSPacket one;</span><br><span class="line">	  try &#123;</span><br><span class="line">		// process datanode IO errors if any</span><br><span class="line">		boolean doSleep = processDatanodeOrExternalError();</span><br><span class="line"></span><br><span class="line">		final int halfSocketTimeout = dfsClient.getConf().getSocketTimeout()/2;</span><br><span class="line">		synchronized (dataQueue) &#123;</span><br><span class="line">		  // wait for a packet to be sent.</span><br><span class="line">		  long now = Time.monotonicNow();</span><br><span class="line">		  while ((!shouldStop() &amp;&amp; dataQueue.size() == 0 &amp;&amp;</span><br><span class="line">			  (stage != BlockConstructionStage.DATA_STREAMING ||</span><br><span class="line">				  now - lastPacket &lt; halfSocketTimeout)) || doSleep) &#123;</span><br><span class="line">			long timeout = halfSocketTimeout - (now-lastPacket);</span><br><span class="line">			timeout = timeout &lt;= 0 ? 1000 : timeout;</span><br><span class="line">			timeout = (stage == BlockConstructionStage.DATA_STREAMING)?</span><br><span class="line">				timeout : 1000;</span><br><span class="line">			try &#123;</span><br><span class="line">			  // 如果dataQueue里面没有数据，代码会阻塞在这儿</span><br><span class="line">			  dataQueue.wait(timeout); // 接收到notify消息</span><br><span class="line">			&#125; catch (InterruptedException  e) &#123;</span><br><span class="line">			  LOG.warn(&quot;Caught exception&quot;, e);</span><br><span class="line">			&#125;</span><br><span class="line">			doSleep = false;</span><br><span class="line">			now = Time.monotonicNow();</span><br><span class="line">		  &#125;</span><br><span class="line">		  if (shouldStop()) &#123;</span><br><span class="line">			continue;</span><br><span class="line">		  &#125;</span><br><span class="line">		  // get packet to be sent.</span><br><span class="line">		  if (dataQueue.isEmpty()) &#123;</span><br><span class="line">			one = createHeartbeatPacket();</span><br><span class="line">		  &#125; else &#123;</span><br><span class="line">			try &#123;</span><br><span class="line">			  backOffIfNecessary();</span><br><span class="line">			&#125; catch (InterruptedException e) &#123;</span><br><span class="line">			  LOG.warn(&quot;Caught exception&quot;, e);</span><br><span class="line">			&#125;</span><br><span class="line">			//  队列不为空，从队列中取出packet</span><br><span class="line">			one = dataQueue.getFirst(); // regular data packet</span><br><span class="line">			SpanId[] parents = one.getTraceParents();</span><br><span class="line">			if (parents.length &gt; 0) &#123;</span><br><span class="line">			  scope = dfsClient.getTracer().</span><br><span class="line">				  newScope(&quot;dataStreamer&quot;, parents[0]);</span><br><span class="line">			  scope.getSpan().setParents(parents);</span><br><span class="line">			&#125;</span><br><span class="line">		  &#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		// get new block from namenode.</span><br><span class="line">		if (LOG.isDebugEnabled()) &#123;</span><br><span class="line">		  LOG.debug(&quot;stage=&quot; + stage + &quot;, &quot; + this);</span><br><span class="line">		&#125;</span><br><span class="line">		if (stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) &#123;</span><br><span class="line">		  LOG.debug(&quot;Allocating new block: &#123;&#125;&quot;, this);</span><br><span class="line">		  // 步骤一：向NameNode 申请block 并建立数据管道</span><br><span class="line">		  setPipeline(nextBlockOutputStream());</span><br><span class="line">		  // 步骤二：启动ResponseProcessor用来监听packet发送是否成功</span><br><span class="line">		  initDataStreaming();</span><br><span class="line">		&#125; else if (stage == BlockConstructionStage.PIPELINE_SETUP_APPEND) &#123;</span><br><span class="line">		  setupPipelineForAppendOrRecovery();</span><br><span class="line">		  if (streamerClosed) &#123;</span><br><span class="line">			continue;</span><br><span class="line">		  &#125;</span><br><span class="line">		  initDataStreaming();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		long lastByteOffsetInBlock = one.getLastByteOffsetBlock();</span><br><span class="line">		if (lastByteOffsetInBlock &gt; stat.getBlockSize()) &#123;</span><br><span class="line">		  throw new IOException(&quot;BlockSize &quot; + stat.getBlockSize() +</span><br><span class="line">			  &quot; &lt; lastByteOffsetInBlock, &quot; + this + &quot;, &quot; + one);</span><br><span class="line">		&#125;</span><br><span class="line">		… …</span><br><span class="line">		// send the packet</span><br><span class="line">		SpanId spanId = SpanId.INVALID;</span><br><span class="line">		synchronized (dataQueue) &#123;</span><br><span class="line"></span><br><span class="line">		  // move packet from dataQueue to ackQueue</span><br><span class="line">		  if (!one.isHeartbeatPacket()) &#123;</span><br><span class="line">			if (scope != null) &#123;</span><br><span class="line">			  spanId = scope.getSpanId();</span><br><span class="line">			  scope.detach();</span><br><span class="line"></span><br><span class="line">			  one.setTraceScope(scope);</span><br><span class="line">			&#125;</span><br><span class="line">			scope = null;</span><br><span class="line">			// 步骤三：从dataQueue 把要发送的这个packet 移除出去</span><br><span class="line">			dataQueue.removeFirst();</span><br><span class="line">			// 步骤四：然后往ackQueue 里面添加这个packet</span><br><span class="line">			ackQueue.addLast(one);</span><br><span class="line">			packetSendTime.put(one.getSeqno(), Time.monotonicNow());</span><br><span class="line">			dataQueue.notifyAll();</span><br><span class="line">		  &#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		LOG.debug(&quot;&#123;&#125; sending &#123;&#125;&quot;, this, one);</span><br><span class="line"></span><br><span class="line">		// write out data to remote datanode</span><br><span class="line">		try (TraceScope ignored = dfsClient.getTracer().</span><br><span class="line">			newScope(&quot;DataStreamer#writeTo&quot;, spanId)) &#123;</span><br><span class="line">		  //  将数据写出去</span><br><span class="line">		  one.writeTo(blockStream);</span><br><span class="line">		  blockStream.flush();</span><br><span class="line">		&#125; catch (IOException e) &#123;</span><br><span class="line">		  errorState.markFirstNodeIfNotMarked();</span><br><span class="line">		  throw e;</span><br><span class="line">		&#125;</span><br><span class="line">		… …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>    点击nextBlockOutputStream</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">protected LocatedBlock nextBlockOutputStream() throws IOException &#123;</span><br><span class="line">  LocatedBlock lb;</span><br><span class="line">  DatanodeInfo[] nodes;</span><br><span class="line">  StorageType[] nextStorageTypes;</span><br><span class="line">  String[] nextStorageIDs;</span><br><span class="line">  int count = dfsClient.getConf().getNumBlockWriteRetry();</span><br><span class="line">  boolean success;</span><br><span class="line">  final ExtendedBlock oldBlock = block.getCurrentBlock();</span><br><span class="line">  do &#123;</span><br><span class="line">    errorState.resetInternalError();</span><br><span class="line">    lastException.clear();</span><br><span class="line"></span><br><span class="line">    DatanodeInfo[] excluded = getExcludedNodes();</span><br><span class="line">	// 向NN获取向哪个DN写数据</span><br><span class="line">    lb = locateFollowingBlock(</span><br><span class="line">        excluded.length &gt; 0 ? excluded : null, oldBlock);</span><br><span class="line"></span><br><span class="line">    // 创建管道</span><br><span class="line">    success = createBlockOutputStream(nodes, nextStorageTypes, nextStorageIDs,</span><br><span class="line">          0L, false);</span><br><span class="line">    … …</span><br><span class="line">  &#125; while (!success &amp;&amp; --count &gt;= 0);</span><br><span class="line"></span><br><span class="line">  if (!success) &#123;</span><br><span class="line">    throw new IOException(&quot;Unable to create new block.&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">  return lb;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private LocatedBlock locateFollowingBlock(DatanodeInfo[] excluded,</span><br><span class="line">    ExtendedBlock oldBlock) throws IOException &#123;</span><br><span class="line">  return DFSOutputStream.addBlock(excluded, dfsClient, src, oldBlock,</span><br><span class="line">      stat.getFileId(), favoredNodes, addBlockFlags);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static LocatedBlock addBlock(DatanodeInfo[] excludedNodes,</span><br><span class="line">      DFSClient dfsClient, String src, ExtendedBlock prevBlock, long fileId,</span><br><span class="line">      String[] favoredNodes, EnumSet&lt;AddBlockFlag&gt; allocFlags)</span><br><span class="line">      throws IOException &#123;</span><br><span class="line">	  ... ...</span><br><span class="line">	  // 向NN获取向哪个DN写数据</span><br><span class="line">	  return dfsClient.namenode.addBlock(src, dfsClient.clientName, prevBlock,</span><br><span class="line">            excludedNodes, fileId, favoredNodes, allocFlags);</span><br><span class="line">	  ... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">LocatedBlock addBlock(String src, String clientName,</span><br><span class="line">      ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId,</span><br><span class="line">      String[] favoredNodes, EnumSet&lt;AddBlockFlag&gt; addBlockFlags)</span><br><span class="line">      throws IOException;</span><br></pre></td></tr></table></figure>

<p>ctrl + h 点击NameNodeRpcServer，在该类中搜索addBlock</p>
<p>    NameNodeRpcServer.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public LocatedBlock addBlock(String src, String clientName,</span><br><span class="line">    ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,</span><br><span class="line">    String[] favoredNodes, EnumSet&lt;AddBlockFlag&gt; addBlockFlags)</span><br><span class="line">    throws IOException &#123;</span><br><span class="line">  checkNNStartup();</span><br><span class="line">  LocatedBlock locatedBlock = namesystem.getAdditionalBlock(src, fileId,</span><br><span class="line">      clientName, previous, excludedNodes, favoredNodes, addBlockFlags);</span><br><span class="line">  if (locatedBlock != null) &#123;</span><br><span class="line">    metrics.incrAddBlockOps();</span><br><span class="line">  &#125;</span><br><span class="line">  return locatedBlock;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>    FSNamesystrm.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">LocatedBlock getAdditionalBlock(</span><br><span class="line">    String src, long fileId, String clientName, ExtendedBlock previous,</span><br><span class="line">    DatanodeInfo[] excludedNodes, String[] favoredNodes,</span><br><span class="line">    EnumSet&lt;AddBlockFlag&gt; flags) throws IOException &#123;</span><br><span class="line">  final String operationName = &quot;getAdditionalBlock&quot;;</span><br><span class="line">  NameNode.stateChangeLog.debug(&quot;BLOCK* getAdditionalBlock: &#123;&#125;  inodeId &#123;&#125;&quot; +</span><br><span class="line">      &quot; for &#123;&#125;&quot;, src, fileId, clientName);</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line">  // 选择块存储位置</span><br><span class="line">  DatanodeStorageInfo[] targets = FSDirWriteFileOp.chooseTargetForNewBlock(</span><br><span class="line">      blockManager, src, excludedNodes, favoredNodes, flags, r);</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line">  return lb;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static DatanodeStorageInfo[] chooseTargetForNewBlock(</span><br><span class="line">    BlockManager bm, String src, DatanodeInfo[] excludedNodes,</span><br><span class="line">    String[] favoredNodes, EnumSet&lt;AddBlockFlag&gt; flags,</span><br><span class="line">    ValidateAddBlockResult r) throws IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  return bm.chooseTarget4NewBlock(src, r.numTargets, clientNode,</span><br><span class="line">                                  excludedNodesSet, r.blockSize,</span><br><span class="line">                                  favoredNodesList, r.storagePolicyID,</span><br><span class="line">                                  r.blockType, r.ecPolicy, flags);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public DatanodeStorageInfo[] chooseTarget4NewBlock(... ...</span><br><span class="line">  ) throws IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line">	  </span><br><span class="line">  final DatanodeStorageInfo[] targets = blockplacement.chooseTarget(src,</span><br><span class="line">      numOfReplicas, client, excludedNodes, blocksize, </span><br><span class="line">      favoredDatanodeDescriptors, storagePolicy, flags);</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line">  return targets;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">DatanodeStorageInfo[] chooseTarget(String src,</span><br><span class="line">    int numOfReplicas, Node writer,</span><br><span class="line">    Set&lt;Node&gt; excludedNodes,</span><br><span class="line">    long blocksize,</span><br><span class="line">    List&lt;DatanodeDescriptor&gt; favoredNodes,</span><br><span class="line">    BlockStoragePolicy storagePolicy,</span><br><span class="line">    EnumSet&lt;AddBlockFlag&gt; flags) &#123;</span><br><span class="line">  </span><br><span class="line">  return chooseTarget(src, numOfReplicas, writer, </span><br><span class="line">      new ArrayList&lt;DatanodeStorageInfo&gt;(numOfReplicas), false,</span><br><span class="line">      excludedNodes, blocksize, storagePolicy, flags);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public abstract DatanodeStorageInfo[] chooseTarget(String srcPath,</span><br><span class="line">    int numOfReplicas,</span><br><span class="line">    Node writer,</span><br><span class="line">    List&lt;DatanodeStorageInfo&gt; chosen,</span><br><span class="line">    boolean returnChosenNodes,</span><br><span class="line">    Set&lt;Node&gt; excludedNodes,</span><br><span class="line">    long blocksize,</span><br><span class="line">    BlockStoragePolicy storagePolicy,</span><br><span class="line">EnumSet&lt;AddBlockFlag&gt; flags);</span><br></pre></td></tr></table></figure>

<pre><code>Crtl + h 查找chooseTarget实现BlockPlacementPolicyDefault.java
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br></pre></td><td class="code"><pre><span class="line">LocatedBlock getAdditionalBlock(</span><br><span class="line">    String src, long fileId, String clientName, ExtendedBlock previous,</span><br><span class="line">    DatanodeInfo[] excludedNodes, String[] favoredNodes,</span><br><span class="line">    EnumSet&lt;AddBlockFlag&gt; flags) throws IOException &#123;</span><br><span class="line">  final String operationName = &quot;getAdditionalBlock&quot;;</span><br><span class="line">  NameNode.stateChangeLog.debug(&quot;BLOCK* getAdditionalBlock: &#123;&#125;  inodeId &#123;&#125;&quot; +</span><br><span class="line">      &quot; for &#123;&#125;&quot;, src, fileId, clientName);</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line">  // 选择块存储位置</span><br><span class="line">  DatanodeStorageInfo[] targets = FSDirWriteFileOp.chooseTargetForNewBlock(</span><br><span class="line">      blockManager, src, excludedNodes, favoredNodes, flags, r);</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line">  return lb;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static DatanodeStorageInfo[] chooseTargetForNewBlock(</span><br><span class="line">    BlockManager bm, String src, DatanodeInfo[] excludedNodes,</span><br><span class="line">    String[] favoredNodes, EnumSet&lt;AddBlockFlag&gt; flags,</span><br><span class="line">    ValidateAddBlockResult r) throws IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  return bm.chooseTarget4NewBlock(src, r.numTargets, clientNode,</span><br><span class="line">                                  excludedNodesSet, r.blockSize,</span><br><span class="line">                                  favoredNodesList, r.storagePolicyID,</span><br><span class="line">                                  r.blockType, r.ecPolicy, flags);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public DatanodeStorageInfo[] chooseTarget4NewBlock(... ...</span><br><span class="line">  ) throws IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line">	  </span><br><span class="line">  final DatanodeStorageInfo[] targets = blockplacement.chooseTarget(src,</span><br><span class="line">      numOfReplicas, client, excludedNodes, blocksize, </span><br><span class="line">      favoredDatanodeDescriptors, storagePolicy, flags);</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line">  return targets;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">DatanodeStorageInfo[] chooseTarget(String src,</span><br><span class="line">    int numOfReplicas, Node writer,</span><br><span class="line">    Set&lt;Node&gt; excludedNodes,</span><br><span class="line">    long blocksize,</span><br><span class="line">    List&lt;DatanodeDescriptor&gt; favoredNodes,</span><br><span class="line">    BlockStoragePolicy storagePolicy,</span><br><span class="line">    EnumSet&lt;AddBlockFlag&gt; flags) &#123;</span><br><span class="line">  </span><br><span class="line">  return chooseTarget(src, numOfReplicas, writer, </span><br><span class="line">      new ArrayList&lt;DatanodeStorageInfo&gt;(numOfReplicas), false,</span><br><span class="line">      excludedNodes, blocksize, storagePolicy, flags);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public abstract DatanodeStorageInfo[] chooseTarget(String srcPath,</span><br><span class="line">    int numOfReplicas,</span><br><span class="line">    Node writer,</span><br><span class="line">    List&lt;DatanodeStorageInfo&gt; chosen,</span><br><span class="line">    boolean returnChosenNodes,</span><br><span class="line">    Set&lt;Node&gt; excludedNodes,</span><br><span class="line">    long blocksize,</span><br><span class="line">    BlockStoragePolicy storagePolicy,</span><br><span class="line">EnumSet&lt;AddBlockFlag&gt; flags);</span><br><span class="line">	Crtl + h 查找chooseTarget实现类BlockPlacementPolicyDefault.java</span><br><span class="line">public DatanodeStorageInfo[] chooseTarget(String srcPath,</span><br><span class="line">    int numOfReplicas,</span><br><span class="line">    Node writer,</span><br><span class="line">    List&lt;DatanodeStorageInfo&gt; chosenNodes,</span><br><span class="line">    boolean returnChosenNodes,</span><br><span class="line">    Set&lt;Node&gt; excludedNodes,</span><br><span class="line">    long blocksize,</span><br><span class="line">    final BlockStoragePolicy storagePolicy,</span><br><span class="line">    EnumSet&lt;AddBlockFlag&gt; flags) &#123;</span><br><span class="line">	</span><br><span class="line">  return chooseTarget(numOfReplicas, writer, chosenNodes, returnChosenNodes,</span><br><span class="line">      excludedNodes, blocksize, storagePolicy, flags, null);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private DatanodeStorageInfo[] chooseTarget(int numOfReplicas,</span><br><span class="line">  Node writer,</span><br><span class="line">  List&lt;DatanodeStorageInfo&gt; chosenStorage,</span><br><span class="line">  boolean returnChosenNodes,</span><br><span class="line">  Set&lt;Node&gt; excludedNodes,</span><br><span class="line">  long blocksize,</span><br><span class="line">  final BlockStoragePolicy storagePolicy,</span><br><span class="line">  EnumSet&lt;AddBlockFlag&gt; addBlockFlags,</span><br><span class="line">  EnumMap&lt;StorageType, Integer&gt; sTypes) &#123;</span><br><span class="line">  … …</span><br><span class="line">   </span><br><span class="line">  int[] result = getMaxNodesPerRack(chosenStorage.size(), numOfReplicas);</span><br><span class="line">  numOfReplicas = result[0];</span><br><span class="line">  int maxNodesPerRack = result[1];</span><br><span class="line">    </span><br><span class="line">  for (DatanodeStorageInfo storage : chosenStorage) &#123;</span><br><span class="line">    // add localMachine and related nodes to excludedNodes</span><br><span class="line">	// 获取不可用的DN</span><br><span class="line">    addToExcludedNodes(storage.getDatanodeDescriptor(), excludedNodes);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  List&lt;DatanodeStorageInfo&gt; results = null;</span><br><span class="line">  Node localNode = null;</span><br><span class="line">  boolean avoidStaleNodes = (stats != null</span><br><span class="line">      &amp;&amp; stats.isAvoidingStaleDataNodesForWrite());</span><br><span class="line">  //   </span><br><span class="line">  boolean avoidLocalNode = (addBlockFlags != null</span><br><span class="line">      &amp;&amp; addBlockFlags.contains(AddBlockFlag.NO_LOCAL_WRITE)</span><br><span class="line">      &amp;&amp; writer != null</span><br><span class="line">      &amp;&amp; !excludedNodes.contains(writer));</span><br><span class="line">  // Attempt to exclude local node if the client suggests so. If no enough</span><br><span class="line">  // nodes can be obtained, it falls back to the default block placement</span><br><span class="line">  // policy.</span><br><span class="line"></span><br><span class="line">  // 有数据正在写，避免都写入本地</span><br><span class="line">  if (avoidLocalNode) &#123;</span><br><span class="line">    results = new ArrayList&lt;&gt;(chosenStorage);</span><br><span class="line">    Set&lt;Node&gt; excludedNodeCopy = new HashSet&lt;&gt;(excludedNodes);</span><br><span class="line">    if (writer != null) &#123;</span><br><span class="line">      excludedNodeCopy.add(writer);</span><br><span class="line">    &#125;</span><br><span class="line">    localNode = chooseTarget(numOfReplicas, writer,</span><br><span class="line">        excludedNodeCopy, blocksize, maxNodesPerRack, results,</span><br><span class="line">        avoidStaleNodes, storagePolicy,</span><br><span class="line">        EnumSet.noneOf(StorageType.class), results.isEmpty(), sTypes);</span><br><span class="line">    if (results.size() &lt; numOfReplicas) &#123;</span><br><span class="line">      // not enough nodes; discard results and fall back</span><br><span class="line">      results = null;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  if (results == null) &#123;</span><br><span class="line">    results = new ArrayList&lt;&gt;(chosenStorage);</span><br><span class="line">	// 真正的选择DN节点</span><br><span class="line">    localNode = chooseTarget(numOfReplicas, writer, excludedNodes,</span><br><span class="line">        blocksize, maxNodesPerRack, results, avoidStaleNodes,</span><br><span class="line">        storagePolicy, EnumSet.noneOf(StorageType.class), results.isEmpty(),</span><br><span class="line">        sTypes);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  if (!returnChosenNodes) &#123;  </span><br><span class="line">    results.removeAll(chosenStorage);</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  // sorting nodes to form a pipeline</span><br><span class="line">  return getPipeline(</span><br><span class="line">      (writer != null &amp;&amp; writer instanceof DatanodeDescriptor) ? writer</span><br><span class="line">          : localNode,</span><br><span class="line">      results.toArray(new DatanodeStorageInfo[results.size()]));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private Node chooseTarget(int numOfReplicas,</span><br><span class="line">   ... ...) &#123;</span><br><span class="line">	</span><br><span class="line">   writer = chooseTargetInOrder(numOfReplicas, writer, excludedNodes, blocksize,</span><br><span class="line">          maxNodesPerRack, results, avoidStaleNodes, newBlock, storageTypes);</span><br><span class="line">   ... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">protected Node chooseTargetInOrder(int numOfReplicas, </span><br><span class="line">                               Node writer,</span><br><span class="line">                               final Set&lt;Node&gt; excludedNodes,</span><br><span class="line">                               final long blocksize,</span><br><span class="line">                               final int maxNodesPerRack,</span><br><span class="line">                               final List&lt;DatanodeStorageInfo&gt; results,</span><br><span class="line">                               final boolean avoidStaleNodes,</span><br><span class="line">                               final boolean newBlock,</span><br><span class="line">                               EnumMap&lt;StorageType, Integer&gt; storageTypes)</span><br><span class="line">                               throws NotEnoughReplicasException &#123;</span><br><span class="line">  final int numOfResults = results.size();</span><br><span class="line">  if (numOfResults == 0) &#123;</span><br><span class="line">	// 第一个块存储在当前节点</span><br><span class="line">    DatanodeStorageInfo storageInfo = chooseLocalStorage(writer,</span><br><span class="line">        excludedNodes, blocksize, maxNodesPerRack, results, avoidStaleNodes,</span><br><span class="line">        storageTypes, true);</span><br><span class="line"></span><br><span class="line">    writer = (storageInfo != null) ? storageInfo.getDatanodeDescriptor()</span><br><span class="line">                                   : null;</span><br><span class="line"></span><br><span class="line">    if (--numOfReplicas == 0) &#123;</span><br><span class="line">      return writer;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  final DatanodeDescriptor dn0 = results.get(0).getDatanodeDescriptor();</span><br><span class="line">  // 第二个块存储在另外一个机架</span><br><span class="line">  if (numOfResults &lt;= 1) &#123;</span><br><span class="line">    chooseRemoteRack(1, dn0, excludedNodes, blocksize, maxNodesPerRack,</span><br><span class="line">        results, avoidStaleNodes, storageTypes);</span><br><span class="line">    if (--numOfReplicas == 0) &#123;</span><br><span class="line">      return writer;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  if (numOfResults &lt;= 2) &#123;</span><br><span class="line">    final DatanodeDescriptor dn1 = results.get(1).getDatanodeDescriptor();</span><br><span class="line">	// 如果第一个和第二个在同一个机架，那么第三个放在其他机架</span><br><span class="line">    if (clusterMap.isOnSameRack(dn0, dn1)) &#123;</span><br><span class="line">      chooseRemoteRack(1, dn0, excludedNodes, blocksize, maxNodesPerRack,</span><br><span class="line">          results, avoidStaleNodes, storageTypes);</span><br><span class="line">    &#125; else if (newBlock)&#123;</span><br><span class="line">	  // 如果是新块，和第二个块存储在同一个机架</span><br><span class="line">      chooseLocalRack(dn1, excludedNodes, blocksize, maxNodesPerRack,</span><br><span class="line">          results, avoidStaleNodes, storageTypes);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">	  // 如果不是新块，放在当前机架</span><br><span class="line">      chooseLocalRack(writer, excludedNodes, blocksize, maxNodesPerRack,</span><br><span class="line">          results, avoidStaleNodes, storageTypes);</span><br><span class="line">    &#125;</span><br><span class="line">    if (--numOfReplicas == 0) &#123;</span><br><span class="line">      return writer;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  chooseRandom(numOfReplicas, NodeBase.ROOT, excludedNodes, blocksize,</span><br><span class="line">      maxNodesPerRack, results, avoidStaleNodes, storageTypes);</span><br><span class="line">  return writer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3 id="3-1-3-建立管道之Socket发送"><a href="#3-1-3-建立管道之Socket发送" class="headerlink" title="3.1.3 建立管道之Socket发送"></a>3.1.3 建立管道之Socket发送</h3><p>    点击nextBlockOutputStream</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">protected LocatedBlock nextBlockOutputStream() throws IOException &#123;</span><br><span class="line">  LocatedBlock lb;</span><br><span class="line">  DatanodeInfo[] nodes;</span><br><span class="line">  StorageType[] nextStorageTypes;</span><br><span class="line">  String[] nextStorageIDs;</span><br><span class="line">  int count = dfsClient.getConf().getNumBlockWriteRetry();</span><br><span class="line">  boolean success;</span><br><span class="line">  final ExtendedBlock oldBlock = block.getCurrentBlock();</span><br><span class="line">  do &#123;</span><br><span class="line">    errorState.resetInternalError();</span><br><span class="line">    lastException.clear();</span><br><span class="line"></span><br><span class="line">    DatanodeInfo[] excluded = getExcludedNodes();</span><br><span class="line">	// 向NN获取向哪个DN写数据</span><br><span class="line">    lb = locateFollowingBlock(</span><br><span class="line">        excluded.length &gt; 0 ? excluded : null, oldBlock);</span><br><span class="line"></span><br><span class="line">    // 创建管道</span><br><span class="line">    success = createBlockOutputStream(nodes, nextStorageTypes, nextStorageIDs,</span><br><span class="line">          0L, false);</span><br><span class="line">    … …</span><br><span class="line">  &#125; while (!success &amp;&amp; --count &gt;= 0);</span><br><span class="line"></span><br><span class="line">  if (!success) &#123;</span><br><span class="line">    throw new IOException(&quot;Unable to create new block.&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">  return lb;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">boolean createBlockOutputStream(DatanodeInfo[] nodes,</span><br><span class="line">      StorageType[] nodeStorageTypes, String[] nodeStorageIDs,</span><br><span class="line">      long newGS, boolean recoveryFlag) &#123;</span><br><span class="line">    ... ...</span><br><span class="line">	// 和DN创建socket</span><br><span class="line">	s = createSocketForPipeline(nodes[0], nodes.length, dfsClient);</span><br><span class="line">	</span><br><span class="line">	// 获取输出流，用于写数据到DN</span><br><span class="line">	OutputStream unbufOut = NetUtils.getOutputStream(s, writeTimeout);</span><br><span class="line">	// 获取输入流，用于读取写数据到DN的结果</span><br><span class="line">    InputStream unbufIn = NetUtils.getInputStream(s, readTimeout);</span><br><span class="line">	</span><br><span class="line">    IOStreamPair saslStreams = dfsClient.saslClient.socketSend(s,</span><br><span class="line">        unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);</span><br><span class="line">    unbufOut = saslStreams.out;</span><br><span class="line">    unbufIn = saslStreams.in;</span><br><span class="line">    out = new DataOutputStream(new BufferedOutputStream(unbufOut,</span><br><span class="line">        DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));</span><br><span class="line">    blockReplyStream = new DataInputStream(unbufIn);</span><br><span class="line">	</span><br><span class="line">	// 发送数据</span><br><span class="line">	new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,</span><br><span class="line">            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,</span><br><span class="line">            nodes.length, block.getNumBytes(), bytesSent, newGS,</span><br><span class="line">            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,</span><br><span class="line">            (targetPinnings != null &amp;&amp; targetPinnings[0]), targetPinnings,</span><br><span class="line">            nodeStorageIDs[0], nodeStorageIDs);</span><br><span class="line">	... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void writeBlock(... ...) throws IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  send(out, Op.WRITE_BLOCK, proto.build());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3 id="3-1-4-建立管道之Socket接收"><a href="#3-1-4-建立管道之Socket接收" class="headerlink" title="3.1.4 建立管道之Socket接收"></a>3.1.4 建立管道之Socket接收</h3><p>Ctrl +n 全局查找DataXceiverServer.java，在该类中查找run方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">public void run() &#123;</span><br><span class="line">  Peer peer = null;</span><br><span class="line">  while (datanode.shouldRun &amp;&amp; !datanode.shutdownForUpgrade) &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">	  // 接收socket的请求</span><br><span class="line">      peer = peerServer.accept();</span><br><span class="line"></span><br><span class="line">      // Make sure the xceiver count is not exceeded</span><br><span class="line">      int curXceiverCount = datanode.getXceiverCount();</span><br><span class="line">      if (curXceiverCount &gt; maxXceiverCount) &#123;</span><br><span class="line">        throw new IOException(&quot;Xceiver count &quot; + curXceiverCount</span><br><span class="line">            + &quot; exceeds the limit of concurrent xcievers: &quot;</span><br><span class="line">            + maxXceiverCount);</span><br><span class="line">      &#125;</span><br><span class="line">	  // 客户端每发送一个block，都启动一个DataXceiver去处理block</span><br><span class="line">      new Daemon(datanode.threadGroup,</span><br><span class="line">          DataXceiver.create(peer, datanode, this))</span><br><span class="line">          .start();</span><br><span class="line">    &#125; catch (SocketTimeoutException ignored) &#123;</span><br><span class="line">      ... ...</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击DataXceiver（线程），查找run方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line">public void run() &#123;</span><br><span class="line">  int opsProcessed = 0;</span><br><span class="line">  Op op = null;</span><br><span class="line"></span><br><span class="line">  try &#123;</span><br><span class="line">    synchronized(this) &#123;</span><br><span class="line">      xceiver = Thread.currentThread();</span><br><span class="line">    &#125;</span><br><span class="line">    dataXceiverServer.addPeer(peer, Thread.currentThread(), this);</span><br><span class="line">    peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);</span><br><span class="line">    InputStream input = socketIn;</span><br><span class="line">    try &#123;</span><br><span class="line">      IOStreamPair saslStreams = datanode.saslServer.receive(peer, socketOut,</span><br><span class="line">        socketIn, datanode.getXferAddress().getPort(),</span><br><span class="line"></span><br><span class="line">      return;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    super.initialize(new DataInputStream(input));</span><br><span class="line">    </span><br><span class="line">    do &#123;</span><br><span class="line">      updateCurrentThreadName(&quot;Waiting for operation #&quot; + (opsProcessed + 1));</span><br><span class="line"></span><br><span class="line">      try &#123;</span><br><span class="line">        if (opsProcessed != 0) &#123;</span><br><span class="line">          assert dnConf.socketKeepaliveTimeout &gt; 0;</span><br><span class="line">          peer.setReadTimeout(dnConf.socketKeepaliveTimeout);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          peer.setReadTimeout(dnConf.socketTimeout);</span><br><span class="line">        &#125;</span><br><span class="line">		// 读取这次数据的请求类型</span><br><span class="line">        op = readOp();</span><br><span class="line">      &#125; catch (InterruptedIOException ignored) &#123;</span><br><span class="line">        // Time out while we wait for client rpc</span><br><span class="line">        break;</span><br><span class="line">      &#125; catch (EOFException | ClosedChannelException e) &#123;</span><br><span class="line">        // Since we optimistically expect the next op, it&#x27;s quite normal to</span><br><span class="line">        // get EOF here.</span><br><span class="line">        LOG.debug(&quot;Cached &#123;&#125; closing after &#123;&#125; ops.  &quot; +</span><br><span class="line">            &quot;This message is usually benign.&quot;, peer, opsProcessed);</span><br><span class="line">        break;</span><br><span class="line">      &#125; catch (IOException err) &#123;</span><br><span class="line">        incrDatanodeNetworkErrors();</span><br><span class="line">        throw err;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      // restore normal timeout</span><br><span class="line">      if (opsProcessed != 0) &#123;</span><br><span class="line">        peer.setReadTimeout(dnConf.socketTimeout);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      opStartTime = monotonicNow();</span><br><span class="line">	  // 根据操作类型处理我们的数据</span><br><span class="line">      processOp(op);</span><br><span class="line">      ++opsProcessed;</span><br><span class="line">    &#125; while ((peer != null) &amp;&amp;</span><br><span class="line">        (!peer.isClosed() &amp;&amp; dnConf.socketKeepaliveTimeout &gt; 0));</span><br><span class="line">  &#125; catch (Throwable t) &#123;</span><br><span class="line">    ... ... </span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">protected final void processOp(Op op) throws IOException &#123;</span><br><span class="line">  switch(op) &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  case WRITE_BLOCK:</span><br><span class="line">    opWriteBlock(in);</span><br><span class="line">    break;</span><br><span class="line">  ... ...</span><br><span class="line">  default:</span><br><span class="line">    throw new IOException(&quot;Unknown op &quot; + op + &quot; in data stream&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private void opWriteBlock(DataInputStream in) throws IOException &#123;</span><br><span class="line">  final OpWriteBlockProto proto = OpWriteBlockProto.parseFrom(vintPrefixed(in));</span><br><span class="line">  final DatanodeInfo[] targets = PBHelperClient.convert(proto.getTargetsList());</span><br><span class="line">  TraceScope traceScope = continueTraceSpan(proto.getHeader(),</span><br><span class="line">      proto.getClass().getSimpleName());</span><br><span class="line">  try &#123;</span><br><span class="line">    writeBlock(PBHelperClient.convert(proto.getHeader().getBaseHeader().getBlock()),</span><br><span class="line">        PBHelperClient.convertStorageType(proto.getStorageType()),</span><br><span class="line">        PBHelperClient.convert(proto.getHeader().getBaseHeader().getToken()),</span><br><span class="line">        proto.getHeader().getClientName(),</span><br><span class="line">        targets,</span><br><span class="line">        PBHelperClient.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length),</span><br><span class="line">        PBHelperClient.convert(proto.getSource()),</span><br><span class="line">        fromProto(proto.getStage()),</span><br><span class="line">        proto.getPipelineSize(),</span><br><span class="line">        proto.getMinBytesRcvd(), proto.getMaxBytesRcvd(),</span><br><span class="line">        proto.getLatestGenerationStamp(),</span><br><span class="line">        fromProto(proto.getRequestedChecksum()),</span><br><span class="line">        (proto.hasCachingStrategy() ?</span><br><span class="line">            getCachingStrategy(proto.getCachingStrategy()) :</span><br><span class="line">          CachingStrategy.newDefaultStrategy()),</span><br><span class="line">        (proto.hasAllowLazyPersist() ? proto.getAllowLazyPersist() : false),</span><br><span class="line">        (proto.hasPinning() ? proto.getPinning(): false),</span><br><span class="line">        (PBHelperClient.convertBooleanList(proto.getTargetPinningsList())),</span><br><span class="line">        proto.getStorageId(),</span><br><span class="line">        proto.getTargetStorageIdsList().toArray(new String[0]));</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">   if (traceScope != null) traceScope.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Ctrl +alt +b 查找writeBlock的实现类DataXceiver.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br></pre></td><td class="code"><pre><span class="line">public void writeBlock(... ...) throws IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  try &#123;</span><br><span class="line">    final Replica replica;</span><br><span class="line">    if (isDatanode || </span><br><span class="line">        stage != BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) &#123;</span><br><span class="line">      // open a block receiver</span><br><span class="line">	  // 创建一个BlockReceiver</span><br><span class="line">      setCurrentBlockReceiver(getBlockReceiver(block, storageType, in,</span><br><span class="line">          peer.getRemoteAddressString(),</span><br><span class="line">          peer.getLocalAddressString(),</span><br><span class="line">          stage, latestGenerationStamp, minBytesRcvd, maxBytesRcvd,</span><br><span class="line">          clientname, srcDataNode, datanode, requestedChecksum,</span><br><span class="line">          cachingStrategy, allowLazyPersist, pinning, storageId));</span><br><span class="line">      replica = blockReceiver.getReplica();</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      replica = datanode.data.recoverClose(</span><br><span class="line">          block, latestGenerationStamp, minBytesRcvd);</span><br><span class="line">    &#125;</span><br><span class="line">    storageUuid = replica.getStorageUuid();</span><br><span class="line">    isOnTransientStorage = replica.isOnTransientStorage();</span><br><span class="line"></span><br><span class="line">    //</span><br><span class="line">    // Connect to downstream machine, if appropriate</span><br><span class="line">    // 继续连接下游的机器</span><br><span class="line">    if (targets.length &gt; 0) &#123;</span><br><span class="line">      InetSocketAddress mirrorTarget = null;</span><br><span class="line">      // Connect to backup machine</span><br><span class="line">      mirrorNode = targets[0].getXferAddr(connectToDnViaHostname);</span><br><span class="line">      LOG.debug(&quot;Connecting to datanode &#123;&#125;&quot;, mirrorNode);</span><br><span class="line">      mirrorTarget = NetUtils.createSocketAddr(mirrorNode);</span><br><span class="line"></span><br><span class="line">	  // 向新的副本发送socket</span><br><span class="line">      mirrorSock = datanode.newSocket();</span><br><span class="line">      try &#123;</span><br><span class="line"></span><br><span class="line">        ... ...</span><br><span class="line">        if (targetPinnings != null &amp;&amp; targetPinnings.length &gt; 0) &#123;</span><br><span class="line">		  // 往下游socket发送数据</span><br><span class="line">          new Sender(mirrorOut).writeBlock(originalBlock, targetStorageTypes[0],</span><br><span class="line">              blockToken, clientname, targets, targetStorageTypes,</span><br><span class="line">              srcDataNode, stage, pipelineSize, minBytesRcvd, maxBytesRcvd,</span><br><span class="line">              latestGenerationStamp, requestedChecksum, cachingStrategy,</span><br><span class="line">              allowLazyPersist, targetPinnings[0], targetPinnings,</span><br><span class="line">              targetStorageId, targetStorageIds);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          new Sender(mirrorOut).writeBlock(originalBlock, targetStorageTypes[0],</span><br><span class="line">              blockToken, clientname, targets, targetStorageTypes,</span><br><span class="line">              srcDataNode, stage, pipelineSize, minBytesRcvd, maxBytesRcvd,</span><br><span class="line">              latestGenerationStamp, requestedChecksum, cachingStrategy,</span><br><span class="line">              allowLazyPersist, false, targetPinnings,</span><br><span class="line">              targetStorageId, targetStorageIds);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        mirrorOut.flush();</span><br><span class="line"></span><br><span class="line">        DataNodeFaultInjector.get().writeBlockAfterFlush();</span><br><span class="line"></span><br><span class="line">        // read connect ack (only for clients, not for replication req)</span><br><span class="line">        if (isClient) &#123;</span><br><span class="line">          BlockOpResponseProto connectAck =</span><br><span class="line">            BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(mirrorIn));</span><br><span class="line">          mirrorInStatus = connectAck.getStatus();</span><br><span class="line">          firstBadLink = connectAck.getFirstBadLink();</span><br><span class="line">          if (mirrorInStatus != SUCCESS) &#123;</span><br><span class="line">            LOG.debug(&quot;Datanode &#123;&#125; got response for connect&quot; +</span><br><span class="line">                &quot;ack  from downstream datanode with firstbadlink as &#123;&#125;&quot;,</span><br><span class="line">                targets.length, firstBadLink);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      … …</span><br><span class="line"></span><br><span class="line">  //update metrics</span><br><span class="line">  datanode.getMetrics().addWriteBlockOp(elapsed());</span><br><span class="line">  datanode.getMetrics().incrWritesFromClient(peer.isLocal(), size);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">BlockReceiver getBlockReceiver(</span><br><span class="line">    final ExtendedBlock block, final StorageType storageType,</span><br><span class="line">    final DataInputStream in,</span><br><span class="line">    final String inAddr, final String myAddr,</span><br><span class="line">    final BlockConstructionStage stage,</span><br><span class="line">    final long newGs, final long minBytesRcvd, final long maxBytesRcvd,</span><br><span class="line">    final String clientname, final DatanodeInfo srcDataNode,</span><br><span class="line">    final DataNode dn, DataChecksum requestedChecksum,</span><br><span class="line">    CachingStrategy cachingStrategy,</span><br><span class="line">    final boolean allowLazyPersist,</span><br><span class="line">    final boolean pinning,</span><br><span class="line">    final String storageId) throws IOException &#123;</span><br><span class="line">  return new BlockReceiver(block, storageType, in,</span><br><span class="line">      inAddr, myAddr, stage, newGs, minBytesRcvd, maxBytesRcvd,</span><br><span class="line">      clientname, srcDataNode, dn, requestedChecksum,</span><br><span class="line">      cachingStrategy, allowLazyPersist, pinning, storageId);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">BlockReceiver(final ExtendedBlock block, final StorageType storageType,</span><br><span class="line">  final DataInputStream in,</span><br><span class="line">  final String inAddr, final String myAddr,</span><br><span class="line">  final BlockConstructionStage stage, </span><br><span class="line">  final long newGs, final long minBytesRcvd, final long maxBytesRcvd, </span><br><span class="line">  final String clientname, final DatanodeInfo srcDataNode,</span><br><span class="line">  final DataNode datanode, DataChecksum requestedChecksum,</span><br><span class="line">  CachingStrategy cachingStrategy,</span><br><span class="line">  final boolean allowLazyPersist,</span><br><span class="line">  final boolean pinning,</span><br><span class="line">  final String storageId) throws IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  if (isDatanode) &#123; //replication or move</span><br><span class="line">    replicaHandler =</span><br><span class="line">        datanode.data.createTemporary(storageType, storageId, block, false);</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    switch (stage) &#123;</span><br><span class="line">    case PIPELINE_SETUP_CREATE:</span><br><span class="line">	  // 创建管道</span><br><span class="line">      replicaHandler = datanode.data.createRbw(storageType, storageId,</span><br><span class="line">          block, allowLazyPersist);</span><br><span class="line">      datanode.notifyNamenodeReceivingBlock(</span><br><span class="line">          block, replicaHandler.getReplica().getStorageUuid());</span><br><span class="line">      break;</span><br><span class="line">    ... ...</span><br><span class="line">    default: throw new IOException(&quot;Unsupported stage &quot; + stage + </span><br><span class="line">          &quot; while receiving block &quot; + block + &quot; from &quot; + inAddr);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public ReplicaHandler createRbw(</span><br><span class="line">    StorageType storageType, String storageId, ExtendedBlock b,</span><br><span class="line">    boolean allowLazyPersist) throws IOException &#123;</span><br><span class="line">  try (AutoCloseableLock lock = datasetLock.acquire()) &#123;</span><br><span class="line">    ... ...</span><br><span class="line"></span><br><span class="line">    if (ref == null) &#123;</span><br><span class="line">      ref = volumes.getNextVolume(storageType, storageId, b.getNumBytes());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    FsVolumeImpl v = (FsVolumeImpl) ref.getVolume();</span><br><span class="line">    // create an rbw file to hold block in the designated volume</span><br><span class="line"></span><br><span class="line">    if (allowLazyPersist &amp;&amp; !v.isTransientStorage()) &#123;</span><br><span class="line">      datanode.getMetrics().incrRamDiskBlocksWriteFallback();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ReplicaInPipeline newReplicaInfo;</span><br><span class="line">    try &#123;</span><br><span class="line">	  // 创建输出流的临时写文件 </span><br><span class="line">      newReplicaInfo = v.createRbw(b);</span><br><span class="line">      if (newReplicaInfo.getReplicaInfo().getState() != ReplicaState.RBW) &#123;</span><br><span class="line">        throw new IOException(&quot;CreateRBW returned a replica of state &quot;</span><br><span class="line">            + newReplicaInfo.getReplicaInfo().getState()</span><br><span class="line">            + &quot; for block &quot; + b.getBlockId());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; catch (IOException e) &#123;</span><br><span class="line">      IOUtils.cleanup(null, ref);</span><br><span class="line">      throw e;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());</span><br><span class="line">    return new ReplicaHandler(newReplicaInfo, ref);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public ReplicaHandler createRbw(</span><br><span class="line">    StorageType storageType, String storageId, ExtendedBlock b,</span><br><span class="line">    boolean allowLazyPersist) throws IOException &#123;</span><br><span class="line">  try (AutoCloseableLock lock = datasetLock.acquire()) &#123;</span><br><span class="line">    ... ...</span><br><span class="line"></span><br><span class="line">    if (ref == null) &#123;</span><br><span class="line">	  // 有可能有多个临时写文件</span><br><span class="line">      ref = volumes.getNextVolume(storageType, storageId, b.getNumBytes());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    FsVolumeImpl v = (FsVolumeImpl) ref.getVolume();</span><br><span class="line">    // create an rbw file to hold block in the designated volume</span><br><span class="line"></span><br><span class="line">    if (allowLazyPersist &amp;&amp; !v.isTransientStorage()) &#123;</span><br><span class="line">      datanode.getMetrics().incrRamDiskBlocksWriteFallback();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ReplicaInPipeline newReplicaInfo;</span><br><span class="line">    try &#123;</span><br><span class="line">	  // 创建输出流的临时写文件 </span><br><span class="line">      newReplicaInfo = v.createRbw(b);</span><br><span class="line">      if (newReplicaInfo.getReplicaInfo().getState() != ReplicaState.RBW) &#123;</span><br><span class="line">        throw new IOException(&quot;CreateRBW returned a replica of state &quot;</span><br><span class="line">            + newReplicaInfo.getReplicaInfo().getState()</span><br><span class="line">            + &quot; for block &quot; + b.getBlockId());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; catch (IOException e) &#123;</span><br><span class="line">      IOUtils.cleanup(null, ref);</span><br><span class="line">      throw e;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());</span><br><span class="line">    return new ReplicaHandler(newReplicaInfo, ref);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public ReplicaInPipeline createRbw(ExtendedBlock b) throws IOException &#123;</span><br><span class="line"></span><br><span class="line">  File f = createRbwFile(b.getBlockPoolId(), b.getLocalBlock());</span><br><span class="line">  LocalReplicaInPipeline newReplicaInfo = new ReplicaBuilder(ReplicaState.RBW)</span><br><span class="line">      .setBlockId(b.getBlockId())</span><br><span class="line">      .setGenerationStamp(b.getGenerationStamp())</span><br><span class="line">      .setFsVolume(this)</span><br><span class="line">      .setDirectoryToUse(f.getParentFile())</span><br><span class="line">      .setBytesToReserve(b.getNumBytes())</span><br><span class="line">      .buildLocalReplicaInPipeline();</span><br><span class="line">  return newReplicaInfo;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3 id="3-1-5-客户端接收DN写数据应答Response"><a href="#3-1-5-客户端接收DN写数据应答Response" class="headerlink" title="3.1.5 客户端接收DN写数据应答Response"></a>3.1.5 客户端接收DN写数据应答Response</h3><p>Ctrl + n全局查找DataStreamer，搜索run方法</p>
<p>DataStreamer.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void run() &#123;</span><br><span class="line"></span><br><span class="line">	long lastPacket = Time.monotonicNow();</span><br><span class="line">	TraceScope scope = null;</span><br><span class="line">	while (!streamerClosed &amp;&amp; dfsClient.clientRunning) &#123;</span><br><span class="line">	  // if the Responder encountered an error, shutdown Responder</span><br><span class="line">	  if (errorState.hasError()) &#123;</span><br><span class="line">		closeResponder();</span><br><span class="line">	  &#125;</span><br><span class="line"></span><br><span class="line">	  DFSPacket one;</span><br><span class="line">	  try &#123;</span><br><span class="line">		// process datanode IO errors if any</span><br><span class="line">		boolean doSleep = processDatanodeOrExternalError();</span><br><span class="line"></span><br><span class="line">		final int halfSocketTimeout = dfsClient.getConf().getSocketTimeout()/2;</span><br><span class="line">		synchronized (dataQueue) &#123;</span><br><span class="line">		  // wait for a packet to be sent.</span><br><span class="line">		  long now = Time.monotonicNow();</span><br><span class="line">		  while ((!shouldStop() &amp;&amp; dataQueue.size() == 0 &amp;&amp;</span><br><span class="line">			  (stage != BlockConstructionStage.DATA_STREAMING ||</span><br><span class="line">				  now - lastPacket &lt; halfSocketTimeout)) || doSleep) &#123;</span><br><span class="line">			long timeout = halfSocketTimeout - (now-lastPacket);</span><br><span class="line">			timeout = timeout &lt;= 0 ? 1000 : timeout;</span><br><span class="line">			timeout = (stage == BlockConstructionStage.DATA_STREAMING)?</span><br><span class="line">				timeout : 1000;</span><br><span class="line">			try &#123;</span><br><span class="line">			  // 如果dataQueue里面没有数据，代码会阻塞在这儿</span><br><span class="line">			  dataQueue.wait(timeout); // 接收到notify消息</span><br><span class="line">			&#125; catch (InterruptedException  e) &#123;</span><br><span class="line">			  LOG.warn(&quot;Caught exception&quot;, e);</span><br><span class="line">			&#125;</span><br><span class="line">			doSleep = false;</span><br><span class="line">			now = Time.monotonicNow();</span><br><span class="line">		  &#125;</span><br><span class="line">		  if (shouldStop()) &#123;</span><br><span class="line">			continue;</span><br><span class="line">		  &#125;</span><br><span class="line">		  // get packet to be sent.</span><br><span class="line">		  if (dataQueue.isEmpty()) &#123;</span><br><span class="line">			one = createHeartbeatPacket();</span><br><span class="line">		  &#125; else &#123;</span><br><span class="line">			try &#123;</span><br><span class="line">			  backOffIfNecessary();</span><br><span class="line">			&#125; catch (InterruptedException e) &#123;</span><br><span class="line">			  LOG.warn(&quot;Caught exception&quot;, e);</span><br><span class="line">			&#125;</span><br><span class="line">			//  队列不为空，从队列中取出packet</span><br><span class="line">			one = dataQueue.getFirst(); // regular data packet</span><br><span class="line">			SpanId[] parents = one.getTraceParents();</span><br><span class="line">			if (parents.length &gt; 0) &#123;</span><br><span class="line">			  scope = dfsClient.getTracer().</span><br><span class="line">				  newScope(&quot;dataStreamer&quot;, parents[0]);</span><br><span class="line">			  scope.getSpan().setParents(parents);</span><br><span class="line">			&#125;</span><br><span class="line">		  &#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		// get new block from namenode.</span><br><span class="line">		if (LOG.isDebugEnabled()) &#123;</span><br><span class="line">		  LOG.debug(&quot;stage=&quot; + stage + &quot;, &quot; + this);</span><br><span class="line">		&#125;</span><br><span class="line">		if (stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) &#123;</span><br><span class="line">		  LOG.debug(&quot;Allocating new block: &#123;&#125;&quot;, this);</span><br><span class="line">		  // 步骤一：向NameNode 申请block 并建立数据管道</span><br><span class="line">		  setPipeline(nextBlockOutputStream());</span><br><span class="line">		  // 步骤二：启动ResponseProcessor用来监听packet发送是否成功</span><br><span class="line">		  initDataStreaming();</span><br><span class="line">		&#125; else if (stage == BlockConstructionStage.PIPELINE_SETUP_APPEND) &#123;</span><br><span class="line">		  LOG.debug(&quot;Append to block &#123;&#125;&quot;, block);</span><br><span class="line">		  setupPipelineForAppendOrRecovery();</span><br><span class="line">		  if (streamerClosed) &#123;</span><br><span class="line">			continue;</span><br><span class="line">		  &#125;</span><br><span class="line">		  initDataStreaming();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		long lastByteOffsetInBlock = one.getLastByteOffsetBlock();</span><br><span class="line">		if (lastByteOffsetInBlock &gt; stat.getBlockSize()) &#123;</span><br><span class="line">		  throw new IOException(&quot;BlockSize &quot; + stat.getBlockSize() +</span><br><span class="line">			  &quot; &lt; lastByteOffsetInBlock, &quot; + this + &quot;, &quot; + one);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		if (one.isLastPacketInBlock()) &#123;</span><br><span class="line">		  // wait for all data packets have been successfully acked</span><br><span class="line">		  synchronized (dataQueue) &#123;</span><br><span class="line">			while (!shouldStop() &amp;&amp; ackQueue.size() != 0) &#123;</span><br><span class="line">			  try &#123;</span><br><span class="line">				// wait for acks to arrive from datanodes</span><br><span class="line">				dataQueue.wait(1000);</span><br><span class="line">			  &#125; catch (InterruptedException  e) &#123;</span><br><span class="line">				LOG.warn(&quot;Caught exception&quot;, e);</span><br><span class="line">			  &#125;</span><br><span class="line">			&#125;</span><br><span class="line">		  &#125;</span><br><span class="line">		  if (shouldStop()) &#123;</span><br><span class="line">			continue;</span><br><span class="line">		  &#125;</span><br><span class="line">		  stage = BlockConstructionStage.PIPELINE_CLOSE;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		// send the packet</span><br><span class="line">		SpanId spanId = SpanId.INVALID;</span><br><span class="line">		synchronized (dataQueue) &#123;</span><br><span class="line"></span><br><span class="line">		  // move packet from dataQueue to ackQueue</span><br><span class="line">		  if (!one.isHeartbeatPacket()) &#123;</span><br><span class="line">			if (scope != null) &#123;</span><br><span class="line">			  spanId = scope.getSpanId();</span><br><span class="line">			  scope.detach();</span><br><span class="line"></span><br><span class="line">			  one.setTraceScope(scope);</span><br><span class="line">			&#125;</span><br><span class="line">			scope = null;</span><br><span class="line">			// 步骤三：从dataQueue 把要发送的这个packet 移除出去</span><br><span class="line">			dataQueue.removeFirst();</span><br><span class="line">			// 步骤四：然后往ackQueue 里面添加这个packet</span><br><span class="line">			ackQueue.addLast(one);</span><br><span class="line">			packetSendTime.put(one.getSeqno(), Time.monotonicNow());</span><br><span class="line">			dataQueue.notifyAll();</span><br><span class="line">		  &#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		LOG.debug(&quot;&#123;&#125; sending &#123;&#125;&quot;, this, one);</span><br><span class="line"></span><br><span class="line">		// write out data to remote datanode</span><br><span class="line">		try (TraceScope ignored = dfsClient.getTracer().</span><br><span class="line">			newScope(&quot;DataStreamer#writeTo&quot;, spanId)) &#123;</span><br><span class="line">		  //  将数据写出去</span><br><span class="line">		  one.writeTo(blockStream);</span><br><span class="line">		  blockStream.flush();</span><br><span class="line">		&#125; catch (IOException e) &#123;</span><br><span class="line">		  errorState.markFirstNodeIfNotMarked();</span><br><span class="line">		  throw e;</span><br><span class="line">		&#125;</span><br><span class="line">		lastPacket = Time.monotonicNow();</span><br><span class="line"></span><br><span class="line">		// update bytesSent</span><br><span class="line">		long tmpBytesSent = one.getLastByteOffsetBlock();</span><br><span class="line">		if (bytesSent &lt; tmpBytesSent) &#123;</span><br><span class="line">		  bytesSent = tmpBytesSent;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		if (shouldStop()) &#123;</span><br><span class="line">		  continue;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		// Is this block full?</span><br><span class="line">		if (one.isLastPacketInBlock()) &#123;</span><br><span class="line">		  // wait for the close packet has been acked</span><br><span class="line">		  synchronized (dataQueue) &#123;</span><br><span class="line">			while (!shouldStop() &amp;&amp; ackQueue.size() != 0) &#123;</span><br><span class="line">			  dataQueue.wait(1000);// wait for acks to arrive from datanodes</span><br><span class="line">			&#125;</span><br><span class="line">		  &#125;</span><br><span class="line">		  if (shouldStop()) &#123;</span><br><span class="line">			continue;</span><br><span class="line">		  &#125;</span><br><span class="line"></span><br><span class="line">		  endBlock();</span><br><span class="line">		&#125;</span><br><span class="line">		if (progress != null) &#123; progress.progress(); &#125;</span><br><span class="line"></span><br><span class="line">		// This is used by unit test to trigger race conditions.</span><br><span class="line">		if (artificialSlowdown != 0 &amp;&amp; dfsClient.clientRunning) &#123;</span><br><span class="line">		  Thread.sleep(artificialSlowdown);</span><br><span class="line">		&#125;</span><br><span class="line">	  &#125; catch (Throwable e) &#123;</span><br><span class="line">		... ...</span><br><span class="line">	  &#125; finally &#123;</span><br><span class="line">		if (scope != null) &#123;</span><br><span class="line">		  scope.close();</span><br><span class="line">		  scope = null;</span><br><span class="line">		&#125;</span><br><span class="line">	  &#125;</span><br><span class="line">	&#125;</span><br><span class="line">	closeInternal();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private void initDataStreaming() &#123;</span><br><span class="line">  this.setName(&quot;DataStreamer for file &quot; + src +</span><br><span class="line">      &quot; block &quot; + block);</span><br><span class="line">  ... ...</span><br><span class="line">  response = new ResponseProcessor(nodes);</span><br><span class="line">  response.start();</span><br><span class="line">  stage = BlockConstructionStage.DATA_STREAMING;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击response再点击ResponseProcessor，ctrl + f 查找run方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public void run() &#123;</span><br><span class="line">    ... ...</span><br><span class="line">	ackQueue.removeFirst();</span><br><span class="line">	packetSendTime.remove(seqno);</span><br><span class="line">	dataQueue.notifyAll();</span><br><span class="line">	... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h1 id="第4章-Yarn源码解析"><a href="#第4章-Yarn源码解析" class="headerlink" title="第4章 Yarn源码解析"></a>第4章 Yarn源码解析</h1><p><img src="https://image.3001.net/images/20221031/16671915538557.png#alt=image-20221031124546767"></p>
<p><img src="https://image.3001.net/images/20221031/16671915679500.png#alt=image-20221031124601390"></p>
<h2 id="4-1-Yarn客户端向RM提交作业"><a href="#4-1-Yarn客户端向RM提交作业" class="headerlink" title="4.1 Yarn客户端向RM提交作业"></a>4.1 Yarn客户端向RM提交作业</h2><p>1）在wordcount程序的驱动类中点击</p>
<p>    Job.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">boolean result = job.waitForCompletion(true);</span><br><span class="line"></span><br><span class="line">public boolean waitForCompletion(boolean verbose</span><br><span class="line">                                 ) throws IOException, InterruptedException,</span><br><span class="line">                                          ClassNotFoundException &#123;</span><br><span class="line">  if (state == JobState.DEFINE) &#123;</span><br><span class="line">    submit();</span><br><span class="line">  &#125;</span><br><span class="line">  if (verbose) &#123;</span><br><span class="line">    monitorAndPrintJob();</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    // get the completion poll interval from the client.</span><br><span class="line">    int completionPollIntervalMillis = </span><br><span class="line">      Job.getCompletionPollInterval(cluster.getConf());</span><br><span class="line">    while (!isComplete()) &#123;</span><br><span class="line">      try &#123;</span><br><span class="line">        Thread.sleep(completionPollIntervalMillis);</span><br><span class="line">      &#125; catch (InterruptedException ie) &#123;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  return isSuccessful();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void submit() </span><br><span class="line">       throws IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">  ensureState(JobState.DEFINE);</span><br><span class="line">  setUseNewAPI();</span><br><span class="line">  connect();</span><br><span class="line">  final JobSubmitter submitter = </span><br><span class="line">      getJobSubmitter(cluster.getFileSystem(), cluster.getClient());</span><br><span class="line">  status = ugi.doAs(new PrivilegedExceptionAction&lt;JobStatus&gt;() &#123;</span><br><span class="line">    public JobStatus run() throws IOException, InterruptedException, </span><br><span class="line">    ClassNotFoundException &#123;</span><br><span class="line">      return submitter.submitJobInternal(Job.this, cluster);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">  state = JobState.RUNNING;</span><br><span class="line">  LOG.info(&quot;The url to track the job: &quot; + getTrackingURL());</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>点击submitJobInternal()</p>
<p>JobSubmitter.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">JobStatus submitJobInternal(Job job, Cluster cluster) </span><br><span class="line">  throws ClassNotFoundException, InterruptedException, IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  status = submitClient.submitJob(</span><br><span class="line">          jobId, submitJobDir.toString(), job.getCredentials()); </span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public JobStatus submitJob(JobID jobId, String jobSubmitDir, Credentials ts) throws IOException, InterruptedException;</span><br></pre></td></tr></table></figure>

<p>2）创建提交环境</p>
<p>ctrl + alt +B 查找submitJob实现类，YARNRunner.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line">public JobStatus submitJob(JobID jobId, String jobSubmitDir, Credentials ts)</span><br><span class="line">throws IOException, InterruptedException &#123;</span><br><span class="line">  </span><br><span class="line">  addHistoryToken(ts);</span><br><span class="line">  // 创建提交环境：</span><br><span class="line">  ApplicationSubmissionContext appContext =</span><br><span class="line">    createApplicationSubmissionContext(conf, jobSubmitDir, ts);</span><br><span class="line"></span><br><span class="line">  // Submit to ResourceManager</span><br><span class="line">  try &#123;</span><br><span class="line">    // 向RM提交一个应用程序，appContext里面封装了启动mrappMaster和运行container的命令</span><br><span class="line">    ApplicationId applicationId =</span><br><span class="line">        resMgrDelegate.submitApplication(appContext);</span><br><span class="line">		</span><br><span class="line">    // 获取提交响应</span><br><span class="line">    ApplicationReport appMaster = resMgrDelegate</span><br><span class="line">        .getApplicationReport(applicationId);</span><br><span class="line">		</span><br><span class="line">    String diagnostics =</span><br><span class="line">        (appMaster == null ?</span><br><span class="line">            &quot;application report is null&quot; : appMaster.getDiagnostics());</span><br><span class="line">    if (appMaster == null</span><br><span class="line">        || appMaster.getYarnApplicationState() == YarnApplicationState.FAILED</span><br><span class="line">        || appMaster.getYarnApplicationState() == YarnApplicationState.KILLED) &#123;</span><br><span class="line">      throw new IOException(&quot;Failed to run job : &quot; +</span><br><span class="line">          diagnostics);</span><br><span class="line">    &#125;</span><br><span class="line">    return clientCache.getClient(jobId).getJobStatus(jobId);</span><br><span class="line">  &#125; catch (YarnException e) &#123;</span><br><span class="line">    throw new IOException(e);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public ApplicationSubmissionContext createApplicationSubmissionContext(</span><br><span class="line">    Configuration jobConf, String jobSubmitDir, Credentials ts)</span><br><span class="line">    throws IOException &#123;</span><br><span class="line">  ApplicationId applicationId = resMgrDelegate.getApplicationId();</span><br><span class="line"></span><br><span class="line">  // Setup LocalResources</span><br><span class="line">  // 封装了本地资源相关路径</span><br><span class="line">  Map&lt;String, LocalResource&gt; localResources =</span><br><span class="line">      setupLocalResources(jobConf, jobSubmitDir);</span><br><span class="line"></span><br><span class="line">  // Setup security tokens</span><br><span class="line">  DataOutputBuffer dob = new DataOutputBuffer();</span><br><span class="line">  ts.writeTokenStorageToStream(dob);</span><br><span class="line">  ByteBuffer securityTokens =</span><br><span class="line">      ByteBuffer.wrap(dob.getData(), 0, dob.getLength());</span><br><span class="line"></span><br><span class="line">  // Setup ContainerLaunchContext for AM container</span><br><span class="line">  // 封装了启动mrappMaster和运行container的命令</span><br><span class="line">  List&lt;String&gt; vargs = setupAMCommand(jobConf);</span><br><span class="line">  ContainerLaunchContext amContainer = setupContainerLaunchContextForAM(</span><br><span class="line">      jobConf, localResources, securityTokens, vargs);</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  return appContext;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private List&lt;String&gt; setupAMCommand(Configuration jobConf) &#123;</span><br><span class="line">  List&lt;String&gt; vargs = new ArrayList&lt;&gt;(8);</span><br><span class="line">  // Java进程启动命令开始</span><br><span class="line">  vargs.add(MRApps.crossPlatformifyMREnv(jobConf, Environment.JAVA_HOME)</span><br><span class="line">      + &quot;/bin/java&quot;);</span><br><span class="line"></span><br><span class="line">  Path amTmpDir =</span><br><span class="line">      new Path(MRApps.crossPlatformifyMREnv(conf, Environment.PWD),</span><br><span class="line">          YarnConfiguration.DEFAULT_CONTAINER_TEMP_DIR);</span><br><span class="line">  vargs.add(&quot;-Djava.io.tmpdir=&quot; + amTmpDir);</span><br><span class="line">  MRApps.addLog4jSystemProperties(null, vargs, conf);</span><br><span class="line"></span><br><span class="line">  // Check for Java Lib Path usage in MAP and REDUCE configs</span><br><span class="line">  warnForJavaLibPath(conf.get(MRJobConfig.MAP_JAVA_OPTS, &quot;&quot;),</span><br><span class="line">      &quot;map&quot;,</span><br><span class="line">      MRJobConfig.MAP_JAVA_OPTS,</span><br><span class="line">      MRJobConfig.MAP_ENV);</span><br><span class="line">  warnForJavaLibPath(conf.get(MRJobConfig.MAPRED_MAP_ADMIN_JAVA_OPTS, &quot;&quot;),</span><br><span class="line">      &quot;map&quot;,</span><br><span class="line">      MRJobConfig.MAPRED_MAP_ADMIN_JAVA_OPTS,</span><br><span class="line">      MRJobConfig.MAPRED_ADMIN_USER_ENV);</span><br><span class="line">  warnForJavaLibPath(conf.get(MRJobConfig.REDUCE_JAVA_OPTS, &quot;&quot;),</span><br><span class="line">      &quot;reduce&quot;,</span><br><span class="line">      MRJobConfig.REDUCE_JAVA_OPTS,</span><br><span class="line">      MRJobConfig.REDUCE_ENV);</span><br><span class="line">  warnForJavaLibPath(conf.get(MRJobConfig.MAPRED_REDUCE_ADMIN_JAVA_OPTS, &quot;&quot;),</span><br><span class="line">      &quot;reduce&quot;,</span><br><span class="line">      MRJobConfig.MAPRED_REDUCE_ADMIN_JAVA_OPTS,</span><br><span class="line">      MRJobConfig.MAPRED_ADMIN_USER_ENV);</span><br><span class="line"></span><br><span class="line">  // Add AM admin command opts before user command opts</span><br><span class="line">  // so that it can be overridden by user</span><br><span class="line">  String mrAppMasterAdminOptions = conf.get(MRJobConfig.MR_AM_ADMIN_COMMAND_OPTS,</span><br><span class="line">      MRJobConfig.DEFAULT_MR_AM_ADMIN_COMMAND_OPTS);</span><br><span class="line">  warnForJavaLibPath(mrAppMasterAdminOptions, &quot;app master&quot;,</span><br><span class="line">      MRJobConfig.MR_AM_ADMIN_COMMAND_OPTS, MRJobConfig.MR_AM_ADMIN_USER_ENV);</span><br><span class="line">  vargs.add(mrAppMasterAdminOptions);</span><br><span class="line"></span><br><span class="line">  // Add AM user command opts 用户命令参数</span><br><span class="line">  String mrAppMasterUserOptions = conf.get(MRJobConfig.MR_AM_COMMAND_OPTS,</span><br><span class="line">      MRJobConfig.DEFAULT_MR_AM_COMMAND_OPTS);</span><br><span class="line">  warnForJavaLibPath(mrAppMasterUserOptions, &quot;app master&quot;,</span><br><span class="line">      MRJobConfig.MR_AM_COMMAND_OPTS, MRJobConfig.MR_AM_ENV);</span><br><span class="line">  vargs.add(mrAppMasterUserOptions);</span><br><span class="line"></span><br><span class="line">  if (jobConf.getBoolean(MRJobConfig.MR_AM_PROFILE,</span><br><span class="line">      MRJobConfig.DEFAULT_MR_AM_PROFILE)) &#123;</span><br><span class="line">    final String profileParams = jobConf.get(MRJobConfig.MR_AM_PROFILE_PARAMS,</span><br><span class="line">        MRJobConfig.DEFAULT_TASK_PROFILE_PARAMS);</span><br><span class="line">    if (profileParams != null) &#123;</span><br><span class="line">      vargs.add(String.format(profileParams,</span><br><span class="line">          ApplicationConstants.LOG_DIR_EXPANSION_VAR + Path.SEPARATOR</span><br><span class="line">              + TaskLog.LogName.PROFILE));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // 封装了要启动的mrappmaster全类名 </span><br><span class="line">  // org.apache.hadoop.mapreduce.v2.app.MRAppMaster</span><br><span class="line">  vargs.add(MRJobConfig.APPLICATION_MASTER_CLASS);</span><br><span class="line">  vargs.add(&quot;1&gt;&quot; + ApplicationConstants.LOG_DIR_EXPANSION_VAR +</span><br><span class="line">      Path.SEPARATOR + ApplicationConstants.STDOUT);</span><br><span class="line">  vargs.add(&quot;2&gt;&quot; + ApplicationConstants.LOG_DIR_EXPANSION_VAR +</span><br><span class="line">      Path.SEPARATOR + ApplicationConstants.STDERR);</span><br><span class="line">  return vargs;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>3）向Yarn提交</p>
<p>点击submitJob方法中的submitApplication()</p>
<p>YARNRunner.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ApplicationId applicationId = </span><br><span class="line">resMgrDelegate.submitApplication(appContext);</span><br><span class="line"></span><br><span class="line">public ApplicationId</span><br><span class="line">    submitApplication(ApplicationSubmissionContext appContext)</span><br><span class="line">        throws YarnException, IOException &#123;</span><br><span class="line">  return client.submitApplication(appContext);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>ctrl + alt +B 查找submitApplication实现类，YarnClientImpl.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">public ApplicationId</span><br><span class="line">    submitApplication(ApplicationSubmissionContext appContext)</span><br><span class="line">        throws YarnException, IOException &#123;</span><br><span class="line">  ApplicationId applicationId = appContext.getApplicationId();</span><br><span class="line">  if (applicationId == null) &#123;</span><br><span class="line">    throw new ApplicationIdNotProvidedException(</span><br><span class="line">        &quot;ApplicationId is not provided in ApplicationSubmissionContext&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">  // 创建一个提交请求</span><br><span class="line">  SubmitApplicationRequest request =</span><br><span class="line">      Records.newRecord(SubmitApplicationRequest.class);</span><br><span class="line">  request.setApplicationSubmissionContext(appContext);</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  //TODO: YARN-1763:Handle RM failovers during the submitApplication call.</span><br><span class="line">  // 继续提交，实现类是ApplicationClientProtocolPBClientImpl</span><br><span class="line">  rmClient.submitApplication(request);</span><br><span class="line"></span><br><span class="line">  int pollCount = 0;</span><br><span class="line">  long startTime = System.currentTimeMillis();</span><br><span class="line">  EnumSet&lt;YarnApplicationState&gt; waitingStates = </span><br><span class="line">                               EnumSet.of(YarnApplicationState.NEW,</span><br><span class="line">                               YarnApplicationState.NEW_SAVING,</span><br><span class="line">                               YarnApplicationState.SUBMITTED);</span><br><span class="line">  EnumSet&lt;YarnApplicationState&gt; failToSubmitStates = </span><br><span class="line">                                EnumSet.of(YarnApplicationState.FAILED,</span><br><span class="line">                                YarnApplicationState.KILLED);		</span><br><span class="line">  while (true) &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">	  // 获取提交给Yarn的反馈</span><br><span class="line">      ApplicationReport appReport = getApplicationReport(applicationId);</span><br><span class="line">      YarnApplicationState state = appReport.getYarnApplicationState();</span><br><span class="line">      ... ...</span><br><span class="line">    &#125; catch (ApplicationNotFoundException ex) &#123;</span><br><span class="line">      // FailOver or RM restart happens before RMStateStore saves</span><br><span class="line">      // ApplicationState</span><br><span class="line">      LOG.info(&quot;Re-submit application &quot; + applicationId + &quot;with the &quot; +</span><br><span class="line">          &quot;same ApplicationSubmissionContext&quot;);</span><br><span class="line">	  // 如果提交失败，则再次提交</span><br><span class="line">      rmClient.submitApplication(request);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  return applicationId;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>ctrl + alt +B 查找submitApplication实现类，ClientRMService.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">public SubmitApplicationResponse submitApplication(</span><br><span class="line">    SubmitApplicationRequest request) throws YarnException, IOException &#123;</span><br><span class="line">  ApplicationSubmissionContext submissionContext = request</span><br><span class="line">      .getApplicationSubmissionContext();</span><br><span class="line">  ApplicationId applicationId = submissionContext.getApplicationId();</span><br><span class="line">  CallerContext callerContext = CallerContext.getCurrent();</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  try &#123;</span><br><span class="line">    // call RMAppManager to submit application directly</span><br><span class="line">    rmAppManager.submitApplication(submissionContext,</span><br><span class="line">        System.currentTimeMillis(), user);</span><br><span class="line"></span><br><span class="line">    LOG.info(&quot;Application with id &quot; + applicationId.getId() + </span><br><span class="line">        &quot; submitted by user &quot; + user);</span><br><span class="line">    RMAuditLogger.logSuccess(user, AuditConstants.SUBMIT_APP_REQUEST,</span><br><span class="line">        &quot;ClientRMService&quot;, applicationId, callerContext,</span><br><span class="line">        submissionContext.getQueue());</span><br><span class="line">  &#125; catch (YarnException e) &#123;</span><br><span class="line">    LOG.info(&quot;Exception in submitting &quot; + applicationId, e);</span><br><span class="line">    RMAuditLogger.logFailure(user, AuditConstants.SUBMIT_APP_REQUEST,</span><br><span class="line">        e.getMessage(), &quot;ClientRMService&quot;,</span><br><span class="line">        &quot;Exception in submitting application&quot;, applicationId, callerContext,</span><br><span class="line">        submissionContext.getQueue());</span><br><span class="line">    throw e;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  return recordFactory</span><br><span class="line">      .newRecordInstance(SubmitApplicationResponse.class);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="4-2-RM启动MRAppMaster"><a href="#4-2-RM启动MRAppMaster" class="headerlink" title="4.2 RM启动MRAppMaster"></a>4.2 RM启动MRAppMaster</h2><p>0）在pom.xml中增加如下依赖</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hadoop-mapreduce-client-app&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.1.3&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>ctrl +n 查找MRAppMaster，搜索main方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">  try &#123;</span><br><span class="line">    ... ...</span><br><span class="line"></span><br><span class="line">	// 初始化一个container</span><br><span class="line">    ContainerId containerId = ContainerId.fromString(containerIdStr);</span><br><span class="line">    ApplicationAttemptId applicationAttemptId =</span><br><span class="line">        containerId.getApplicationAttemptId();</span><br><span class="line">    if (applicationAttemptId != null) &#123;</span><br><span class="line">      CallerContext.setCurrent(new CallerContext.Builder(</span><br><span class="line">          &quot;mr_appmaster_&quot; + applicationAttemptId.toString()).build());</span><br><span class="line">    &#125;</span><br><span class="line">    long appSubmitTime = Long.parseLong(appSubmitTimeStr);</span><br><span class="line">    </span><br><span class="line">    // 创建appMaster对象</span><br><span class="line">    MRAppMaster appMaster =</span><br><span class="line">        new MRAppMaster(applicationAttemptId, containerId, nodeHostString,</span><br><span class="line">            Integer.parseInt(nodePortString),</span><br><span class="line">            Integer.parseInt(nodeHttpPortString), appSubmitTime);</span><br><span class="line">    ... ...</span><br><span class="line">	</span><br><span class="line">	// 初始化并启动AppMaster</span><br><span class="line">    initAndStartAppMaster(appMaster, conf, jobUserName);</span><br><span class="line">  &#125; catch (Throwable t) &#123;</span><br><span class="line">    LOG.error(&quot;Error starting MRAppMaster&quot;, t);</span><br><span class="line">    ExitUtil.terminate(1, t);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">protected static void initAndStartAppMaster(final MRAppMaster appMaster,</span><br><span class="line">    final JobConf conf, String jobUserName) throws IOException,</span><br><span class="line">    InterruptedException &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  conf.getCredentials().addAll(credentials);</span><br><span class="line">  appMasterUgi.doAs(new PrivilegedExceptionAction&lt;Object&gt;() &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public Object run() throws Exception &#123;</span><br><span class="line">	  // 初始化</span><br><span class="line">      appMaster.init(conf);</span><br><span class="line">	  // 启动</span><br><span class="line">      appMaster.start();</span><br><span class="line">      if(appMaster.errorHappenedShutDown) &#123;</span><br><span class="line">        throw new IOException(&quot;Was asked to shut down.&quot;);</span><br><span class="line">      &#125;</span><br><span class="line">      return null;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void init(Configuration conf) &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  synchronized (stateChangeLock) &#123;</span><br><span class="line">    if (enterState(STATE.INITED) != STATE.INITED) &#123;</span><br><span class="line">      setConfig(conf);</span><br><span class="line">      try &#123;</span><br><span class="line">	    // 调用MRAppMaster中的serviceInit()方法</span><br><span class="line">        serviceInit(config);</span><br><span class="line">        if (isInState(STATE.INITED)) &#123;</span><br><span class="line">          //if the service ended up here during init,</span><br><span class="line">          //notify the listeners</span><br><span class="line">		  // 如果初始化完成，通知监听器</span><br><span class="line">          notifyListeners();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; catch (Exception e) &#123;</span><br><span class="line">        noteFailure(e);</span><br><span class="line">        ServiceOperations.stopQuietly(LOG, this);</span><br><span class="line">        throw ServiceStateException.convert(e);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>ctrl + alt +B 查找serviceInit实现类，MRAppMaster.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">protected void serviceInit(final Configuration conf) throws Exception &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  // 创建提交路径</span><br><span class="line">  clientService = createClientService(context);</span><br><span class="line">  </span><br><span class="line">  // 创建调度器</span><br><span class="line">  clientService.init(conf);</span><br><span class="line">  </span><br><span class="line">  // 创建job提交RPC客户端</span><br><span class="line">  containerAllocator = createContainerAllocator(clientService, context);</span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击MRAppMaster.java 中的initAndStartAppMaster 方法中的appMaster.start();</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">public void start() &#123;</span><br><span class="line">  if (isInState(STATE.STARTED)) &#123;</span><br><span class="line">    return;</span><br><span class="line">  &#125;</span><br><span class="line">  //enter the started state</span><br><span class="line">  synchronized (stateChangeLock) &#123;</span><br><span class="line">    if (stateModel.enterState(STATE.STARTED) != STATE.STARTED) &#123;</span><br><span class="line">      try &#123;</span><br><span class="line">        startTime = System.currentTimeMillis();</span><br><span class="line">		// 调用MRAppMaster中的serviceStart()方法</span><br><span class="line">        serviceStart();</span><br><span class="line">        if (isInState(STATE.STARTED)) &#123;</span><br><span class="line">          //if the service started (and isn&#x27;t now in a later state), notify</span><br><span class="line">          LOG.debug(&quot;Service &#123;&#125; is started&quot;, getName());</span><br><span class="line">          notifyListeners();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; catch (Exception e) &#123;</span><br><span class="line">        noteFailure(e);</span><br><span class="line">        ServiceOperations.stopQuietly(LOG, this);</span><br><span class="line">        throw ServiceStateException.convert(e);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">protected void serviceStart() throws Exception &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  if (initFailed) &#123;</span><br><span class="line">    JobEvent initFailedEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT_FAILED);</span><br><span class="line">    jobEventDispatcher.handle(initFailedEvent);</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    // All components have started, start the job.</span><br><span class="line">	// 初始化成功后，提交Job到队列中</span><br><span class="line">    startJobs();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">protected void startJobs() &#123;</span><br><span class="line">  /** create a job-start event to get this ball rolling */</span><br><span class="line">  JobEvent startJobEvent = new JobStartEvent(job.getID(),</span><br><span class="line">      recoveredJobStartTime);</span><br><span class="line">  /** send the job-start event. this triggers the job execution. */</span><br><span class="line">  // 这里将job存放到yarn队列</span><br><span class="line">  // dispatcher = AsyncDispatcher</span><br><span class="line">  // getEventHandler()返回的是GenericEventHandler</span><br><span class="line">  dispatcher.getEventHandler().handle(startJobEvent);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>ctrl + alt +B 查找handle实现类，GenericEventHandler.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class GenericEventHandler implements EventHandler&lt;Event&gt; &#123;</span><br><span class="line">  public void handle(Event event) &#123;</span><br><span class="line">    ... ...</span><br><span class="line">    try &#123;</span><br><span class="line">	  // 将job存储到yarn队列中</span><br><span class="line">      eventQueue.put(event);</span><br><span class="line">    &#125; catch (InterruptedException e) &#123;</span><br><span class="line">      ... ...</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="4-3-调度器任务执行（YarnChild）"><a href="#4-3-调度器任务执行（YarnChild）" class="headerlink" title="4.3 调度器任务执行（YarnChild）"></a>4.3 调度器任务执行（YarnChild）</h2><p>1）启动MapTask</p>
<p>ctrl +n 查找YarnChild，搜索main方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">public static void main(String[] args) throws Throwable &#123;</span><br><span class="line">  Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());</span><br><span class="line">  LOG.debug(&quot;Child starting&quot;);</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  task = myTask.getTask();</span><br><span class="line">  YarnChild.taskid = task.getTaskID();</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  // Create a final reference to the task for the doAs block</span><br><span class="line">  final Task taskFinal = task;</span><br><span class="line">  childUGI.doAs(new PrivilegedExceptionAction&lt;Object&gt;() &#123;</span><br><span class="line">      @Override</span><br><span class="line">      public Object run() throws Exception &#123;</span><br><span class="line">        // use job-specified working directory</span><br><span class="line">        setEncryptedSpillKeyIfRequired(taskFinal);</span><br><span class="line">        FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory());</span><br><span class="line">		// 调用task执行（maptask或者reducetask）</span><br><span class="line">        taskFinal.run(job, umbilical); // run the task</span><br><span class="line">        return null;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125; </span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>ctrl + alt +B 查找run实现类，maptask.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">public void run(final JobConf job, final TaskUmbilicalProtocol umbilical)</span><br><span class="line">  throws IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">  this.umbilical = umbilical;</span><br><span class="line"></span><br><span class="line">  // 判断是否是MapTask</span><br><span class="line">  if (isMapTask()) &#123;</span><br><span class="line">    // If there are no reducers then there won&#x27;t be any sort. Hence the map </span><br><span class="line">    // phase will govern the entire attempt&#x27;s progress.</span><br><span class="line">	// 如果reducetask个数为零，maptask占用整个任务的100%</span><br><span class="line">    if (conf.getNumReduceTasks() == 0) &#123;</span><br><span class="line">      mapPhase = getProgress().addPhase(&quot;map&quot;, 1.0f);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      // If there are reducers then the entire attempt&#x27;s progress will be </span><br><span class="line">      // split between the map phase (67%) and the sort phase (33%).</span><br><span class="line">	  // 如果reduceTask个数不为零，MapTask占用整个任务的66.7% sort阶段占比</span><br><span class="line">      mapPhase = getProgress().addPhase(&quot;map&quot;, 0.667f);</span><br><span class="line">      sortPhase  = getProgress().addPhase(&quot;sort&quot;, 0.333f);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ... ...</span><br><span class="line">  if (useNewApi) &#123;</span><br><span class="line">    // 调用新的API执行maptask</span><br><span class="line">    runNewMapper(job, splitMetaInfo, umbilical, reporter);</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    runOldMapper(job, splitMetaInfo, umbilical, reporter);</span><br><span class="line">  &#125;</span><br><span class="line">  done(umbilical, reporter);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void runNewMapper(final JobConf job,</span><br><span class="line">                  final TaskSplitIndex splitIndex,</span><br><span class="line">                  final TaskUmbilicalProtocol umbilical,</span><br><span class="line">                  TaskReporter reporter</span><br><span class="line">                  ) throws IOException, ClassNotFoundException,</span><br><span class="line">                           InterruptedException &#123;</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  try &#123;</span><br><span class="line">    input.initialize(split, mapperContext);</span><br><span class="line">	// 运行maptask</span><br><span class="line">    mapper.run(mapperContext);</span><br><span class="line">	</span><br><span class="line">    mapPhase.complete();</span><br><span class="line">    setPhase(TaskStatus.Phase.SORT);</span><br><span class="line">    statusUpdate(umbilical);</span><br><span class="line">    input.close();</span><br><span class="line">    input = null;</span><br><span class="line">    output.close(mapperContext);</span><br><span class="line">    output = null;</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    closeQuietly(input);</span><br><span class="line">    closeQuietly(output, mapperContext);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Mapper.java（和Map联系在一起）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public void run(Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">  setup(context);</span><br><span class="line">  try &#123;</span><br><span class="line">    while (context.nextKeyValue()) &#123;</span><br><span class="line">      map(context.getCurrentKey(), context.getCurrentValue(), context);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    cleanup(context);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2）启动ReduceTask</p>
<p>在YarnChild.java类中的main方法中ctrl + alt +B 查找run实现类，reducetask.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">public void run(JobConf job, final TaskUmbilicalProtocol umbilical)</span><br><span class="line">  throws IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">  job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  if (useNewApi) &#123;</span><br><span class="line">	// 调用新API执行reduce</span><br><span class="line">    runNewReducer(job, umbilical, reporter, rIter, comparator, </span><br><span class="line">                  keyClass, valueClass);</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    runOldReducer(job, umbilical, reporter, rIter, comparator, </span><br><span class="line">                  keyClass, valueClass);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  shuffleConsumerPlugin.close();</span><br><span class="line">  done(umbilical, reporter);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void runNewReducer(JobConf job,</span><br><span class="line">                   final TaskUmbilicalProtocol umbilical,</span><br><span class="line">                   final TaskReporter reporter,</span><br><span class="line">                   RawKeyValueIterator rIter,</span><br><span class="line">                   RawComparator&lt;INKEY&gt; comparator,</span><br><span class="line">                   Class&lt;INKEY&gt; keyClass,</span><br><span class="line">                   Class&lt;INVALUE&gt; valueClass</span><br><span class="line">                   ) throws IOException,InterruptedException, </span><br><span class="line">                            ClassNotFoundException &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  try &#123;</span><br><span class="line">    // 调用reducetask的run方法</span><br><span class="line">    reducer.run(reducerContext);</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    trackedRW.close(reducerContext);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Reduce.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public void run(Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">  setup(context);</span><br><span class="line">  try &#123;</span><br><span class="line">    while (context.nextKey()) &#123;</span><br><span class="line">      reduce(context.getCurrentKey(), context.getValues(), context);</span><br><span class="line">      // If a back up store is used, reset it</span><br><span class="line">      Iterator&lt;VALUEIN&gt; iter = context.getValues().iterator();</span><br><span class="line">      if(iter instanceof ReduceContext.ValueIterator) &#123;</span><br><span class="line">        ((ReduceContext.ValueIterator&lt;VALUEIN&gt;)iter).resetBackupStore();        </span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    cleanup(context);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h1 id="第5章-MapReduce源码解析"><a href="#第5章-MapReduce源码解析" class="headerlink" title="第5章 MapReduce源码解析"></a>第5章 MapReduce源码解析</h1><p>说明：在讲MapReduce课程时，已经讲过源码，在这就不再赘述。</p>
<h2 id="5-1-Job提交流程源码和切片源码详解"><a href="#5-1-Job提交流程源码和切片源码详解" class="headerlink" title="5.1 Job提交流程源码和切片源码详解"></a>5.1 Job提交流程源码和切片源码详解</h2><p><strong>1）Job提交流程源码详解</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">waitForCompletion()</span><br><span class="line"></span><br><span class="line">submit();</span><br><span class="line"></span><br><span class="line">// 1建立连接</span><br><span class="line">	connect();	</span><br><span class="line">		// 1）创建提交Job的代理</span><br><span class="line">		new Cluster(getConfiguration());</span><br><span class="line">			// （1）判断是本地运行环境还是yarn集群运行环境</span><br><span class="line">			initialize(jobTrackAddr, conf); </span><br><span class="line"></span><br><span class="line">// 2 提交job</span><br><span class="line">submitter.submitJobInternal(Job.this, cluster)</span><br><span class="line"></span><br><span class="line">	// 1）创建给集群提交数据的Stag路径</span><br><span class="line">	Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line"></span><br><span class="line">	// 2）获取jobid ，并创建Job路径</span><br><span class="line">	JobID jobId = submitClient.getNewJobID();</span><br><span class="line"></span><br><span class="line">	// 3）拷贝jar包到集群</span><br><span class="line">copyAndConfigureFiles(job, submitJobDir);	</span><br><span class="line">	rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line"></span><br><span class="line">	// 4）计算切片，生成切片规划文件</span><br><span class="line">writeSplits(job, submitJobDir);</span><br><span class="line">		maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">		input.getSplits(job);</span><br><span class="line"></span><br><span class="line">	// 5）向Stag路径写XML配置文件</span><br><span class="line">writeConf(conf, submitJobFile);</span><br><span class="line">	conf.writeXml(out);</span><br><span class="line"></span><br><span class="line">	// 6）提交Job,返回提交状态</span><br><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure>

<p><img src="https://image.3001.net/images/20221031/16671922343697.png#alt=image-20221031125708097"></p>
<p><strong>2）FileInputFormat切片源码解析（input.getSplits(job)）</strong></p>
<p><img src="https://image.3001.net/images/20221031/16671922632386.png#alt=image-20221031125737219"></p>
<h2 id="5-2-MapTask-amp-ReduceTask源码解析"><a href="#5-2-MapTask-amp-ReduceTask源码解析" class="headerlink" title="5.2 MapTask &amp; ReduceTask源码解析"></a>5.2 MapTask &amp; ReduceTask源码解析</h2><p><strong>1）MapTask源码解析流程</strong></p>
<p><img src="https://image.3001.net/images/20221031/16671923487676.png#alt=image-20221031125901743"></p>
<p><strong>2）ReduceTask源码解析流程</strong></p>
<p><img src="https://image.3001.net/images/20221031/16671923852419.png#alt=image-20221031125938614"></p>
<h1 id="第6章-Hadoop源码编译"><a href="#第6章-Hadoop源码编译" class="headerlink" title="第6章 Hadoop源码编译"></a>第6章 Hadoop源码编译</h1><h2 id="6-1-前期准备工作"><a href="#6-1-前期准备工作" class="headerlink" title="6.1 前期准备工作"></a>6.1 前期准备工作</h2><p><strong>1）官网下载源码</strong></p>
<p><a target="_blank" rel="noopener" href="https://hadoop.apache.org/release/3.1.3.html">https://hadoop.apache.org/release/3.1.3.html</a></p>
<p><strong>2）修改源码中的HDFS副本数的设置</strong></p>
<p><strong>3）CentOS虚拟机准备</strong></p>
<p>（1）CentOS联网</p>
<p>配置CentOS能连接外网。Linux虚拟机ping <a target="_blank" rel="noopener" href="http://www.baidu.com/">www.baidu.com</a> 是畅通的</p>
<p>注意：采用root角色编译，减少文件夹权限出现问题</p>
<p>（2）Jar包准备（Hadoop源码、JDK8、Maven、Ant 、Protobuf）</p>
<ul>
<li><p>hadoop-3.1.3-src.tar.gz</p>
</li>
<li><p>jdk-8u212-linux-x64.tar.gz</p>
</li>
<li><p>apache-maven-3.6.3-bin.tar.gz</p>
</li>
<li><p>protobuf-2.5.0.tar.gz（序列化的框架）</p>
</li>
<li><p>cmake-3.17.0.tar.gz</p>
</li>
</ul>
<h2 id="6-2-工具包安装"><a href="#6-2-工具包安装" class="headerlink" title="6.2 工具包安装"></a>6.2 工具包安装</h2><p>注意：所有操作必须在root用户下完成</p>
<p><strong>0）分别创建/opt/software/hadoop_source和/opt/module/hadoop_source路径</strong></p>
<p>*<em>1</em>***）上传软件包到指定的目录，例如</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop_source]$ pwd</span><br><span class="line">/opt/software/hadoop_source</span><br><span class="line">[root@hadoop101 hadoop_source]$ ll</span><br><span class="line">总用量 55868</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu  9506321 3月  28 13:23 apache-maven-3.6.3-bin.tar.gz</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu  8614663 3月  28 13:23 cmake-3.17.0.tar.gz</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 29800905 3月  28 13:23 hadoop-3.1.3-src.tar.gz</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu  2401901 3月  28 13:23 protobuf-2.5.0.tar.gz</span><br></pre></td></tr></table></figure>

<p>*<em>2</em>***）解压软件包指定的目录，例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop_source]$ tar -zxvf apache-maven-3.6.3-bin.tar.gz -C  /opt/module/hadoop_source/</span><br><span class="line"></span><br><span class="line">[root@hadoop101 hadoop_source]$ tar -zxvf cmake-3.17.0.tar.gz -C  /opt/module/hadoop_source/</span><br><span class="line"></span><br><span class="line">[root@hadoop101 hadoop_source]$ tar -zxvf hadoop-3.1.3-src.tar.gz -C  /opt/module/hadoop_source/</span><br><span class="line"></span><br><span class="line">[root@hadoop101 hadoop_source]$ tar -zxvf protobuf-2.5.0.tar.gz -C  /opt/module/hadoop_source/</span><br><span class="line"></span><br><span class="line">[root@hadoop101 hadoop_source]$ pwd</span><br><span class="line">/opt/module/hadoop_source</span><br><span class="line"></span><br><span class="line">[root@hadoop101 hadoop_source]$ ll</span><br><span class="line">总用量 20</span><br><span class="line">drwxrwxr-x.  6 atguigu atguigu 4096 3月  28 13:25 apache-maven-3.6.3</span><br><span class="line">drwxr-xr-x. 15 root    root    4096 3月  28 13:43 cmake-3.17.0</span><br><span class="line">drwxr-xr-x. 18 atguigu atguigu 4096 9月  12 2019 hadoop-3.1.3-src</span><br><span class="line">drwxr-xr-x. 10 atguigu atguigu 4096 3月  28 13:44 protobuf-2.5.0</span><br></pre></td></tr></table></figure>

<p><strong>3）安装JDK</strong></p>
<p>（1）解压JDK</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop_source]# tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/hadoop_source/</span><br></pre></td></tr></table></figure>

<p>（2）配置环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 jdk1.8.0_212]# vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>输入如下内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#JAVA_HOME</span><br><span class="line"></span><br><span class="line">export JAVA_HOME=/opt/module/hadoop_source/jdk1.8.0_212</span><br><span class="line"></span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure>

<p>（3）刷新JDK环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 jdk1.8.0_212]# source /etc/profile</span><br></pre></td></tr></table></figure>

<p>（4）验证JDK是否安装成功</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop_source]$ java -version</span><br><span class="line"></span><br><span class="line">java version &quot;1.8.0_212&quot;</span><br><span class="line"></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_212-b10)</span><br><span class="line"></span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.212-b10, mixed mode)</span><br></pre></td></tr></table></figure>

<p><strong>4）配置maven环境变量，maven镜像，并验证</strong></p>
<p>（1）配置maven的环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop_source]#  vim /etc/profile.d/my_env.sh</span><br><span class="line"></span><br><span class="line">#MAVEN_HOME</span><br><span class="line">MAVEN_HOME=/opt/module/hadoop_source/apache-maven-3.6.3</span><br><span class="line">PATH=$PATH:$JAVA_HOME/bin:$MAVEN_HOME/bin</span><br><span class="line"></span><br><span class="line">[root@hadoop101 hadoop_source]#  source /etc/profile</span><br></pre></td></tr></table></figure>

<p>（2）修改maven的镜像</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 apache-maven-3.6.3]# vi conf/settings.xml</span><br><span class="line"></span><br><span class="line"># 在 mirrors节点中添加阿里云镜像</span><br><span class="line">&lt;mirrors&gt;</span><br><span class="line">    &lt;mirror&gt;</span><br><span class="line">         &lt;id&gt;nexus-aliyun&lt;/id&gt;</span><br><span class="line">         &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;</span><br><span class="line">         &lt;name&gt;Nexus aliyun&lt;/name&gt;</span><br><span class="line">                &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;</span><br><span class="line">    &lt;/mirror&gt;</span><br><span class="line">&lt;/mirrors&gt;</span><br></pre></td></tr></table></figure>

<p>（3）验证maven安装是否成功</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop_source]# mvn -version </span><br><span class="line">Apache Maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f)</span><br><span class="line">Maven home: /opt/module/hadoop_source/apache-maven-3.6.3</span><br><span class="line">Java version: 1.8.0_212, vendor: Oracle Corporation, runtime: /opt/module/hadoop_source/jdk1.8.0_212/jre</span><br><span class="line">Default locale: zh_CN, platform encoding: UTF-8</span><br><span class="line">OS name: &quot;linux&quot;, version: &quot;3.10.0-862.el7.x86_64&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot;</span><br></pre></td></tr></table></figure>

<p><strong>5）安装相关的依赖(注意安装顺序不可乱，可能会出现依赖找不到问题)</strong></p>
<p>（1）安装gcc make</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop_source]# yum install -y gcc* make</span><br></pre></td></tr></table></figure>

<p>（2）安装压缩工具</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop_source]# yum -y install snappy* bzip2* lzo* zlib* lz4* gzip*</span><br></pre></td></tr></table></figure>

<p>（3）安装一些基本工具</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop_source]# yum -y install openssl* svn ncurses* autoconf automake libtool</span><br></pre></td></tr></table></figure>

<p>（4）安装扩展源，才可安装zstd</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop_source]# yum -y install epel-release</span><br></pre></td></tr></table></figure>

<p>（5）安装zstd</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop_source]# yum -y install *zstd*</span><br></pre></td></tr></table></figure>

<p><strong>6）手动安装cmake</strong></p>
<p>（1）在解压好的cmake目录下，执行./bootstrap进行编译，此过程需一小时请耐心等待</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 cmake-3.17.0]$ pwd</span><br><span class="line">/opt/module/hadoop_source/cmake-3.17.0</span><br><span class="line">[atguigu@hadoop101 cmake-3.17.0]$ ./bootstrap</span><br></pre></td></tr></table></figure>

<p>（2）执行安装</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 cmake-3.17.0]$ make &amp;&amp; make install</span><br></pre></td></tr></table></figure>

<p>（3）验证安装是否成功</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 cmake-3.17.0]$ cmake -version</span><br><span class="line">cmake version 3.17.0</span><br><span class="line">CMake suite maintained and supported by Kitware (kitware.com/cmake).</span><br></pre></td></tr></table></figure>

<p><strong>7）安装protobuf，进入到解压后的protobuf目录</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 protobuf-2.5.0]$ pwd</span><br><span class="line"></span><br><span class="line">/opt/module/hadoop_source/protobuf-2.5.0</span><br></pre></td></tr></table></figure>

<p>（1）依次执行下列命令 –prefix 指定安装到当前目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 protobuf-2.5.0]$ ./configure --prefix=/opt/module/hadoop_source/protobuf-2.5.0 </span><br><span class="line"></span><br><span class="line">[root@hadoop101 protobuf-2.5.0]$ make &amp;&amp; make install</span><br></pre></td></tr></table></figure>

<p>（2）配置环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 protobuf-2.5.0]$ vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>输入如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PROTOC_HOME=/opt/module/hadoop_source/protobuf-2.5.0</span><br><span class="line"></span><br><span class="line">PATH=$PATH:$JAVA_HOME/bin:$MAVEN_HOME/bin:$PROTOC_HOME/bin</span><br></pre></td></tr></table></figure>

<p>（3）验证</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 protobuf-2.5.0]$ source /etc/profile</span><br><span class="line"></span><br><span class="line">[root@hadoop101 protobuf-2.5.0]$ protoc --version</span><br><span class="line"></span><br><span class="line">libprotoc 2.5.0</span><br></pre></td></tr></table></figure>

<p><strong>8）到此，软件包安装配置工作完成。</strong></p>
<h2 id="6-3-编译源码"><a href="#6-3-编译源码" class="headerlink" title="6.3 编译源码"></a>6.3 编译源码</h2><p><strong>1）进入解压后的Hadoop源码目录下</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop-3.1.3-src]$ pwd</span><br><span class="line">/opt/module/hadoop_source/hadoop-3.1.3-src</span><br><span class="line"></span><br><span class="line">#开始编译</span><br><span class="line">[root@hadoop101 hadoop-3.1.3-src]$ mvn clean package -DskipTests -Pdist,native -Dtar</span><br></pre></td></tr></table></figure>

<p>注意：第一次编译需要下载很多依赖jar包，编译时间会很久，预计1小时左右，最终成功是全部SUCCESS，爽!!!</p>
<p>     <img src="https://image.3001.net/images/20221031/16671926785130.png#alt=image-20221031130432412"></p>
<p><strong>2）成功的64位hadoop包在/opt/module/hadoop_source/hadoop-3.1.3-src/hadoop-dist/target下</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 target]# pwd</span><br><span class="line"></span><br><span class="line">/opt/module/hadoop_source/hadoop-3.1.3-src/hadoop-dist/target</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="希文"
      src="/images/%E5%A4%B4%E5%83%8F.gif">
  <p class="site-author-name" itemprop="name">希文</p>
  <div class="site-description" itemprop="description">日常笔记</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">49</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yangmour" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yangmour" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:xiwenya999@gmail.com" title="E-Mail → mailto:xiwenya999@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://poc10.cn/" title="http:&#x2F;&#x2F;poc10.cn&#x2F;" rel="noopener" target="_blank">网络安全方面</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">希文</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">804k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">12:11</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
