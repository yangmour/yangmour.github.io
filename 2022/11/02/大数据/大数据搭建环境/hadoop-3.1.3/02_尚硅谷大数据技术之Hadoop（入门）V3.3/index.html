<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yangmour.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="[TOC] 尚硅谷大数据技术之Hadoop（入门） （作者：尚硅谷大数据研发部） 版本：V3.3 第1章 Hadoop概述1.1 Hadoop是什么                                1.2 Hadoop发展历史（了解）  1.3 Hadoop三大发行版本（了解）Hadoop三大发行版本：Apache、Cloudera、Hortonworks。 Apache版本最原始（">
<meta property="og:type" content="article">
<meta property="og:title" content="02_尚硅谷大数据技术之Hadoop（入门）">
<meta property="og:url" content="https://yangmour.github.io/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/02_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88%E5%85%A5%E9%97%A8%EF%BC%89V3.3/index.html">
<meta property="og:site_name" content="希文的个人博客">
<meta property="og:description" content="[TOC] 尚硅谷大数据技术之Hadoop（入门） （作者：尚硅谷大数据研发部） 版本：V3.3 第1章 Hadoop概述1.1 Hadoop是什么                                1.2 Hadoop发展历史（了解）  1.3 Hadoop三大发行版本（了解）Hadoop三大发行版本：Apache、Cloudera、Hortonworks。 Apache版本最原始（">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://image.3001.net/images/20221031/1667194025732.png#crop=0&crop=0&crop=1&crop=1&id=j5iH0&originHeight=885&originWidth=1598&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221031/1667194037334.png#crop=0&crop=0&crop=1&crop=1&id=evRgg&originHeight=888&originWidth=1545&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221031/16671940571077.png#crop=0&crop=0&crop=1&crop=1&id=PXRXd&originHeight=887&originWidth=1579&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221031/16671941257461.png#crop=0&crop=0&crop=1&crop=1&id=jfarw&originHeight=680&originWidth=1483&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221031/16671941366585.png#crop=0&crop=0&crop=1&crop=1&id=tHCdw&originHeight=342&originWidth=1502&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221031/16671943599655.png#crop=0&crop=0&crop=1&crop=1&id=e0K1k&originHeight=857&originWidth=1561&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221031/16671943728508.png#crop=0&crop=0&crop=1&crop=1&id=EiMSY&originHeight=894&originWidth=1579&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221031/16671943927639.png#crop=0&crop=0&crop=1&crop=1&id=aqNTg&originHeight=768&originWidth=1576&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221031/1667194405929.png#crop=0&crop=0&crop=1&crop=1&id=cBR9g&originHeight=854&originWidth=1577&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221031/1667194420824.png#crop=0&crop=0&crop=1&crop=1&id=pSzkM&originHeight=861&originWidth=1564&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221031/16671944389897.png#crop=0&crop=0&crop=1&crop=1&id=xieVc&originHeight=864&originWidth=1602&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221031/16671944656951.png#crop=0&crop=0&crop=1&crop=1&id=OHgg4&originHeight=803&originWidth=1478&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221031/16671944773962.png#crop=0&crop=0&crop=1&crop=1&id=VtmhW&originHeight=792&originWidth=1432&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221031/16671945032468.png#crop=0&crop=0&crop=1&crop=1&id=KxPbX&originHeight=796&originWidth=1457&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221031/16671951899900.png#crop=0&crop=0&crop=1&crop=1&id=HSsXD&originHeight=458&originWidth=527&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221031/16671952029218.png#crop=0&crop=0&crop=1&crop=1&id=N4xIT&originHeight=485&originWidth=457&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221031/16671952098350.png#crop=0&crop=0&crop=1&crop=1&id=P3sFx&originHeight=581&originWidth=461&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221031/1667195379816.png#crop=0&crop=0&crop=1&crop=1&id=dEdTT&originHeight=845&originWidth=1234&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221104/16675228951696.png#crop=0&crop=0&crop=1&crop=1&id=K6Rig&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221104/16675258082005.png#crop=0&crop=0&crop=1&crop=1&id=zfcux&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221104/16675324111724.png#crop=0&crop=0&crop=1&crop=1&id=UW5mV&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221104/16675325708222.png#crop=0&crop=0&crop=1&crop=1&id=L7UIu&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221104/16675325798812.png#crop=0&crop=0&crop=1&crop=1&id=rnxrT&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221104/16675325889825.png#crop=0&crop=0&crop=1&crop=1&id=ZpUju&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="og:image" content="https://image.3001.net/images/20221104/16675333856288.png#crop=0&crop=0&crop=1&crop=1&id=izPch&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">
<meta property="article:published_time" content="2022-11-02T02:06:17.000Z">
<meta property="article:modified_time" content="2022-11-10T09:32:34.978Z">
<meta property="article:author" content="希文">
<meta property="article:tag" content="hadoop">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://image.3001.net/images/20221031/1667194025732.png#crop=0&crop=0&crop=1&crop=1&id=j5iH0&originHeight=885&originWidth=1598&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=">

<link rel="canonical" href="https://yangmour.github.io/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/02_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88%E5%85%A5%E9%97%A8%EF%BC%89V3.3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>02_尚硅谷大数据技术之Hadoop（入门） | 希文的个人博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="希文的个人博客" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">希文的个人博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">白日依山尽，黄河入海流。欲穷千里目，更上一层楼。</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签<span class="badge">15</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类<span class="badge">0</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档<span class="badge">49</span></a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="heartbeat fa-fw"></i>公益 404</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yangmour.github.io/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/02_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88%E5%85%A5%E9%97%A8%EF%BC%89V3.3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E5%A4%B4%E5%83%8F.gif">
      <meta itemprop="name" content="希文">
      <meta itemprop="description" content="日常笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="希文的个人博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          02_尚硅谷大数据技术之Hadoop（入门）
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-11-02 10:06:17" itemprop="dateCreated datePublished" datetime="2022-11-02T10:06:17+08:00">2022-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-10 17:32:34" itemprop="dateModified" datetime="2022-11-10T17:32:34+08:00">2022-11-10</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>25k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>23 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>[TOC]</p>
<p>尚硅谷大数据技术之Hadoop（入门）</p>
<p>（作者：尚硅谷大数据研发部）</p>
<p>版本：V3.3</p>
<h1 id="第1章-Hadoop概述"><a href="#第1章-Hadoop概述" class="headerlink" title="第1章 Hadoop概述"></a>第1章 Hadoop概述</h1><h2 id="1-1-Hadoop是什么"><a href="#1-1-Hadoop是什么" class="headerlink" title="1.1 Hadoop是什么"></a>1.1 Hadoop是什么</h2><p>                               <img src="https://image.3001.net/images/20221031/1667194025732.png#crop=0&crop=0&crop=1&crop=1&id=j5iH0&originHeight=885&originWidth=1598&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h2 id="1-2-Hadoop发展历史（了解）"><a href="#1-2-Hadoop发展历史（了解）" class="headerlink" title="1.2 Hadoop发展历史（了解）"></a>1.2 Hadoop发展历史（了解）</h2><p><img src="https://image.3001.net/images/20221031/1667194037334.png#crop=0&crop=0&crop=1&crop=1&id=evRgg&originHeight=888&originWidth=1545&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p><img src="https://image.3001.net/images/20221031/16671940571077.png#crop=0&crop=0&crop=1&crop=1&id=PXRXd&originHeight=887&originWidth=1579&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h2 id="1-3-Hadoop三大发行版本（了解）"><a href="#1-3-Hadoop三大发行版本（了解）" class="headerlink" title="1.3 Hadoop三大发行版本（了解）"></a>1.3 Hadoop三大发行版本（了解）</h2><p>Hadoop三大发行版本：Apache、Cloudera、Hortonworks。</p>
<p>Apache版本最原始（最基础）的版本，对于入门学习最好。2006</p>
<p>Cloudera内部集成了很多大数据框架，对应产品CDH。2008</p>
<p>Hortonworks文档较好，对应产品HDP。2011</p>
<p>Hortonworks现在已经被Cloudera公司收购，推出新的品牌CDP。</p>
<p><img src="https://image.3001.net/images/20221031/16671941257461.png#crop=0&crop=0&crop=1&crop=1&id=jfarw&originHeight=680&originWidth=1483&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p><img src="https://image.3001.net/images/20221031/16671941366585.png#crop=0&crop=0&crop=1&crop=1&id=tHCdw&originHeight=342&originWidth=1502&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p><strong>1</strong>）<strong>Apache Hadoop</strong></p>
<p>官网地址：<a target="_blank" rel="noopener" href="http://hadoop.apache.org/">http://hadoop.apache.org</a></p>
<p>下载地址：<a target="_blank" rel="noopener" href="https://hadoop.apache.org/releases.html">https://hadoop.apache.org/releases.html</a></p>
<p><strong>2</strong>）<strong>Cloudera Hadoop</strong></p>
<p>官网地址：<a target="_blank" rel="noopener" href="https://www.cloudera.com/downloads/cdh">https://www.cloudera.com/downloads/cdh</a></p>
<p>下载地址：<a target="_blank" rel="noopener" href="https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_cdh_6_download.html">https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_cdh_6_download.html</a></p>
<p>（1）2008年成立的Cloudera是最早将Hadoop商用的公司，为合作伙伴提供Hadoop的商用解决方案，主要是包括支持、咨询服务、培训。</p>
<p><strong>（2）</strong> <strong>2009</strong> 年<strong>Hadoop</strong>的创始人<strong>Doug Cutting</strong>也加盟<strong>Cloudera</strong>公司。Cloudera产品主要为CDH，Cloudera Manager，Cloudera Support</p>
<p>（3）CDH是Cloudera的Hadoop发行版，完全开源，比Apache Hadoop在兼容性，安全性，稳定性上有所增强。Cloudera的标价为每年每个节点<strong>10000</strong>美元。</p>
<p>（4）Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。</p>
<p><strong>3</strong>）<strong>Hortonworks Hadoop</strong></p>
<p>官网地址：<a target="_blank" rel="noopener" href="https://hortonworks.com/products/data-center/hdp/">https://hortonworks.com/products/data-center/hdp/</a></p>
<p>下载地址：<a target="_blank" rel="noopener" href="https://hortonworks.com/downloads/#data-platform">https://hortonworks.com/downloads/#data-platform</a></p>
<p>（1）2011年成立的Hortonworks是雅虎与硅谷风投公司Benchmark Capital合资组建。</p>
<p><strong>（2）</strong>公司成立之初就吸纳了大约<strong>25</strong>名至<strong>30</strong>名专门研究<strong>Hadoop</strong>的雅虎工程师，上述工程师均在<strong>2005</strong>年开始协助雅虎开发<strong>Hadoop</strong>，贡献了**Hadoop80%**的代码。</p>
<p>（3）Hortonworks的主打产品是Hortonworks Data Platform（HDP），也同样是100%开源的产品，HDP除常见的项目外还包括了<strong>Ambari</strong>，一款开源的安装和管理系统。</p>
<p>（4）2018年Hortonworks目前已经被<strong>Cloudera</strong>公司收购。</p>
<h2 id="1-4-Hadoop优势（4高）"><a href="#1-4-Hadoop优势（4高）" class="headerlink" title="1.4 Hadoop优势（4高）"></a>1.4 Hadoop优势（4高）</h2><p><img src="https://image.3001.net/images/20221031/16671943599655.png#crop=0&crop=0&crop=1&crop=1&id=e0K1k&originHeight=857&originWidth=1561&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p><img src="https://image.3001.net/images/20221031/16671943728508.png#crop=0&crop=0&crop=1&crop=1&id=EiMSY&originHeight=894&originWidth=1579&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h2 id="1-5-Hadoop组成（面试重点）"><a href="#1-5-Hadoop组成（面试重点）" class="headerlink" title="1.5 Hadoop组成（面试重点）"></a>1.5 Hadoop组成（面试重点）</h2><p><img src="https://image.3001.net/images/20221031/16671943927639.png#crop=0&crop=0&crop=1&crop=1&id=aqNTg&originHeight=768&originWidth=1576&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h3 id="1-5-1-HDFS架构概述"><a href="#1-5-1-HDFS架构概述" class="headerlink" title="1.5.1 HDFS架构概述"></a>1.5.1 HDFS架构概述</h3><p>Hadoop Distributed File System，简称_HDFS_，是一个分布式文件系统。</p>
<p><img src="https://image.3001.net/images/20221031/1667194405929.png#crop=0&crop=0&crop=1&crop=1&id=cBR9g&originHeight=854&originWidth=1577&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h3 id="1-5-2-YARN架构概述"><a href="#1-5-2-YARN架构概述" class="headerlink" title="1.5.2 YARN架构概述"></a>1.5.2 YARN架构概述</h3><p>Yet Another Resource Negotiator简称YARN ，另一种资源协调者，是Hadoop的资源管理器。</p>
<p><img src="https://image.3001.net/images/20221031/1667194420824.png#crop=0&crop=0&crop=1&crop=1&id=pSzkM&originHeight=861&originWidth=1564&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h3 id="1-5-3-MapReduce架构概述"><a href="#1-5-3-MapReduce架构概述" class="headerlink" title="1.5.3 MapReduce架构概述"></a>1.5.3 MapReduce架构概述</h3><p>MapReduce将计算过程分为两个阶段：Map和Reduce</p>
<p>1）Map阶段并行处理输入数据</p>
<p>2）Reduce阶段对Map结果进行汇总</p>
<p><img src="https://image.3001.net/images/20221031/16671944389897.png#crop=0&crop=0&crop=1&crop=1&id=xieVc&originHeight=864&originWidth=1602&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h3 id="1-5-4-HDFS、YARN、MapReduce三者关系"><a href="#1-5-4-HDFS、YARN、MapReduce三者关系" class="headerlink" title="1.5.4 HDFS、YARN、MapReduce三者关系"></a>1.5.4 HDFS、YARN、MapReduce三者关系</h3><p><img src="https://image.3001.net/images/20221031/16671944656951.png#crop=0&crop=0&crop=1&crop=1&id=OHgg4&originHeight=803&originWidth=1478&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h2 id="1-6-大数据技术生态体系"><a href="#1-6-大数据技术生态体系" class="headerlink" title="1.6 大数据技术生态体系"></a>1.6 大数据技术生态体系</h2><p><img src="https://image.3001.net/images/20221031/16671944773962.png#crop=0&crop=0&crop=1&crop=1&id=VtmhW&originHeight=792&originWidth=1432&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>图中涉及的技术名词解释如下：</p>
<p>1）Sqoop：Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库（MySQL）间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。</p>
<p>2）Flume：Flume是一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；</p>
<p>3）Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统；</p>
<p>4）Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。</p>
<p>5）Flink：Flink是当前最流行的开源大数据内存计算框架。用于实时计算的场景较多。</p>
<p>6）Oozie：Oozie是一个管理Hadoop作业（job）的工作流程调度管理系统。</p>
<p>7）Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。</p>
<p>8）Hive：Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</p>
<p>9）ZooKeeper：它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。</p>
<h2 id="1-7-推荐系统框架图"><a href="#1-7-推荐系统框架图" class="headerlink" title="1.7 推荐系统框架图"></a>1.7 推荐系统框架图</h2><p><img src="https://image.3001.net/images/20221031/16671945032468.png#crop=0&crop=0&crop=1&crop=1&id=KxPbX&originHeight=796&originWidth=1457&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h1 id="第2章-Hadoop运行环境搭建（开发重点）"><a href="#第2章-Hadoop运行环境搭建（开发重点）" class="headerlink" title="第2章 Hadoop运行环境搭建（开发重点）"></a>第2章 Hadoop运行环境搭建（开发重点）</h1><h2 id="2-1-模板虚拟机环境准备"><a href="#2-1-模板虚拟机环境准备" class="headerlink" title="2.1 模板虚拟机环境准备"></a>2.1 模板虚拟机环境准备</h2><p><strong>0）安装模板虚拟机，</strong>IP<strong>地址</strong>192.168.10.100<strong>、主机名称</strong>hadoop100<strong>、内存</strong>4G<strong>、硬盘</strong>50G**</p>
<p>02.1_尚硅谷大数据技术之模板虚拟机环境准备</p>
<p><strong>1</strong>）<strong>hadoop100</strong>虚拟机配置要求如下（本文<strong>Linux</strong>系统全部以<strong>CentOS-7.5-x86-1804</strong>为例）</p>
<p>（1）使用yum安装需要虚拟机可以正常上网，yum安装前可以先测试下虚拟机联网情况</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# ping www.baidu.com</span><br><span class="line">PING www.baidu.com (14.215.177.39) 56(84) bytes of data.</span><br><span class="line">64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=1 ttl=128 time=8.60 ms</span><br><span class="line">64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=2 ttl=128 time=7.72 ms</span><br></pre></td></tr></table></figure>

<p>（2）安装epel-release</p>
<p>注：Extra Packages for Enterprise Linux是为“红帽系”的操作系统提供额外的软件包，适用于RHEL、CentOS和Scientific Linux。相当于是一个软件仓库，大多数rpm包在官方 repository 中是找不到的）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# yum install -y epel-release</span><br></pre></td></tr></table></figure>

<p>（3）注意：如果Linux安装的是最小系统版，还需要安装如下工具；如果安装的是Linux桌面标准版，不需要执行如下操作</p>
<p>Ø net-tool：工具包集合，包含ifconfig等命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# yum install -y net-tools</span><br></pre></td></tr></table></figure>

<p>Ø vim：编辑器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# yum install -y vim</span><br></pre></td></tr></table></figure>

<p><strong>2</strong>）关闭防火墙，关闭防火墙开机自启</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# systemctl stop firewalld</span><br><span class="line">[root@hadoop100 ~]# systemctl disable firewalld.service</span><br></pre></td></tr></table></figure>

<p>    注意：在企业开发时，通常单个服务器的防火墙时关闭的。公司整体对外会设置非常安全的防火墙</p>
<p><strong>3）创建atguigu用户，并修改atguigu用户的密码</strong></p>
<p>[root<a href="/hadoop100">@hadoop100 </a> ~]# useradd atguigu </p>
<p>[root<a href="/hadoop100">@hadoop100 </a> ~]# passwd atguigu </p>
<p><strong>4</strong>）配置<strong>atguigu</strong>用户具有<strong>root</strong>权限，方便后期加<strong>sudo</strong>执行<strong>root</strong>权限的命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# vim /etc/sudoers</span><br></pre></td></tr></table></figure>

<p>修改/etc/sudoers文件，在%wheel这行下面添加一行，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">## Allow root to run any commands anywhere</span><br><span class="line">root    ALL=(ALL)     ALL</span><br><span class="line"></span><br><span class="line">## Allows people in group wheel to run all commands</span><br><span class="line">%wheel  ALL=(ALL)       ALL</span><br><span class="line">atguigu   ALL=(ALL)     NOPASSWD:ALL</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>：atguigu这一行不要直接放到root行下面，因为所有用户都属于wheel组，你先配置了atguigu具有免密功能，但是程序执行到%wheel行时，该功能又被覆盖回需要密码。所以atguigu要放到%wheel这行下面。</p>
<p><strong>5</strong>）在**/opt<strong>目录下创建文件夹，并修改所属主和所属组</strong></p>
<p>（1）在/opt目录下创建module、software文件夹</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# mkdir /opt/module</span><br><span class="line">[root@hadoop100 ~]# mkdir /opt/software</span><br></pre></td></tr></table></figure>

<p>    （2）修改module、software文件夹的所有者和所属组均为atguigu用户</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# chown atguigu:atguigu /opt/module </span><br><span class="line">[root@hadoop100 ~]# chown atguigu:atguigu /opt/software</span><br></pre></td></tr></table></figure>

<p>（3）查看module、software文件夹的所有者和所属组</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# cd /opt/</span><br><span class="line">[root@hadoop100 opt]# ll</span><br><span class="line">总用量 12</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu 4096 5月 28 17:18 module</span><br><span class="line">drwxr-xr-x. 2 root  root  4096 9月  7 2017 rh</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu 4096 5月 28 17:18 software</span><br></pre></td></tr></table></figure>

<p><strong>6</strong>）卸载虚拟机自带的JDK</p>
<p>    注意：如果你的虚拟机是最小化安装不需要执行这一步。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# rpm -qa | grep -i java | xargs -n1 rpm -e --nodeps</span><br></pre></td></tr></table></figure>

<p>Ø rpm -qa：查询所安装的所有rpm软件包</p>
<p>Ø grep -i：忽略大小写</p>
<p>Ø xargs -n1：表示每次只传递一个参数</p>
<p>Ø rpm -e –nodeps：强制卸载软件</p>
<p><strong>7</strong>）重启虚拟机</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# reboot</span><br></pre></td></tr></table></figure>

<h2 id="2-2-克隆虚拟机"><a href="#2-2-克隆虚拟机" class="headerlink" title="2.2 克隆虚拟机"></a>2.2 克隆虚拟机</h2><p><strong>1</strong>）利用模板机hadoop100**，克隆三台虚拟机：hadoop102 hadoop103 hadoop104</p>
<p>    注意：克隆时，要先关闭hadoop100</p>
<p><strong>2）修改克隆机IP，以下以hadoop102举例说明</strong></p>
<p>（1）修改克隆虚拟机的静态IP</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br></pre></td></tr></table></figure>

<p>改成</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=ens33</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">ONBOOT=yes</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">NAME=&quot;ens33&quot;</span><br><span class="line">IPADDR=192.168.10.102</span><br><span class="line">PREFIX=24</span><br><span class="line">GATEWAY=192.168.10.2</span><br><span class="line">DNS1=192.168.10.2</span><br></pre></td></tr></table></figure>

<p>（2）查看Linux虚拟机的虚拟网络编辑器，编辑-&gt;虚拟网络编辑器-&gt;VMnet8</p>
<p>     <img src="https://image.3001.net/images/20221031/16671951899900.png#crop=0&crop=0&crop=1&crop=1&id=HSsXD&originHeight=458&originWidth=527&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p><img src="https://image.3001.net/images/20221031/16671952029218.png#crop=0&crop=0&crop=1&crop=1&id=N4xIT&originHeight=485&originWidth=457&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>（3）查看Windows系统适配器VMware Network Adapter VMnet8的IP地址</p>
<p>     <img src="https://image.3001.net/images/20221031/16671952098350.png#crop=0&crop=0&crop=1&crop=1&id=P3sFx&originHeight=581&originWidth=461&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>（4）保证Linux系统ifcfg-ens33文件中IP地址、虚拟网络编辑器地址和Windows系统VM8网络IP地址相同。</p>
<p><strong>3）修改克隆机主机名，以下以hadoop102举例说明</strong></p>
<p>    （1）修改主机名称</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# vim /etc/hostname</span><br><span class="line">hadoop102</span><br></pre></td></tr></table></figure>

<p>（2）配置Linux克隆机主机名称映射hosts文件，打开/etc/hosts</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# vim /etc/hosts</span><br></pre></td></tr></table></figure>

<p>添加如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">192.168.10.100 hadoop100</span><br><span class="line">192.168.10.101 hadoop101</span><br><span class="line">192.168.10.102 hadoop102</span><br><span class="line">192.168.10.103 hadoop103</span><br><span class="line">192.168.10.104 hadoop104</span><br><span class="line">192.168.10.105 hadoop105</span><br><span class="line">192.168.10.106 hadoop106</span><br><span class="line">192.168.10.107 hadoop107</span><br><span class="line">192.168.10.108 hadoop108</span><br></pre></td></tr></table></figure>

<p><strong>4</strong>）重启克隆机hadoop102</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# reboot</span><br></pre></td></tr></table></figure>

<p><strong>5</strong>）修改<strong>windows</strong>的主机映射文件（<strong>hosts</strong>文件）</p>
<p>（1）如果操作系统是window7，可以直接修改</p>
<p>    （a）进入<strong>C:\Windows\System32\drivers\etc</strong>路径</p>
<p>    （b）打开hosts文件并添加如下内容，然后保存</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">192.168.10.100 hadoop100</span><br><span class="line">192.168.10.101 hadoop101</span><br><span class="line">192.168.10.102 hadoop102</span><br><span class="line">192.168.10.103 hadoop103</span><br><span class="line">192.168.10.104 hadoop104</span><br><span class="line">192.168.10.105 hadoop105</span><br><span class="line">192.168.10.106 hadoop106</span><br><span class="line">192.168.10.107 hadoop107</span><br><span class="line">192.168.10.108 hadoop108</span><br></pre></td></tr></table></figure>

<p>（2）如果操作系统是window10，先拷贝出来，修改保存以后，再覆盖即可</p>
<p>（a）进入<strong>C:\Windows\System32\drivers\etc</strong>路径</p>
<p>（b）拷贝hosts文件到桌面</p>
<p>（c）打开桌面hosts文件并添加如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">192.168.10.100 hadoop100</span><br><span class="line">192.168.10.101 hadoop101</span><br><span class="line">192.168.10.102 hadoop102</span><br><span class="line">192.168.10.103 hadoop103</span><br><span class="line">192.168.10.104 hadoop104</span><br><span class="line">192.168.10.105 hadoop105</span><br><span class="line">192.168.10.106 hadoop106</span><br><span class="line">192.168.10.107 hadoop107</span><br><span class="line">192.168.10.108 hadoop108</span><br></pre></td></tr></table></figure>

<p>（d）将桌面hosts文件覆盖<strong>C:\Windows\System32\drivers\etc</strong>路径hosts文件</p>
<h2 id="2-3-在hadoop102安装JDK"><a href="#2-3-在hadoop102安装JDK" class="headerlink" title="2.3 在hadoop102安装JDK"></a>2.3 在hadoop102安装JDK</h2><p><strong>1</strong>）卸载现有<strong>JDK</strong></p>
<p>注意：安装JDK前，一定确保提前删除了虚拟机自带的JDK。详细步骤见问文档3.1节中卸载JDK步骤。</p>
<p><strong>2</strong>）用XShell传输工具将JDK导入到op目录下面的software文件夹下面**</p>
<p>     <img src="https://image.3001.net/images/20221031/1667195379816.png#crop=0&crop=0&crop=1&crop=1&id=dEdTT&originHeight=845&originWidth=1234&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p><strong>3</strong>）在Linux系统下的opt目录中查看软件包是否导入成功</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ ls /opt/software/</span><br></pre></td></tr></table></figure>

<p>看到如下结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jdk-8u212-linux-x64.tar.gz</span><br></pre></td></tr></table></figure>

<p><strong>4</strong>）解压JDK到/opt/module目录下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p><strong>5</strong>）配置JDK环境变量</p>
<p>    （1）新建/etc/profile.d/my_env.sh文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>添加如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#JAVA_HOME</span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure>

<p>    （2）保存后退出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:wq</span><br></pre></td></tr></table></figure>

<p>    （3）source一下/etc/profile文件，让新的环境变量PATH生效</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ source /etc/profile</span><br></pre></td></tr></table></figure>

<p><strong>6）测试JDK是否安装成功</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ java -version</span><br></pre></td></tr></table></figure>

<p>如果能看到以下结果，则代表Java安装成功。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java version &quot;1.8.0_212&quot;</span><br></pre></td></tr></table></figure>

<p>注意：重启（如果java -version可以用就不用重启）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo reboot</span><br></pre></td></tr></table></figure>

<h2 id="2-4-在hadoop102安装Hadoop"><a href="#2-4-在hadoop102安装Hadoop" class="headerlink" title="2.4 在hadoop102安装Hadoop"></a>2.4 在hadoop102安装Hadoop</h2><p>Hadoop下载地址：<a target="_blank" rel="noopener" href="https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/">https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/</a></p>
<p><strong>1</strong>）用XShell文件传输工具将hadoop-3.1.3.tar.gz导入到opt目录下面的software文件夹下面</p>
<p>     <img src="https://image.3001.net/images/20221104/16675228951696.png#crop=0&crop=0&crop=1&crop=1&id=K6Rig&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p><strong>2</strong>）进入到Hadoop安装包路径下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ cd /opt/software/</span><br></pre></td></tr></table></figure>

<p><strong>3</strong>）解压安装文件到/opt/module下面</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p><strong>4</strong>）查看是否解压成功</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ ls /opt/module/</span><br><span class="line">hadoop-3.1.3</span><br></pre></td></tr></table></figure>

<p><strong>5</strong>）将Hadoop添加到环境变量</p>
<p>    （1）获取Hadoop安装路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ pwd</span><br><span class="line">/opt/module/hadoop-3.1.3</span><br></pre></td></tr></table></figure>

<p>    （2）打开/etc/profile.d/my_env.sh文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ sudo vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>Ø 在my_env.sh文件末尾添加如下内容：（shift+g）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#HADOOP_HOME</span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<p>Ø 保存并退出： :wq</p>
<p>    （3）让修改后的文件生效</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ source /etc/profile</span><br></pre></td></tr></table></figure>

<p><strong>6</strong>）测试是否安装成功</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop version</span><br><span class="line">Hadoop 3.1.3</span><br></pre></td></tr></table></figure>

<p><strong>7</strong>）重启（如果Hadoop命令不能用再重启虚拟机）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ sudo reboot</span><br></pre></td></tr></table></figure>

<h2 id="2-5-Hadoop目录结构"><a href="#2-5-Hadoop目录结构" class="headerlink" title="2.5 Hadoop目录结构"></a>2.5 Hadoop目录结构</h2><p><strong>1</strong>）查看Hadoop目录结构</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ ll</span><br><span class="line">总用量 52</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 bin</span><br><span class="line">drwxr-xr-x. 3 atguigu atguigu 4096 5月 22 2017 etc</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 include</span><br><span class="line">drwxr-xr-x. 3 atguigu atguigu 4096 5月 22 2017 lib</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 libexec</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu 15429 5月 22 2017 LICENSE.txt</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu  101 5月 22 2017 NOTICE.txt</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu 1366 5月 22 2017 README.txt</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 sbin</span><br><span class="line">drwxr-xr-x. 4 atguigu atguigu 4096 5月 22 2017 share</span><br></pre></td></tr></table></figure>

<p><strong>2</strong>）重要目录</p>
<p>（1）bin目录：存放对Hadoop相关服务（hdfs，yarn，mapred）进行操作的脚本</p>
<p>（2）etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件</p>
<p>（3）lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能）</p>
<p>（4）sbin目录：存放启动或停止Hadoop相关服务的脚本</p>
<p>（5）share目录：存放Hadoop的依赖jar包、文档、和官方案例</p>
<h1 id="第3章-Hadoop运行模式"><a href="#第3章-Hadoop运行模式" class="headerlink" title="第3章 Hadoop运行模式"></a>第3章 Hadoop运行模式</h1><p>1）Hadoop官方网站：<a target="_blank" rel="noopener" href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></p>
<p>2）Hadoop运行模式包括：<strong>本地模式</strong>、<strong>伪分布式模式</strong>以及<strong>完全分布式模式</strong>。</p>
<p>Ø <strong>本地模式</strong>：单机运行，只是用来演示一下官方案例。生产环境不用。</p>
<p>Ø <strong>伪分布式模式：</strong>也是单机运行，但是具备Hadoop集群的所有功能，一台服务器模拟一个分布式的环境。个别缺钱的公司用来测试，生产环境不用。</p>
<p>Ø <strong>完全分布式模式：</strong>多台服务器组成分布式环境。生产环境使用。</p>
<h2 id="3-1-本地运行模式（官方WordCount）"><a href="#3-1-本地运行模式（官方WordCount）" class="headerlink" title="3.1 本地运行模式（官方WordCount）"></a>3.1 本地运行模式（官方WordCount）</h2><p><strong>1</strong>）创建在hadoop-3.1.3文件下面创建一个wcinput文件夹</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ mkdir wcinput</span><br></pre></td></tr></table></figure>

<p><strong>2</strong>）在wcinput文件下创建一个word.txt文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ cd wcinput</span><br></pre></td></tr></table></figure>

<p><strong>3</strong>）编辑word.txt文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 wcinput]$ vim word.txt</span><br></pre></td></tr></table></figure>

<p>Ø 在文件中输入如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop yarn</span><br><span class="line">hadoop mapreduce</span><br><span class="line">atguigu</span><br><span class="line">atguigu</span><br></pre></td></tr></table></figure>

<p>Ø 保存退出：:wq</p>
<p><strong>4</strong>）回到Hadoop目录/opt/module/hadoop-3.1.3</p>
<p><strong>5</strong>）执行程序</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput wcoutput</span><br></pre></td></tr></table></figure>

<p><strong>6</strong>）查看结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ cat wcoutput/part-r-00000</span><br></pre></td></tr></table></figure>

<p>看到如下结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">atguigu 2</span><br><span class="line">hadoop 2</span><br><span class="line">mapreduce    1</span><br><span class="line">yarn  1</span><br></pre></td></tr></table></figure>

<h2 id="3-2-完全分布式运行模式（开发重点）"><a href="#3-2-完全分布式运行模式（开发重点）" class="headerlink" title="3.2 完全分布式运行模式（开发重点）"></a>3.2 完全分布式运行模式（开发重点）</h2><p>分析：</p>
<p>    1）准备3台客户机（关闭防火墙、静态IP、主机名称）</p>
<p>    2）安装JDK</p>
<p>    3）配置环境变量</p>
<p>    4）安装Hadoop</p>
<p>    5）配置环境变量</p>
<pre><code>6）配置集群
</code></pre>
<p>    7）单点启动</p>
<p>    8）配置ssh</p>
<p>    9）群起并测试集群</p>
<h3 id="3-2-1-虚拟机准备"><a href="#3-2-1-虚拟机准备" class="headerlink" title="3.2.1 虚拟机准备"></a>3.2.1 虚拟机准备</h3><p>详见2.1、2.2两节。</p>
<h3 id="3-2-2-编写集群分发脚本xsync"><a href="#3-2-2-编写集群分发脚本xsync" class="headerlink" title="3.2.2 编写集群分发脚本xsync"></a>3.2.2 编写集群分发脚本xsync</h3><p><strong>1</strong>）scp（secure copy）安全拷贝</p>
<p>（1）scp定义</p>
<p>scp可以实现服务器与服务器之间的数据拷贝。（from server1 to server2）</p>
<p>    （2）基本语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp   -r    $pdir/$fname       $user@$host:$pdir/$fname</span><br></pre></td></tr></table></figure>

<p>命令  递归   要拷贝的文件路径/名称  目的地用户@主机:目的地路径/名称</p>
<p>（3）案例实操</p>
<pre><code>Ø 前提：在hadoop102、hadoop103、hadoop104都已经创建好的/opt/module、      /opt/software两个目录，并且已经把这两个目录修改为atguigu:atguigu
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo chown atguigu:atguigu -R /opt/module</span><br></pre></td></tr></table></figure>

<pre><code>（a）在hadoop102上，将hadoop102中/opt/module/jdk1.8.0_212目录拷贝到hadoop103上。
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ scp -r /opt/module/jdk1.8.0_212 atguigu@hadoop103:/opt/module</span><br></pre></td></tr></table></figure>

<pre><code>（b）在hadoop103上，将hadoop102中/opt/module/hadoop-3.1.3目录拷贝到hadoop103上。
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ scp -r atguigu@hadoop102:/opt/module/hadoop-3.1.3 /opt/module/</span><br></pre></td></tr></table></figure>

<pre><code>（c）在hadoop103上操作，将hadoop102中/opt/module目录下所有目录拷贝到hadoop104上。
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 opt]$ scp -r atguigu@hadoop102:/opt/module/* atguigu@hadoop104:/opt/module</span><br></pre></td></tr></table></figure>

<p><strong>2</strong>）rsync远程同步工具</p>
<p>rsync主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。</p>
<p>rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去。</p>
<p>    （1）基本语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rsync   -av    $pdir/$fname       $user@$host:$pdir/$fname</span><br></pre></td></tr></table></figure>

<p>命令  选项参数  要拷贝的文件路径/名称  目的地用户@主机:目的地路径/名称</p>
<p>     选项参数说明</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>归档拷贝</td>
</tr>
<tr>
<td>-v</td>
<td>显示复制过程</td>
</tr>
</tbody></table>
<p>（2）案例实操</p>
<p>    （a）删除hadoop103中/opt/module/hadoop-3.1.3/wcinput</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ rm -rf wcinput/</span><br></pre></td></tr></table></figure>

<p>    （b）同步hadoop102中的/opt/module/hadoop-3.1.3到hadoop103</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ rsync -av hadoop-3.1.3/ atguigu@hadoop103:/opt/module/hadoop-3.1.3/</span><br></pre></td></tr></table></figure>

<p><strong>3</strong>）xsync集群分发脚本</p>
<p>（1）需求：循环复制文件到所有节点的相同目录下</p>
<p>（2）需求分析：</p>
<p>（a）rsync命令原始拷贝：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rsync -av   /opt/module     atguigu@hadoop103:/opt/</span><br></pre></td></tr></table></figure>

<p>（b）期望脚本：</p>
<p>xsync要同步的文件名称</p>
<p>（c）期望脚本在任何路径都能使用（脚本放在声明了全局环境变量的路径）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ echo $PATH</span><br><span class="line">/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/atguigu/.local/bin:/home/atguigu/bin:/opt/module/jdk1.8.0_212/bin</span><br></pre></td></tr></table></figure>

<p>（3）脚本实现</p>
<p>（a）在/home/atguigu/bin目录下创建xsync文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 opt]$ cd /home/atguigu</span><br><span class="line">[atguigu@hadoop102 ~]$ mkdir bin</span><br><span class="line">[atguigu@hadoop102 ~]$ cd bin</span><br><span class="line">[atguigu@hadoop102 bin]$ vim xsync</span><br></pre></td></tr></table></figure>

<p>在该文件中编写如下代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">#1. 判断参数个数</span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo Not Enough Arguement!</span><br><span class="line">    exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#2. 遍历集群所有机器</span><br><span class="line">for host in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">    echo ====================  $host  ====================</span><br><span class="line">    #3. 遍历所有目录，挨个发送</span><br><span class="line"></span><br><span class="line">    for file in $@</span><br><span class="line">    do</span><br><span class="line">        #4. 判断文件是否存在</span><br><span class="line">        if [ -e $file ]</span><br><span class="line">            then</span><br><span class="line">                #5. 获取父目录</span><br><span class="line">                pdir=$(cd -P $(dirname $file); pwd)</span><br><span class="line"></span><br><span class="line">                #6. 获取当前文件的名称</span><br><span class="line">                fname=$(basename $file)</span><br><span class="line">                ssh $host &quot;mkdir -p $pdir&quot;</span><br><span class="line">                rsync -av $pdir/$fname $host:$pdir</span><br><span class="line">            else</span><br><span class="line">                echo $file does not exists!</span><br><span class="line">        fi</span><br><span class="line">    done</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>（b）修改脚本 xsync 具有执行权限</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ chmod +x xsync</span><br></pre></td></tr></table></figure>

<p>（c）测试脚本</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ xsync /home/atguigu/bin</span><br></pre></td></tr></table></figure>

<p>（d）将脚本复制到/bin中，以便全局调用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ sudo cp xsync /bin/</span><br></pre></td></tr></table></figure>

<p>（e）同步环境变量配置（root所有者）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo ./bin/xsync /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>注意：如果用了sudo，那么xsync一定要给它的路径补全。</p>
<p>让环境变量生效</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 bin]$ source /etc/profile</span><br><span class="line">[atguigu@hadoop104 opt]$ source /etc/profile</span><br></pre></td></tr></table></figure>

<h3 id="3-2-3-SSH无密登录配置"><a href="#3-2-3-SSH无密登录配置" class="headerlink" title="3.2.3 SSH无密登录配置"></a>3.2.3 SSH无密登录配置</h3><p><strong>1</strong>）配置ssh</p>
<p>（1）基本语法</p>
<p>ssh另一台电脑的IP地址</p>
<p>（2）ssh连接时出现Host key verification failed的解决方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ ssh hadoop103</span><br></pre></td></tr></table></figure>

<p>Ø 如果出现如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Are you sure you want to continue connecting (yes/no)?</span><br></pre></td></tr></table></figure>

<p>Ø 输入yes，并回车</p>
<p>（3）退回到hadoop102</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ exit</span><br></pre></td></tr></table></figure>

<p><strong>2）无密钥配置</strong></p>
<p>（1）免密登录原理</p>
<p><img src="https://image.3001.net/images/20221104/16675258082005.png#crop=0&crop=0&crop=1&crop=1&id=zfcux&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>（2）生成公钥和私钥</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 .ssh]$ pwd</span><br><span class="line">/home/atguigu/.ssh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>

<p>然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）</p>
<p>（3）将公钥拷贝到要免密登录的目标机器上</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop102</span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop103</span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop104</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<p>还需要在hadoop102上采用atguigu账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。</p>
<p>还需要在hadoop103上采用atguigu账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。</p>
<p>还需要在hadoop104上采用atguigu账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。</p>
<p>还需要在hadoop102上采用root账号，配置一下无密登录到hadoop102、hadoop103、hadoop104；</p>
<p><strong>3</strong>）.ssh文件夹下（~/.ssh）的文件功能解释**</p>
<table>
<thead>
<tr>
<th>known_hosts</th>
<th>记录ssh访问过计算机的公钥（public  key）</th>
</tr>
</thead>
<tbody><tr>
<td>id_rsa</td>
<td>生成的私钥</td>
</tr>
<tr>
<td>id_rsa.pub</td>
<td>生成的公钥</td>
</tr>
<tr>
<td>authorized_keys</td>
<td>存放授权过的无密登录服务器公钥</td>
</tr>
</tbody></table>
<h3 id="3-2-4-集群配置"><a href="#3-2-4-集群配置" class="headerlink" title="3.2.4 集群配置"></a>3.2.4 集群配置</h3><p><strong>1）集群部署规划</strong></p>
<p>     注意：</p>
<p>Ø NameNode和SecondaryNameNode不要安装在同一台服务器</p>
<p>Ø ResourceManager也很消耗内存，不要和NameNode、SecondaryNameNode配置在同一台机器上。</p>
<table>
<thead>
<tr>
<th></th>
<th>hadoop102</th>
<th>hadoop103</th>
<th>hadoop104</th>
</tr>
</thead>
<tbody><tr>
<td>HDFS</td>
<td>NameNode  DataNode</td>
<td>DataNode</td>
<td>SecondaryNameNode  DataNode</td>
</tr>
<tr>
<td>YARN</td>
<td>NodeManager</td>
<td>ResourceManager  NodeManager</td>
<td>NodeManager</td>
</tr>
</tbody></table>
<p><strong>2</strong>）配置文件说明</p>
<p>Hadoop配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。</p>
<p>（1）默认配置文件：</p>
<table>
<thead>
<tr>
<th>要获取的默认文件</th>
<th>文件存放在Hadoop的jar包中的位置</th>
</tr>
</thead>
<tbody><tr>
<td>[core-default.xml]</td>
<td>hadoop-common-3.1.3.jar/core-default.xml</td>
</tr>
<tr>
<td>[hdfs-default.xml]</td>
<td>hadoop-hdfs-3.1.3.jar/hdfs-default.xml</td>
</tr>
<tr>
<td>[yarn-default.xml]</td>
<td>hadoop-yarn-common-3.1.3.jar/yarn-default.xml</td>
</tr>
<tr>
<td>[mapred-default.xml]</td>
<td>hadoop-mapreduce-client-core-3.1.3.jar/mapred-default.xml</td>
</tr>
</tbody></table>
<p>（2）自定义配置文件：</p>
<p>    <strong>core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml</strong>四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置。</p>
<p><strong>3）配置集群</strong></p>
<p>（1）核心配置文件</p>
<p>配置core-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ cd $HADOOP_HOME/etc/hadoop</span><br><span class="line">[atguigu@hadoop102 hadoop]$ vim core-site.xml</span><br></pre></td></tr></table></figure>

<p>文件内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定NameNode的地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop102:8020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 指定hadoop数据的存储目录 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/opt/module/hadoop-3.1.3/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>（2）HDFS配置文件</p>
<p>配置hdfs-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ vim hdfs-site.xml</span><br></pre></td></tr></table></figure>

<p>文件内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;!-- nn web端访问地址--&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop102:9870&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">	&lt;!-- 2nn web端访问地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop104:9868&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>（3）YARN配置文件</p>
<p>配置yarn-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ vim yarn-site.xml</span><br></pre></td></tr></table></figure>

<p>文件内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定MR走shuffle --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 指定ResourceManager的地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop103&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 环境变量的继承 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>（4）MapReduce配置文件</p>
<p>配置mapred-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ vim mapred-site.xml</span><br></pre></td></tr></table></figure>

<p>文件内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p><strong>4</strong>）在集群上分发配置好的Hadoop配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ xsync /opt/module/hadoop-3.1.3/etc/hadoop/</span><br></pre></td></tr></table></figure>

<p><strong>5</strong>）去103和104上查看文件分发情况</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ cat /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml</span><br><span class="line">[atguigu@hadoop104 ~]$ cat /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml</span><br></pre></td></tr></table></figure>

<h3 id="3-2-5-群起集群"><a href="#3-2-5-群起集群" class="headerlink" title="3.2.5 群起集群"></a>3.2.5 群起集群</h3><p><strong>1</strong>）配置workers</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ vim /opt/module/hadoop-3.1.3/etc/hadoop/workers</span><br></pre></td></tr></table></figure>

<p>在该文件中增加如下内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure>

<p>注意：该文件中添加的内容结尾不允许有空格，文件中不允许有空行。</p>
<p>同步所有节点配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ xsync /opt/module/hadoop-3.1.3/etc</span><br></pre></td></tr></table></figure>

<p><strong>2</strong>）启动集群</p>
<p>    （1）<strong>如果集群是第一次启动</strong>，需要在hadoop102节点格式化NameNode（注意：格式化NameNode，会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到已往数据。如果集群在运行过程中报错，需要重新格式化NameNode的话，一定要先停止namenode和datanode进程，并且要删除所有机器的data和logs目录，然后再进行格式化。）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hdfs namenode -format</span><br></pre></td></tr></table></figure>

<p>（2）启动HDFS</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<p>（3）在配置了ResourceManager的节点（hadoop103）启动YARN</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<p>（4）Web端查看HDFS的NameNode</p>
<p>（a）浏览器中输入：<a target="_blank" rel="noopener" href="http://hadoop102:9870/">http://hadoop102:9870</a></p>
<p>        （b）查看HDFS上存储的数据信息</p>
<p>（5）Web端查看YARN的ResourceManager</p>
<p>（a）浏览器中输入：<a target="_blank" rel="noopener" href="http://hadoop103:8088/">http://hadoop103:8088</a></p>
<p>    （b）查看YARN上运行的Job信息</p>
<p><strong>3</strong>）集群基本测试</p>
<p>（1）上传文件到集群</p>
<p>Ø 上传小文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ hadoop fs -mkdir /input</span><br><span class="line">[atguigu@hadoop102 ~]$ hadoop fs -put $HADOOP_HOME/wcinput/word.txt /input</span><br></pre></td></tr></table></figure>

<p>Ø  上传大文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ hadoop fs -put  /opt/software/jdk-8u212-linux-x64.tar.gz  /</span><br></pre></td></tr></table></figure>

<p>（2）上传文件后查看文件存放在什么位置</p>
<p>Ø 查看HDFS文件存储路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 subdir0]$ pwd</span><br><span class="line">/opt/module/hadoop-3.1.3/data/dfs/data/current/BP-1436128598-192.168.10.102-1610603650062/current/finalized/subdir0/subdir0</span><br></pre></td></tr></table></figure>

<p>Ø 查看HDFS在磁盘存储文件内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 subdir0]$ cat blk_1073741825</span><br><span class="line">hadoop yarn</span><br><span class="line">hadoop mapreduce </span><br><span class="line">atguigu</span><br><span class="line">atguigu</span><br></pre></td></tr></table></figure>

<p>（3）拼接</p>
<p>-rw-rw-r–. 1 atguigu atguigu 134217728 5月 23 16:01 <strong>blk_1073741836</strong></p>
<p>-rw-rw-r–. 1 atguigu atguigu  1048583 5月 23 16:01 blk_1073741836_1012.meta</p>
<p>-rw-rw-r–. 1 atguigu atguigu 63439959 5月 23 16:01 <strong>blk_1073741837</strong></p>
<p>-rw-rw-r–. 1 atguigu atguigu  495635 5月 23 16:01 blk_1073741837_1013.meta</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 subdir0]$ cat blk_1073741836&gt;&gt;tmp.tar.gz</span><br><span class="line">[atguigu@hadoop102 subdir0]$ cat blk_1073741837&gt;&gt;tmp.tar.gz</span><br><span class="line">[atguigu@hadoop102 subdir0]$ tar -zxvf tmp.tar.gz</span><br></pre></td></tr></table></figure>

<p>（4）下载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 software]$ hadoop fs -get /jdk-8u212-linux-x64.tar.gz ./</span><br></pre></td></tr></table></figure>

<p>（5）执行wordcount程序</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output</span><br></pre></td></tr></table></figure>

<h3 id="3-2-6-配置历史服务器"><a href="#3-2-6-配置历史服务器" class="headerlink" title="3.2.6 配置历史服务器"></a>3.2.6 配置历史服务器</h3><p>为了查看程序的历史运行情况，需要配置一下历史服务器。具体配置步骤如下：</p>
<p><strong>1</strong>）配置mapred-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ vim mapred-site.xml</span><br></pre></td></tr></table></figure>

<p>在该文件里面增加如下配置。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 历史服务器端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop102:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 历史服务器web端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop102:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p><strong>2</strong>）分发配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ xsync $HADOOP_HOME/etc/hadoop/mapred-site.xml</span><br></pre></td></tr></table></figure>

<p><strong>3</strong>）在hadoop102启动历史服务器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ mapred --daemon start historyserver</span><br></pre></td></tr></table></figure>

<p><strong>4</strong>）查看历史服务器是否启动</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ jps</span><br></pre></td></tr></table></figure>

<p><strong>5</strong>）查看JobHistory</p>
<p><a target="_blank" rel="noopener" href="http://hadoop102:19888/jobhistory">http://hadoop102:19888/jobhistory</a></p>
<h3 id="3-2-7-配置日志的聚集"><a href="#3-2-7-配置日志的聚集" class="headerlink" title="3.2.7 配置日志的聚集"></a>3.2.7 配置日志的聚集</h3><p>日志聚集概念：应用运行完成以后，将程序运行日志信息上传到HDFS系统上。</p>
<p>     <img src="https://image.3001.net/images/20221104/16675324111724.png#crop=0&crop=0&crop=1&crop=1&id=UW5mV&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>日志聚集功能好处：可以方便的查看到程序运行详情，方便开发调试。</p>
<pre><code>注意：开启日志聚集功能，需要重新启动NodeManager 、ResourceManager和HistoryServer。
</code></pre>
<p>开启日志聚集功能具体步骤如下：</p>
<p><strong>1</strong>）配置yarn-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ vim yarn-site.xml</span><br></pre></td></tr></table></figure>

<p>在该文件里面增加如下配置。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 开启日志聚集功能 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 设置日志聚集服务器地址 --&gt;</span><br><span class="line">&lt;property&gt;  </span><br><span class="line">    &lt;name&gt;yarn.log.server.url&lt;/name&gt;  </span><br><span class="line">    &lt;value&gt;http://hadoop102:19888/jobhistory/logs&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 设置日志保留时间为7天 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p><strong>2</strong>）分发配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ xsync $HADOOP_HOME/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure>

<p><strong>3</strong>）关闭NodeManager、ResourceManager和HistoryServer</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ sbin/stop-yarn.sh</span><br><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ mapred --daemon stop historyserver</span><br></pre></td></tr></table></figure>

<p><strong>4</strong>）启动NodeManager 、ResourceManage和HistoryServer</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ start-yarn.sh</span><br><span class="line">[atguigu@hadoop102 ~]$ mapred --daemon start historyserver</span><br></pre></td></tr></table></figure>

<p><strong>5</strong>）删除HDFS上已经存在的输出文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ hadoop fs -rm -r /output</span><br></pre></td></tr></table></figure>

<p><strong>6</strong>）执行WordCount程序</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output</span><br></pre></td></tr></table></figure>

<p><strong>7</strong>）查看日志</p>
<p>    （1）历史服务器地址</p>
<p><a target="_blank" rel="noopener" href="http://hadoop102:19888/jobhistory">http://hadoop102:19888/jobhistory</a></p>
<p>    （2）历史任务列表</p>
<p>     <img src="https://image.3001.net/images/20221104/16675325708222.png#crop=0&crop=0&crop=1&crop=1&id=L7UIu&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>    （3）查看任务运行日志</p>
<p>     <img src="https://image.3001.net/images/20221104/16675325798812.png#crop=0&crop=0&crop=1&crop=1&id=rnxrT&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>    （4）运行日志详情</p>
<p>     <img src="https://image.3001.net/images/20221104/16675325889825.png#crop=0&crop=0&crop=1&crop=1&id=ZpUju&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<h3 id="3-2-8-集群启动-停止方式总结"><a href="#3-2-8-集群启动-停止方式总结" class="headerlink" title="3.2.8 集群启动/停止方式总结"></a>3.2.8 集群启动/停止方式总结</h3><p><strong>1</strong>）各个模块分开启动/停止（配置ssh是前提）常用</p>
<p>    （1）整体启动/停止HDFS</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh/stop-dfs.sh</span><br></pre></td></tr></table></figure>

<p>    （2）整体启动/停止YARN</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh/stop-yarn.sh</span><br></pre></td></tr></table></figure>

<p><strong>2）各个服务组件逐一启动/停止</strong></p>
<p>    （1）分别启动/停止HDFS组件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs --daemon start/stop namenode/datanode/secondarynamenode</span><br></pre></td></tr></table></figure>

<p>    （2）启动/停止YARN</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn --daemon start/stop resourcemanager/nodemanager</span><br></pre></td></tr></table></figure>

<h3 id="3-2-9-编写Hadoop集群常用脚本"><a href="#3-2-9-编写Hadoop集群常用脚本" class="headerlink" title="3.2.9 编写Hadoop集群常用脚本"></a>3.2.9 编写Hadoop集群常用脚本</h3><p><strong>1</strong>）Hadoop集群启停脚本（包含HDFS，Yarn，Historyserver）：myhadoop.sh</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ cd /home/atguigu/bin</span><br><span class="line">[atguigu@hadoop102 bin]$ vim myhadoop.sh</span><br></pre></td></tr></table></figure>

<p>Ø 输入如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; =================== 启动 hadoop集群 ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot; --------------- 启动 hdfs ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/sbin/start-dfs.sh&quot;</span><br><span class="line">        echo &quot; --------------- 启动 yarn ---------------&quot;</span><br><span class="line">        ssh hadoop103 &quot;/opt/module/hadoop-3.1.3/sbin/start-yarn.sh&quot;</span><br><span class="line">        echo &quot; --------------- 启动 historyserver ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; =================== 关闭 hadoop集群 ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot; --------------- 关闭 historyserver ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver&quot;</span><br><span class="line">        echo &quot; --------------- 关闭 yarn ---------------&quot;</span><br><span class="line">        ssh hadoop103 &quot;/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh&quot;</span><br><span class="line">        echo &quot; --------------- 关闭 hdfs ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>Ø 保存后退出，然后赋予脚本执行权限</p>
<p>[atguigu<a href="/hadoop102">@hadoop102 </a> bin]$ chmod +x myhadoop.sh </p>
<p><strong>2）查看三台服务器Java进程脚本：jpsall</strong></p>
<p>[atguigu<a href="/hadoop102">@hadoop102 </a> ~]$ cd /home/atguigu/bin </p>
<p>[atguigu<a href="/hadoop102">@hadoop102 </a> bin]$ vim jpsall </p>
<p>Ø 输入如下内容</p>
<p>#!/bin/bash</p>
<p>for host in hadoop102 hadoop103 hadoop104</p>
<p>do</p>
<p>    echo =============== $host ===============</p>
<p>    ssh $host jps</p>
<p>done</p>
<p>Ø 保存后退出，然后赋予脚本执行权限</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ chmod +x jpsall</span><br></pre></td></tr></table></figure>

<p><strong>3</strong>）分发/home/atguigu/bin目录，保证自定义脚本在三台机器上都可以使用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ xsync /home/atguigu/bin/</span><br></pre></td></tr></table></figure>

<h3 id="3-2-10-常用端口号说明"><a href="#3-2-10-常用端口号说明" class="headerlink" title="3.2.10 常用端口号说明"></a>3.2.10 常用端口号说明</h3><table>
<thead>
<tr>
<th>端口名称</th>
<th>Hadoop2.x</th>
<th>Hadoop3.x</th>
</tr>
</thead>
<tbody><tr>
<td>NameNode内部通信端口</td>
<td>8020 / 9000</td>
<td>8020 /  9000/9820</td>
</tr>
<tr>
<td>NameNode HTTP UI</td>
<td>50070</td>
<td>9870</td>
</tr>
<tr>
<td>MapReduce查看执行任务端口</td>
<td>8088</td>
<td>8088</td>
</tr>
<tr>
<td>历史服务器通信端口</td>
<td>19888</td>
<td>19888</td>
</tr>
</tbody></table>
<h3 id="3-2-11-集群时间同步"><a href="#3-2-11-集群时间同步" class="headerlink" title="3.2.11 集群时间同步"></a>3.2.11 集群时间同步</h3><p>如果服务器在公网环境（能连接外网），可以不采用集群时间同步，因为服务器会定期和公网时间进行校准；</p>
<p>如果服务器在内网环境，必须要配置集群时间同步，否则时间久了，会产生时间偏差，导致集群执行任务时间不同步。</p>
<p><strong>1</strong>）需求</p>
<p>找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，生产环境根据任务对时间的准确程度要求周期同步。测试环境为了尽快看到效果，采用1分钟同步一次。</p>
<p><strong>2</strong>）时间服务器配置（必须root用户）</p>
<p>（1）查看所有节点ntpd服务状态和开机自启动状态</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo systemctl status ntpd</span><br><span class="line">[atguigu@hadoop102 ~]$ sudo systemctl start ntpd</span><br><span class="line">[atguigu@hadoop102 ~]$ sudo systemctl is-enabled ntpd</span><br></pre></td></tr></table></figure>

<p>（2）修改hadoop102的ntp.conf配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo vim /etc/ntp.conf</span><br></pre></td></tr></table></figure>

<p>修改内容如下</p>
<p>（a）修改1（授权192.168.10.0-192.168.10.255网段上的所有机器可以从这台机器上查询和同步时间）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#restrict 192.168.10.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line">为restrict 192.168.10.0 mask 255.255.255.0 nomodify notrap</span><br></pre></td></tr></table></figure>

<p>    （b）修改2（集群在局域网中，不使用其他互联网上的时间）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">server 0.centos.pool.ntp.org iburst</span><br><span class="line">server 1.centos.pool.ntp.org iburst</span><br><span class="line">server 2.centos.pool.ntp.org iburst</span><br><span class="line">server 3.centos.pool.ntp.org iburst</span><br></pre></td></tr></table></figure>

<p>为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#server 0.centos.pool.ntp.org iburst</span><br><span class="line">#server 1.centos.pool.ntp.org iburst</span><br><span class="line">#server 2.centos.pool.ntp.org iburst</span><br><span class="line">#server 3.centos.pool.ntp.org iburst</span><br></pre></td></tr></table></figure>

<p>（c）添加3（当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure>

<p>（3）修改hadoop102的/etc/sysconfig/ntpd 文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo vim /etc/sysconfig/ntpd</span><br></pre></td></tr></table></figure>

<p>增加内容如下（让硬件时间与系统时间一起同步）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SYNC_HWCLOCK=yes</span><br></pre></td></tr></table></figure>

<p>（4）重新启动ntpd服务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo systemctl start ntpd</span><br></pre></td></tr></table></figure>

<p>（5）设置ntpd服务开机启动</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo systemctl enable ntpd</span><br></pre></td></tr></table></figure>

<p><strong>3</strong>）其他机器配置（必须root用户）</p>
<p>（1）关闭所有节点上ntp服务和自启动</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ sudo systemctl stop ntpd</span><br><span class="line">[atguigu@hadoop103 ~]$ sudo systemctl disable ntpd</span><br><span class="line">[atguigu@hadoop104 ~]$ sudo systemctl stop ntpd</span><br><span class="line">[atguigu@hadoop104 ~]$ sudo systemctl disable ntpd</span><br></pre></td></tr></table></figure>

<p>（2）在其他机器配置1分钟与时间服务器同步一次</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ sudo crontab -e</span><br></pre></td></tr></table></figure>

<p>编写定时任务如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*/1 * * * * /usr/sbin/ntpdate hadoop102</span><br></pre></td></tr></table></figure>

<p>（3）修改任意机器时间</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ sudo date -s &quot;2021-9-11 11:11:11&quot;</span><br></pre></td></tr></table></figure>

<p>（4）1分钟后查看机器是否与时间服务器同步</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ sudo date</span><br></pre></td></tr></table></figure>

<h1 id="第4章-常见错误及解决方案"><a href="#第4章-常见错误及解决方案" class="headerlink" title="第4章 常见错误及解决方案"></a>第4章 常见错误及解决方案</h1><p>1）防火墙没关闭、或者没有启动YARN</p>
<ul>
<li>INFO client.RMProxy: Connecting to ResourceManager at hadoop108/192.168.10.108:8032*</li>
</ul>
<p>2）主机名称配置错误</p>
<p>3）IP地址配置错误</p>
<p>4）ssh没有配置好</p>
<p>5）root用户和atguigu两个用户启动集群不统一</p>
<p>6）配置文件修改不细心</p>
<p>7）不识别主机名称</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">java.net.UnknownHostException: hadoop102: hadoop102</span><br><span class="line">        at java.net.InetAddress.getLocalHost(InetAddress.java:1475)</span><br><span class="line">        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:146)</span><br><span class="line">        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)</span><br><span class="line">        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)</span><br><span class="line">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">at javax.security.auth.Subject.doAs(Subject.java:415)</span><br></pre></td></tr></table></figure>

<p>解决办法：</p>
<pre><code>（1）在/etc/hosts文件中添加192.168.10.102 hadoop102
</code></pre>
<p>    （2）主机名称不要起hadoop hadoop000等特殊名称</p>
<p>8）DataNode和NameNode进程同时只能工作一个。</p>
<p><img src="https://image.3001.net/images/20221104/16675333856288.png#crop=0&crop=0&crop=1&crop=1&id=izPch&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>
<p>9）执行命令不生效，粘贴Word中命令时，遇到-和长–没区分开。导致命令失效</p>
<pre><code>    解决办法：尽量不要粘贴Word中代码。
</code></pre>
<p>10）jps发现进程已经没有，但是重新启动集群，提示进程已经开启。</p>
<pre><code>    原因是在Linux的根目录下/tmp目录中存在启动的进程临时文件，将集群相关进程删除掉，再重新启动集群。
</code></pre>
<p>11）jps不生效</p>
<pre><code>    原因：全局变量hadoop java没有生效。解决办法：需要source /etc/profile文件。
</code></pre>
<p>12）8088端口连接不上</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 桌面]$ cat /etc/hosts</span><br></pre></td></tr></table></figure>

<p>注释掉如下代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#127.0.0.1  localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">#::1     hadoop102</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/hadoop/" rel="tag"># hadoop</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/05_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88Yarn%EF%BC%89V3.3/" rel="prev" title="05_尚硅谷大数据技术之Hadoop（Yarn）V3.3">
      <i class="fa fa-chevron-left"></i> 05_尚硅谷大数据技术之Hadoop（Yarn）V3.3
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/03_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88HDFS%EF%BC%89V3.3/" rel="next" title="03_尚硅谷大数据技术之Hadoop（HDFS）">
      03_尚硅谷大数据技术之Hadoop（HDFS） <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC1%E7%AB%A0-Hadoop%E6%A6%82%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">第1章 Hadoop概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-Hadoop%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 Hadoop是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-Hadoop%E5%8F%91%E5%B1%95%E5%8E%86%E5%8F%B2%EF%BC%88%E4%BA%86%E8%A7%A3%EF%BC%89"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 Hadoop发展历史（了解）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-Hadoop%E4%B8%89%E5%A4%A7%E5%8F%91%E8%A1%8C%E7%89%88%E6%9C%AC%EF%BC%88%E4%BA%86%E8%A7%A3%EF%BC%89"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 Hadoop三大发行版本（了解）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-Hadoop%E4%BC%98%E5%8A%BF%EF%BC%884%E9%AB%98%EF%BC%89"><span class="nav-number">1.4.</span> <span class="nav-text">1.4 Hadoop优势（4高）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-Hadoop%E7%BB%84%E6%88%90%EF%BC%88%E9%9D%A2%E8%AF%95%E9%87%8D%E7%82%B9%EF%BC%89"><span class="nav-number">1.5.</span> <span class="nav-text">1.5 Hadoop组成（面试重点）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-1-HDFS%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0"><span class="nav-number">1.5.1.</span> <span class="nav-text">1.5.1 HDFS架构概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-2-YARN%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0"><span class="nav-number">1.5.2.</span> <span class="nav-text">1.5.2 YARN架构概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-3-MapReduce%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0"><span class="nav-number">1.5.3.</span> <span class="nav-text">1.5.3 MapReduce架构概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-4-HDFS%E3%80%81YARN%E3%80%81MapReduce%E4%B8%89%E8%80%85%E5%85%B3%E7%B3%BB"><span class="nav-number">1.5.4.</span> <span class="nav-text">1.5.4 HDFS、YARN、MapReduce三者关系</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-6-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E7%94%9F%E6%80%81%E4%BD%93%E7%B3%BB"><span class="nav-number">1.6.</span> <span class="nav-text">1.6 大数据技术生态体系</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-7-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%A1%86%E6%9E%B6%E5%9B%BE"><span class="nav-number">1.7.</span> <span class="nav-text">1.7 推荐系统框架图</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC2%E7%AB%A0-Hadoop%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%EF%BC%88%E5%BC%80%E5%8F%91%E9%87%8D%E7%82%B9%EF%BC%89"><span class="nav-number">2.</span> <span class="nav-text">第2章 Hadoop运行环境搭建（开发重点）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E6%A8%A1%E6%9D%BF%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 模板虚拟机环境准备</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E5%85%8B%E9%9A%86%E8%99%9A%E6%8B%9F%E6%9C%BA"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 克隆虚拟机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-%E5%9C%A8hadoop102%E5%AE%89%E8%A3%85JDK"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 在hadoop102安装JDK</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-%E5%9C%A8hadoop102%E5%AE%89%E8%A3%85Hadoop"><span class="nav-number">2.4.</span> <span class="nav-text">2.4 在hadoop102安装Hadoop</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5-Hadoop%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84"><span class="nav-number">2.5.</span> <span class="nav-text">2.5 Hadoop目录结构</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC3%E7%AB%A0-Hadoop%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F"><span class="nav-number">3.</span> <span class="nav-text">第3章 Hadoop运行模式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%EF%BC%88%E5%AE%98%E6%96%B9WordCount%EF%BC%89"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 本地运行模式（官方WordCount）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%EF%BC%88%E5%BC%80%E5%8F%91%E9%87%8D%E7%82%B9%EF%BC%89"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 完全分布式运行模式（开发重点）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%87%86%E5%A4%87"><span class="nav-number">3.2.1.</span> <span class="nav-text">3.2.1 虚拟机准备</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-%E7%BC%96%E5%86%99%E9%9B%86%E7%BE%A4%E5%88%86%E5%8F%91%E8%84%9A%E6%9C%ACxsync"><span class="nav-number">3.2.2.</span> <span class="nav-text">3.2.2 编写集群分发脚本xsync</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-3-SSH%E6%97%A0%E5%AF%86%E7%99%BB%E5%BD%95%E9%85%8D%E7%BD%AE"><span class="nav-number">3.2.3.</span> <span class="nav-text">3.2.3 SSH无密登录配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-4-%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE"><span class="nav-number">3.2.4.</span> <span class="nav-text">3.2.4 集群配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-5-%E7%BE%A4%E8%B5%B7%E9%9B%86%E7%BE%A4"><span class="nav-number">3.2.5.</span> <span class="nav-text">3.2.5 群起集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-6-%E9%85%8D%E7%BD%AE%E5%8E%86%E5%8F%B2%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="nav-number">3.2.6.</span> <span class="nav-text">3.2.6 配置历史服务器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-7-%E9%85%8D%E7%BD%AE%E6%97%A5%E5%BF%97%E7%9A%84%E8%81%9A%E9%9B%86"><span class="nav-number">3.2.7.</span> <span class="nav-text">3.2.7 配置日志的聚集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-8-%E9%9B%86%E7%BE%A4%E5%90%AF%E5%8A%A8-%E5%81%9C%E6%AD%A2%E6%96%B9%E5%BC%8F%E6%80%BB%E7%BB%93"><span class="nav-number">3.2.8.</span> <span class="nav-text">3.2.8 集群启动&#x2F;停止方式总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-9-%E7%BC%96%E5%86%99Hadoop%E9%9B%86%E7%BE%A4%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC"><span class="nav-number">3.2.9.</span> <span class="nav-text">3.2.9 编写Hadoop集群常用脚本</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-10-%E5%B8%B8%E7%94%A8%E7%AB%AF%E5%8F%A3%E5%8F%B7%E8%AF%B4%E6%98%8E"><span class="nav-number">3.2.10.</span> <span class="nav-text">3.2.10 常用端口号说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-11-%E9%9B%86%E7%BE%A4%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5"><span class="nav-number">3.2.11.</span> <span class="nav-text">3.2.11 集群时间同步</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC4%E7%AB%A0-%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">4.</span> <span class="nav-text">第4章 常见错误及解决方案</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="希文"
      src="/images/%E5%A4%B4%E5%83%8F.gif">
  <p class="site-author-name" itemprop="name">希文</p>
  <div class="site-description" itemprop="description">日常笔记</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">49</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yangmour" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yangmour" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:xiwenya999@gmail.com" title="E-Mail → mailto:xiwenya999@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://poc10.cn/" title="http:&#x2F;&#x2F;poc10.cn&#x2F;" rel="noopener" target="_blank">网络安全方面</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">希文</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">804k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">12:11</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
